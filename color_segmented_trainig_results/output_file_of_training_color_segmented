2018-10-16 10:56:50.229466: Pre-trained model restored from /home/administrator/Documents/Finalproject/inception-v3-model/inception-v3/model.ckpt-157585
2018-10-16 10:57:14.032096: step 0, loss = 3.57 (1.9 examples/sec; 17.214 sec/batch)
2018-10-16 10:57:36.269867: step 10, loss = 3.46 (46.9 examples/sec; 0.682 sec/batch)
2018-10-16 10:57:43.049976: step 20, loss = 3.16 (47.2 examples/sec; 0.678 sec/batch)
2018-10-16 10:57:49.930459: step 30, loss = 2.98 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 10:57:56.654826: step 40, loss = 2.96 (48.0 examples/sec; 0.667 sec/batch)
2018-10-16 10:58:03.405565: step 50, loss = 3.16 (47.9 examples/sec; 0.668 sec/batch)
2018-10-16 10:58:10.637408: step 60, loss = 2.50 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 10:58:18.199853: step 70, loss = 2.95 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 10:58:25.480862: step 80, loss = 2.54 (42.3 examples/sec; 0.757 sec/batch)
2018-10-16 10:58:32.716591: step 90, loss = 2.56 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 10:58:39.898458: step 100, loss = 2.58 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 10:58:49.623741: step 110, loss = 2.48 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 10:58:56.753767: step 120, loss = 2.30 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 10:59:03.987294: step 130, loss = 1.98 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 10:59:11.190797: step 140, loss = 2.09 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 10:59:18.510692: step 150, loss = 2.40 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 10:59:25.744374: step 160, loss = 2.14 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 10:59:32.939183: step 170, loss = 2.11 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 10:59:40.231353: step 180, loss = 1.98 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 10:59:47.486805: step 190, loss = 2.16 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 10:59:54.646590: step 200, loss = 1.90 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 11:00:04.366640: step 210, loss = 1.87 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:00:11.549797: step 220, loss = 1.62 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 11:00:18.745426: step 230, loss = 2.08 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 11:00:26.006525: step 240, loss = 1.96 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 11:00:33.173949: step 250, loss = 1.80 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 11:00:40.325205: step 260, loss = 1.90 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 11:00:47.569211: step 270, loss = 1.53 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 11:00:54.838070: step 280, loss = 1.73 (42.0 examples/sec; 0.762 sec/batch)
2018-10-16 11:01:02.058239: step 290, loss = 1.86 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 11:01:09.340668: step 300, loss = 1.86 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 11:01:19.036149: step 310, loss = 1.65 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 11:01:26.292662: step 320, loss = 1.88 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:01:33.488012: step 330, loss = 1.45 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 11:01:40.756611: step 340, loss = 1.46 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:01:47.907058: step 350, loss = 1.50 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 11:01:55.191725: step 360, loss = 1.42 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 11:02:02.408619: step 370, loss = 1.80 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 11:02:09.639687: step 380, loss = 1.68 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 11:02:16.898209: step 390, loss = 1.69 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 11:02:24.160319: step 400, loss = 1.53 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 11:02:34.883263: step 410, loss = 1.79 (46.9 examples/sec; 0.682 sec/batch)
2018-10-16 11:02:42.121575: step 420, loss = 1.39 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 11:02:49.330362: step 430, loss = 1.49 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 11:02:56.475471: step 440, loss = 1.45 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:03:03.655035: step 450, loss = 1.55 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 11:03:10.820995: step 460, loss = 1.54 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 11:03:18.025460: step 470, loss = 1.56 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 11:03:25.198568: step 480, loss = 1.48 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 11:03:32.405798: step 490, loss = 1.31 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 11:03:39.511189: step 500, loss = 1.39 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 11:03:49.229334: step 510, loss = 1.32 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 11:03:56.403443: step 520, loss = 1.60 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 11:04:03.581415: step 530, loss = 1.53 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:04:10.758357: step 540, loss = 1.70 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 11:04:17.905364: step 550, loss = 1.30 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 11:04:25.141220: step 560, loss = 1.25 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 11:04:32.425528: step 570, loss = 1.36 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 11:04:39.577181: step 580, loss = 1.37 (46.4 examples/sec; 0.689 sec/batch)
2018-10-16 11:04:46.725122: step 590, loss = 1.59 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 11:04:53.856157: step 600, loss = 1.52 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 11:05:03.520237: step 610, loss = 1.48 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 11:05:10.664808: step 620, loss = 1.31 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 11:05:17.840669: step 630, loss = 1.48 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:05:25.051209: step 640, loss = 1.42 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:05:32.141729: step 650, loss = 1.50 (47.2 examples/sec; 0.678 sec/batch)
2018-10-16 11:05:39.283407: step 660, loss = 1.54 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 11:05:46.454150: step 670, loss = 1.26 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 11:05:53.614986: step 680, loss = 1.56 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 11:06:00.744941: step 690, loss = 1.41 (42.9 examples/sec; 0.747 sec/batch)
2018-10-16 11:06:07.867282: step 700, loss = 1.53 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 11:06:17.576529: step 710, loss = 1.57 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 11:06:24.712148: step 720, loss = 1.46 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 11:06:31.871407: step 730, loss = 1.53 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:06:38.981271: step 740, loss = 1.61 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:06:46.094710: step 750, loss = 1.39 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 11:06:53.272442: step 760, loss = 1.58 (45.7 examples/sec; 0.699 sec/batch)
2018-10-16 11:07:00.422096: step 770, loss = 1.25 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 11:07:07.597916: step 780, loss = 1.32 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:07:14.758509: step 790, loss = 1.66 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:07:21.989959: step 800, loss = 1.28 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 11:07:31.589413: step 810, loss = 1.32 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 11:07:38.750321: step 820, loss = 1.58 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 11:07:45.884304: step 830, loss = 1.49 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 11:07:53.039385: step 840, loss = 1.26 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 11:08:00.247886: step 850, loss = 1.16 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 11:08:07.330267: step 860, loss = 1.22 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 11:08:14.461980: step 870, loss = 1.28 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 11:08:21.629724: step 880, loss = 1.23 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 11:08:28.791013: step 890, loss = 1.44 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 11:08:35.967747: step 900, loss = 1.34 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 11:08:45.678491: step 910, loss = 1.38 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:08:52.804364: step 920, loss = 1.34 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:08:59.874994: step 930, loss = 1.47 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 11:09:06.994265: step 940, loss = 1.29 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 11:09:14.075225: step 950, loss = 1.34 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 11:09:21.239420: step 960, loss = 1.36 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:09:28.332822: step 970, loss = 1.51 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 11:09:35.433123: step 980, loss = 1.23 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 11:09:42.592262: step 990, loss = 1.37 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:09:49.704555: step 1000, loss = 1.25 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 11:09:59.465999: step 1010, loss = 1.45 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:10:06.684130: step 1020, loss = 1.54 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:10:13.864202: step 1030, loss = 1.27 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 11:10:21.066327: step 1040, loss = 1.49 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 11:10:28.255638: step 1050, loss = 1.46 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 11:10:35.446501: step 1060, loss = 1.26 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 11:10:42.586784: step 1070, loss = 1.21 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 11:10:49.823566: step 1080, loss = 1.31 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 11:10:56.971806: step 1090, loss = 1.57 (42.9 examples/sec; 0.747 sec/batch)
2018-10-16 11:11:04.115627: step 1100, loss = 1.29 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 11:11:13.883094: step 1110, loss = 1.35 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:11:21.020173: step 1120, loss = 1.08 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 11:11:28.202961: step 1130, loss = 1.60 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:11:35.333500: step 1140, loss = 1.52 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 11:11:42.445290: step 1150, loss = 1.22 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:11:49.577025: step 1160, loss = 1.27 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 11:11:56.782650: step 1170, loss = 1.15 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:12:03.953053: step 1180, loss = 1.37 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:12:11.181342: step 1190, loss = 1.49 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 11:12:18.431627: step 1200, loss = 1.11 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 11:12:28.134408: step 1210, loss = 1.32 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 11:12:35.180322: step 1220, loss = 1.11 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 11:12:42.317575: step 1230, loss = 1.33 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:12:49.441836: step 1240, loss = 1.40 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:12:56.664718: step 1250, loss = 1.42 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 11:13:03.773131: step 1260, loss = 1.25 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 11:13:10.914590: step 1270, loss = 1.40 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 11:13:18.211741: step 1280, loss = 1.36 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 11:13:25.363946: step 1290, loss = 1.25 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 11:13:32.586947: step 1300, loss = 1.62 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 11:13:42.499732: step 1310, loss = 1.51 (45.9 examples/sec; 0.696 sec/batch)
2018-10-16 11:13:49.653208: step 1320, loss = 1.21 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 11:13:56.784466: step 1330, loss = 1.46 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:14:04.013295: step 1340, loss = 1.49 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 11:14:11.147509: step 1350, loss = 1.26 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 11:14:18.276538: step 1360, loss = 1.29 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:14:25.448017: step 1370, loss = 1.27 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 11:14:32.614193: step 1380, loss = 1.31 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 11:14:39.830971: step 1390, loss = 1.28 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 11:14:46.918375: step 1400, loss = 1.26 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 11:14:56.794461: step 1410, loss = 1.37 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 11:15:03.977571: step 1420, loss = 1.20 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 11:15:11.145158: step 1430, loss = 1.36 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 11:15:18.364559: step 1440, loss = 1.29 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 11:15:25.468726: step 1450, loss = 1.39 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:15:32.690752: step 1460, loss = 1.33 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 11:15:39.779985: step 1470, loss = 1.27 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:15:46.971362: step 1480, loss = 1.19 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 11:15:54.139387: step 1490, loss = 1.43 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 11:16:01.294450: step 1500, loss = 1.22 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:16:10.986828: step 1510, loss = 1.30 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 11:16:18.100756: step 1520, loss = 1.47 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 11:16:25.191995: step 1530, loss = 1.20 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 11:16:32.419990: step 1540, loss = 1.20 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 11:16:39.599663: step 1550, loss = 1.42 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 11:16:46.692531: step 1560, loss = 1.13 (45.7 examples/sec; 0.699 sec/batch)
2018-10-16 11:16:53.940564: step 1570, loss = 1.22 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 11:17:01.130573: step 1580, loss = 1.08 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 11:17:08.295113: step 1590, loss = 1.39 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 11:17:15.415664: step 1600, loss = 1.11 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 11:17:25.110973: step 1610, loss = 1.64 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:17:32.349063: step 1620, loss = 1.16 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 11:17:39.494515: step 1630, loss = 1.22 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 11:17:46.673883: step 1640, loss = 1.52 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 11:17:53.856940: step 1650, loss = 1.14 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 11:18:00.962905: step 1660, loss = 1.14 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 11:18:08.076725: step 1670, loss = 1.17 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 11:18:15.218486: step 1680, loss = 1.27 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:18:22.333365: step 1690, loss = 1.29 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 11:18:29.540693: step 1700, loss = 1.30 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 11:18:39.343424: step 1710, loss = 1.21 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 11:18:46.473688: step 1720, loss = 1.20 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 11:18:53.640699: step 1730, loss = 1.28 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 11:19:00.790408: step 1740, loss = 1.29 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 11:19:07.920600: step 1750, loss = 1.37 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 11:19:15.138363: step 1760, loss = 1.16 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:19:22.285091: step 1770, loss = 1.39 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 11:19:29.423844: step 1780, loss = 1.41 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 11:19:36.596633: step 1790, loss = 1.41 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 11:19:43.865048: step 1800, loss = 1.08 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 11:19:53.651503: step 1810, loss = 1.16 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 11:20:00.776573: step 1820, loss = 1.40 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 11:20:07.989687: step 1830, loss = 1.30 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:20:15.159525: step 1840, loss = 1.45 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 11:20:22.368379: step 1850, loss = 1.24 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 11:20:29.511153: step 1860, loss = 1.24 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 11:20:36.629245: step 1870, loss = 1.18 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 11:20:43.811428: step 1880, loss = 1.63 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 11:20:50.999888: step 1890, loss = 1.39 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 11:20:58.172851: step 1900, loss = 1.26 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 11:21:07.842779: step 1910, loss = 1.40 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 11:21:14.916240: step 1920, loss = 1.31 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 11:21:22.029613: step 1930, loss = 1.14 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 11:21:29.276074: step 1940, loss = 1.27 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 11:21:36.445307: step 1950, loss = 1.17 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 11:21:43.667841: step 1960, loss = 1.13 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:21:50.799186: step 1970, loss = 1.19 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:21:57.965892: step 1980, loss = 1.18 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 11:22:05.157983: step 1990, loss = 1.26 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 11:22:12.245379: step 2000, loss = 1.26 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 11:22:22.433450: step 2010, loss = 1.23 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 11:22:29.566844: step 2020, loss = 1.19 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:22:36.785959: step 2030, loss = 1.24 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 11:22:43.941441: step 2040, loss = 1.17 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:22:51.130880: step 2050, loss = 1.43 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 11:22:58.245974: step 2060, loss = 1.13 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 11:23:05.399428: step 2070, loss = 1.18 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 11:23:12.641514: step 2080, loss = 1.33 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 11:23:19.812212: step 2090, loss = 1.16 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 11:23:26.964971: step 2100, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 11:23:36.612172: step 2110, loss = 1.23 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 11:23:43.787662: step 2120, loss = 1.16 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 11:23:51.053026: step 2130, loss = 1.45 (43.4 examples/sec; 0.736 sec/batch)
2018-10-16 11:23:58.187524: step 2140, loss = 1.14 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 11:24:05.333912: step 2150, loss = 1.08 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 11:24:12.501448: step 2160, loss = 1.58 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 11:24:19.660045: step 2170, loss = 1.22 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 11:24:26.904442: step 2180, loss = 1.41 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 11:24:34.088446: step 2190, loss = 1.12 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 11:24:41.255396: step 2200, loss = 1.12 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 11:24:51.008627: step 2210, loss = 1.19 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 11:24:58.166862: step 2220, loss = 1.25 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 11:25:05.314086: step 2230, loss = 1.54 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 11:25:12.438213: step 2240, loss = 1.44 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 11:25:19.561648: step 2250, loss = 1.23 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 11:25:26.743461: step 2260, loss = 1.18 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 11:25:33.845929: step 2270, loss = 1.17 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 11:25:40.967883: step 2280, loss = 1.34 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 11:25:48.128978: step 2290, loss = 1.39 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:25:55.346073: step 2300, loss = 1.45 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 11:26:05.642069: step 2310, loss = 1.12 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:26:12.816056: step 2320, loss = 1.39 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:26:19.955411: step 2330, loss = 1.07 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 11:26:27.117501: step 2340, loss = 1.43 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 11:26:34.215678: step 2350, loss = 1.10 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 11:26:41.370300: step 2360, loss = 1.36 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 11:26:48.518784: step 2370, loss = 1.18 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:26:55.741614: step 2380, loss = 1.26 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 11:27:02.887806: step 2390, loss = 1.36 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:27:10.038675: step 2400, loss = 1.34 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 11:27:20.003980: step 2410, loss = 1.20 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 11:27:27.132272: step 2420, loss = 1.51 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 11:27:34.306529: step 2430, loss = 1.16 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 11:27:41.465425: step 2440, loss = 1.22 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 11:27:48.563693: step 2450, loss = 1.37 (46.6 examples/sec; 0.687 sec/batch)
2018-10-16 11:27:55.648008: step 2460, loss = 1.14 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 11:28:02.815362: step 2470, loss = 1.10 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 11:28:09.955618: step 2480, loss = 1.11 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 11:28:17.057990: step 2490, loss = 1.06 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 11:28:24.232319: step 2500, loss = 1.21 (46.6 examples/sec; 0.686 sec/batch)
2018-10-16 11:28:33.986675: step 2510, loss = 1.27 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 11:28:41.033270: step 2520, loss = 1.06 (47.1 examples/sec; 0.680 sec/batch)
2018-10-16 11:28:48.159654: step 2530, loss = 1.29 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 11:28:55.323494: step 2540, loss = 1.25 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 11:29:02.504085: step 2550, loss = 1.09 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 11:29:09.712381: step 2560, loss = 1.23 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 11:29:16.833395: step 2570, loss = 1.15 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 11:29:23.964219: step 2580, loss = 1.19 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 11:29:31.167488: step 2590, loss = 1.43 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 11:29:38.438357: step 2600, loss = 1.08 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 11:29:48.088671: step 2610, loss = 1.76 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 11:29:55.157037: step 2620, loss = 1.11 (46.6 examples/sec; 0.687 sec/batch)
2018-10-16 11:30:02.404612: step 2630, loss = 1.11 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 11:30:09.535886: step 2640, loss = 1.07 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 11:30:16.646800: step 2650, loss = 1.15 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 11:30:23.847861: step 2660, loss = 1.26 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 11:30:30.971979: step 2670, loss = 1.29 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 11:30:38.150190: step 2680, loss = 1.09 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 11:30:45.262525: step 2690, loss = 1.08 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 11:30:52.411289: step 2700, loss = 1.37 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 11:31:02.043035: step 2710, loss = 1.40 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 11:31:09.156716: step 2720, loss = 1.32 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 11:31:16.247275: step 2730, loss = 1.45 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 11:31:23.291835: step 2740, loss = 1.42 (47.3 examples/sec; 0.677 sec/batch)
2018-10-16 11:31:30.386934: step 2750, loss = 1.19 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 11:31:37.574637: step 2760, loss = 1.19 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:31:44.787106: step 2770, loss = 1.37 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 11:31:51.918838: step 2780, loss = 1.24 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 11:31:59.125362: step 2790, loss = 1.33 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 11:32:06.265107: step 2800, loss = 1.28 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 11:32:15.980006: step 2810, loss = 1.31 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 11:32:23.108254: step 2820, loss = 1.15 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:32:30.229497: step 2830, loss = 1.13 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 11:32:37.404047: step 2840, loss = 1.03 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 11:32:44.498607: step 2850, loss = 1.33 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:32:51.614585: step 2860, loss = 1.21 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 11:32:58.797865: step 2870, loss = 1.14 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 11:33:05.839282: step 2880, loss = 1.11 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:33:12.987589: step 2890, loss = 1.19 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 11:33:20.108111: step 2900, loss = 1.07 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 11:33:29.741082: step 2910, loss = 1.09 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 11:33:36.914974: step 2920, loss = 1.34 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 11:33:44.053848: step 2930, loss = 1.15 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 11:33:51.174894: step 2940, loss = 1.12 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 11:33:58.347450: step 2950, loss = 1.06 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 11:34:05.545882: step 2960, loss = 1.20 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 11:34:12.712092: step 2970, loss = 1.20 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:34:19.865312: step 2980, loss = 1.07 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 11:34:27.060332: step 2990, loss = 1.16 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 11:34:34.160277: step 3000, loss = 1.39 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 11:34:43.858545: step 3010, loss = 1.09 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 11:34:50.944939: step 3020, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:34:58.141744: step 3030, loss = 1.17 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 11:35:05.262103: step 3040, loss = 1.18 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 11:35:12.509272: step 3050, loss = 1.26 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 11:35:19.602433: step 3060, loss = 1.29 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 11:35:26.664475: step 3070, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:35:33.782503: step 3080, loss = 1.09 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 11:35:41.046044: step 3090, loss = 1.03 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 11:35:48.139041: step 3100, loss = 1.15 (47.6 examples/sec; 0.673 sec/batch)
2018-10-16 11:35:58.011764: step 3110, loss = 1.16 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 11:36:05.070342: step 3120, loss = 1.14 (47.2 examples/sec; 0.678 sec/batch)
2018-10-16 11:36:12.254272: step 3130, loss = 1.21 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 11:36:19.483999: step 3140, loss = 1.10 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:36:26.533486: step 3150, loss = 1.17 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 11:36:33.655818: step 3160, loss = 1.13 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:36:40.848001: step 3170, loss = 1.27 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:36:47.995974: step 3180, loss = 1.23 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:36:55.172982: step 3190, loss = 1.14 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 11:37:02.256092: step 3200, loss = 1.25 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:37:12.194688: step 3210, loss = 1.10 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:37:19.309234: step 3220, loss = 1.16 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 11:37:26.506267: step 3230, loss = 1.23 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:37:33.676959: step 3240, loss = 1.14 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 11:37:40.857326: step 3250, loss = 1.31 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 11:37:48.014923: step 3260, loss = 1.14 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 11:37:55.217750: step 3270, loss = 1.21 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 11:38:02.353340: step 3280, loss = 1.47 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 11:38:09.574328: step 3290, loss = 1.20 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:38:16.844180: step 3300, loss = 1.10 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:38:26.724384: step 3310, loss = 1.14 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 11:38:33.837937: step 3320, loss = 1.17 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 11:38:40.999119: step 3330, loss = 1.14 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 11:38:48.167468: step 3340, loss = 1.30 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:38:55.310229: step 3350, loss = 1.07 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 11:39:02.384397: step 3360, loss = 1.29 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:39:09.527301: step 3370, loss = 1.12 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 11:39:16.711956: step 3380, loss = 1.21 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 11:39:23.796941: step 3390, loss = 1.16 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 11:39:30.937880: step 3400, loss = 1.14 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 11:39:40.603469: step 3410, loss = 1.16 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 11:39:47.754184: step 3420, loss = 1.34 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 11:39:54.895805: step 3430, loss = 1.26 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 11:40:01.979219: step 3440, loss = 1.40 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 11:40:09.150148: step 3450, loss = 1.12 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 11:40:16.172918: step 3460, loss = 1.59 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 11:40:23.324839: step 3470, loss = 1.29 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 11:40:30.494021: step 3480, loss = 1.29 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 11:40:37.649515: step 3490, loss = 1.27 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 11:40:44.807852: step 3500, loss = 1.11 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 11:40:54.509152: step 3510, loss = 1.11 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 11:41:01.679981: step 3520, loss = 1.14 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 11:41:08.961370: step 3530, loss = 1.23 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 11:41:16.053981: step 3540, loss = 1.06 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 11:41:23.159441: step 3550, loss = 1.39 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 11:41:30.308053: step 3560, loss = 1.11 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 11:41:37.478054: step 3570, loss = 1.20 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 11:41:44.622632: step 3580, loss = 1.06 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 11:41:51.764170: step 3590, loss = 1.09 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 11:41:58.888268: step 3600, loss = 1.06 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 11:42:09.409988: step 3610, loss = 1.19 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 11:42:16.489799: step 3620, loss = 1.12 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 11:42:23.714034: step 3630, loss = 1.09 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 11:42:30.953370: step 3640, loss = 1.10 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 11:42:37.980163: step 3650, loss = 1.16 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 11:42:45.186288: step 3660, loss = 1.12 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:42:52.369492: step 3670, loss = 1.34 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 11:42:59.590427: step 3680, loss = 1.20 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:43:06.785538: step 3690, loss = 1.19 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 11:43:13.966684: step 3700, loss = 1.26 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 11:43:23.699150: step 3710, loss = 1.35 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:43:30.862160: step 3720, loss = 1.12 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 11:43:38.108108: step 3730, loss = 1.16 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 11:43:45.302624: step 3740, loss = 1.26 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 11:43:52.488087: step 3750, loss = 1.21 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 11:43:59.615713: step 3760, loss = 1.59 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 11:44:06.858223: step 3770, loss = 1.04 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 11:44:14.048398: step 3780, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:44:21.123430: step 3790, loss = 1.06 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 11:44:28.265191: step 3800, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 11:44:38.031620: step 3810, loss = 1.11 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 11:44:45.195614: step 3820, loss = 1.22 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 11:44:52.338759: step 3830, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:44:59.513333: step 3840, loss = 1.11 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 11:45:06.682202: step 3850, loss = 1.41 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 11:45:13.852410: step 3860, loss = 1.26 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 11:45:21.022071: step 3870, loss = 1.43 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 11:45:28.248906: step 3880, loss = 1.44 (42.3 examples/sec; 0.757 sec/batch)
2018-10-16 11:45:35.383731: step 3890, loss = 1.11 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 11:45:42.523721: step 3900, loss = 1.11 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 11:45:52.579433: step 3910, loss = 1.11 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 11:45:59.723630: step 3920, loss = 1.10 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 11:46:06.775437: step 3930, loss = 1.07 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 11:46:13.853738: step 3940, loss = 1.31 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 11:46:21.065982: step 3950, loss = 1.08 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 11:46:28.262792: step 3960, loss = 1.24 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 11:46:35.454538: step 3970, loss = 1.29 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 11:46:42.596880: step 3980, loss = 1.11 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 11:46:49.831533: step 3990, loss = 1.40 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 11:46:56.997899: step 4000, loss = 1.36 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 11:47:06.783685: step 4010, loss = 1.27 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 11:47:13.892652: step 4020, loss = 1.13 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 11:47:21.096076: step 4030, loss = 1.26 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 11:47:28.261991: step 4040, loss = 1.20 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 11:47:35.410455: step 4050, loss = 1.24 (44.0 examples/sec; 0.726 sec/batch)
2018-10-16 11:47:42.558158: step 4060, loss = 1.14 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 11:47:49.659561: step 4070, loss = 1.31 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 11:47:56.852397: step 4080, loss = 1.32 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:48:04.004841: step 4090, loss = 1.16 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:48:11.161165: step 4100, loss = 1.25 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 11:48:20.779309: step 4110, loss = 1.09 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 11:48:27.887970: step 4120, loss = 1.20 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 11:48:34.977881: step 4130, loss = 1.18 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 11:48:42.155593: step 4140, loss = 1.15 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:48:49.366405: step 4150, loss = 1.22 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 11:48:56.557534: step 4160, loss = 1.23 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 11:49:03.748964: step 4170, loss = 1.36 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 11:49:10.886236: step 4180, loss = 1.08 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 11:49:18.058385: step 4190, loss = 1.31 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 11:49:25.177206: step 4200, loss = 1.09 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 11:49:35.222876: step 4210, loss = 1.12 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 11:49:42.365866: step 4220, loss = 1.11 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 11:49:49.526941: step 4230, loss = 1.29 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 11:49:56.662991: step 4240, loss = 1.21 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 11:50:03.782773: step 4250, loss = 1.06 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 11:50:10.959143: step 4260, loss = 1.07 (42.5 examples/sec; 0.753 sec/batch)
2018-10-16 11:50:18.149193: step 4270, loss = 1.08 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 11:50:25.273717: step 4280, loss = 1.09 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 11:50:32.421311: step 4290, loss = 1.04 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 11:50:39.581617: step 4300, loss = 1.12 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 11:50:49.200321: step 4310, loss = 1.24 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 11:50:56.334189: step 4320, loss = 1.24 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 11:51:03.493573: step 4330, loss = 1.10 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 11:51:10.637097: step 4340, loss = 1.13 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 11:51:17.840883: step 4350, loss = 1.35 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 11:51:25.050262: step 4360, loss = 1.16 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 11:51:32.243285: step 4370, loss = 1.21 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 11:51:39.404970: step 4380, loss = 1.31 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 11:51:46.562777: step 4390, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 11:51:53.735105: step 4400, loss = 1.18 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 11:52:03.464042: step 4410, loss = 1.12 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:52:10.612461: step 4420, loss = 1.10 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 11:52:17.730101: step 4430, loss = 1.17 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 11:52:24.846388: step 4440, loss = 1.10 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 11:52:31.981089: step 4450, loss = 1.34 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:52:39.095025: step 4460, loss = 1.07 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:52:46.315058: step 4470, loss = 1.12 (42.2 examples/sec; 0.757 sec/batch)
2018-10-16 11:52:53.436115: step 4480, loss = 1.08 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 11:53:00.610348: step 4490, loss = 1.30 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 11:53:07.763339: step 4500, loss = 1.14 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 11:53:17.440176: step 4510, loss = 1.20 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 11:53:24.555083: step 4520, loss = 1.17 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 11:53:31.677985: step 4530, loss = 1.03 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 11:53:38.888038: step 4540, loss = 1.04 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 11:53:45.975243: step 4550, loss = 1.25 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 11:53:53.109327: step 4560, loss = 1.09 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 11:54:00.286064: step 4570, loss = 1.27 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 11:54:07.434074: step 4580, loss = 1.17 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 11:54:14.645968: step 4590, loss = 1.33 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:54:21.782428: step 4600, loss = 1.19 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 11:54:31.561809: step 4610, loss = 1.17 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:54:38.728527: step 4620, loss = 1.08 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 11:54:45.985959: step 4630, loss = 1.39 (42.6 examples/sec; 0.750 sec/batch)
2018-10-16 11:54:53.267046: step 4640, loss = 1.14 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 11:55:00.507717: step 4650, loss = 1.10 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 11:55:07.705562: step 4660, loss = 1.14 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 11:55:14.863827: step 4670, loss = 1.23 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 11:55:22.072907: step 4680, loss = 1.23 (43.2 examples/sec; 0.742 sec/batch)
2018-10-16 11:55:29.208085: step 4690, loss = 1.14 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 11:55:36.237770: step 4700, loss = 1.08 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 11:55:45.954384: step 4710, loss = 1.35 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 11:55:53.059230: step 4720, loss = 1.18 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 11:56:00.232863: step 4730, loss = 1.23 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 11:56:07.363771: step 4740, loss = 1.12 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 11:56:14.524035: step 4750, loss = 1.09 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 11:56:21.651019: step 4760, loss = 1.05 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 11:56:28.845731: step 4770, loss = 1.16 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 11:56:36.088648: step 4780, loss = 1.20 (43.7 examples/sec; 0.731 sec/batch)
2018-10-16 11:56:43.361131: step 4790, loss = 1.21 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 11:56:50.577544: step 4800, loss = 1.04 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 11:57:00.285756: step 4810, loss = 1.07 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 11:57:07.429577: step 4820, loss = 1.09 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 11:57:14.562960: step 4830, loss = 1.21 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 11:57:21.672430: step 4840, loss = 1.03 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 11:57:28.859226: step 4850, loss = 1.18 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 11:57:36.036458: step 4860, loss = 1.14 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:57:43.151794: step 4870, loss = 1.18 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 11:57:50.306975: step 4880, loss = 1.11 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 11:57:57.450058: step 4890, loss = 1.09 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 11:58:04.592360: step 4900, loss = 1.10 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 11:58:14.360575: step 4910, loss = 1.13 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 11:58:21.493958: step 4920, loss = 1.07 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 11:58:28.677269: step 4930, loss = 1.06 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 11:58:35.840894: step 4940, loss = 1.04 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 11:58:42.982133: step 4950, loss = 1.09 (46.1 examples/sec; 0.693 sec/batch)
2018-10-16 11:58:50.093500: step 4960, loss = 1.20 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 11:58:57.281356: step 4970, loss = 1.10 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 11:59:04.433223: step 4980, loss = 1.16 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 11:59:11.634584: step 4990, loss = 1.21 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 11:59:18.830488: step 5000, loss = 1.04 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 11:59:31.810787: step 5010, loss = 1.05 (47.1 examples/sec; 0.679 sec/batch)
2018-10-16 11:59:38.583951: step 5020, loss = 1.32 (47.8 examples/sec; 0.669 sec/batch)
2018-10-16 11:59:45.715618: step 5030, loss = 1.17 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 11:59:52.876374: step 5040, loss = 1.22 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 12:00:00.021385: step 5050, loss = 1.23 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 12:00:07.197427: step 5060, loss = 1.18 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 12:00:14.326386: step 5070, loss = 1.07 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:00:21.486145: step 5080, loss = 1.22 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:00:28.532334: step 5090, loss = 1.11 (46.9 examples/sec; 0.682 sec/batch)
2018-10-16 12:00:35.758258: step 5100, loss = 1.09 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 12:00:46.181858: step 5110, loss = 1.18 (47.9 examples/sec; 0.668 sec/batch)
2018-10-16 12:00:53.377913: step 5120, loss = 1.19 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 12:01:00.637773: step 5130, loss = 1.10 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 12:01:07.734989: step 5140, loss = 1.16 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 12:01:14.891881: step 5150, loss = 1.43 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 12:01:22.065840: step 5160, loss = 1.16 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 12:01:29.226420: step 5170, loss = 1.10 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:01:36.359563: step 5180, loss = 1.21 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 12:01:43.543923: step 5190, loss = 1.13 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 12:01:50.657998: step 5200, loss = 1.37 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 12:02:00.386896: step 5210, loss = 1.13 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 12:02:07.475356: step 5220, loss = 1.17 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 12:02:14.594247: step 5230, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:02:21.763057: step 5240, loss = 1.13 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 12:02:28.930919: step 5250, loss = 1.18 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 12:02:36.173917: step 5260, loss = 1.05 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 12:02:43.328090: step 5270, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 12:02:50.531666: step 5280, loss = 1.18 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 12:02:57.659661: step 5290, loss = 1.13 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:03:04.771957: step 5300, loss = 1.11 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:03:14.541288: step 5310, loss = 1.42 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 12:03:21.621762: step 5320, loss = 1.24 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:03:28.740507: step 5330, loss = 1.10 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 12:03:35.943351: step 5340, loss = 1.39 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 12:03:43.166228: step 5350, loss = 1.09 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 12:03:50.223777: step 5360, loss = 1.14 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 12:03:57.388295: step 5370, loss = 1.20 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 12:04:04.591801: step 5380, loss = 1.40 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 12:04:11.791026: step 5390, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 12:04:18.984676: step 5400, loss = 1.28 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 12:04:28.756020: step 5410, loss = 1.12 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 12:04:35.948200: step 5420, loss = 1.33 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 12:04:43.095202: step 5430, loss = 1.33 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 12:04:50.275824: step 5440, loss = 1.22 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 12:04:57.369438: step 5450, loss = 1.06 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 12:05:04.501693: step 5460, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 12:05:11.643753: step 5470, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:05:18.816768: step 5480, loss = 1.27 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 12:05:25.993415: step 5490, loss = 1.10 (44.4 examples/sec; 0.722 sec/batch)
2018-10-16 12:05:33.153765: step 5500, loss = 1.04 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 12:05:42.929885: step 5510, loss = 1.09 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 12:05:50.127698: step 5520, loss = 1.12 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 12:05:57.242297: step 5530, loss = 1.20 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 12:06:04.388476: step 5540, loss = 1.15 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 12:06:11.536951: step 5550, loss = 1.27 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 12:06:18.687257: step 5560, loss = 1.15 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:06:25.988228: step 5570, loss = 1.44 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 12:06:33.141986: step 5580, loss = 1.14 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 12:06:40.295195: step 5590, loss = 1.15 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 12:06:47.446539: step 5600, loss = 1.13 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 12:06:57.444169: step 5610, loss = 1.05 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:07:04.600906: step 5620, loss = 1.16 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 12:07:11.817201: step 5630, loss = 1.32 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 12:07:19.058011: step 5640, loss = 1.05 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 12:07:26.225375: step 5650, loss = 1.22 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 12:07:33.368418: step 5660, loss = 1.08 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 12:07:40.512772: step 5670, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:07:47.688166: step 5680, loss = 1.15 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 12:07:54.811222: step 5690, loss = 1.11 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 12:08:02.018070: step 5700, loss = 1.12 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 12:08:11.766195: step 5710, loss = 1.13 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 12:08:18.893376: step 5720, loss = 1.27 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 12:08:25.985171: step 5730, loss = 1.27 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:08:33.204821: step 5740, loss = 1.13 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 12:08:40.387657: step 5750, loss = 1.20 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 12:08:47.573894: step 5760, loss = 1.10 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 12:08:54.741653: step 5770, loss = 1.08 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 12:09:01.894233: step 5780, loss = 1.17 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 12:09:08.995804: step 5790, loss = 1.06 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 12:09:16.193569: step 5800, loss = 1.18 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 12:09:26.240617: step 5810, loss = 1.09 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 12:09:33.375498: step 5820, loss = 1.07 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 12:09:40.490999: step 5830, loss = 1.16 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 12:09:47.626217: step 5840, loss = 1.26 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 12:09:54.804794: step 5850, loss = 1.17 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 12:10:01.912663: step 5860, loss = 1.29 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 12:10:09.096597: step 5870, loss = 1.13 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 12:10:16.197403: step 5880, loss = 1.03 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 12:10:23.317634: step 5890, loss = 1.15 (46.6 examples/sec; 0.687 sec/batch)
2018-10-16 12:10:30.494786: step 5900, loss = 1.12 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 12:10:40.218960: step 5910, loss = 1.38 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 12:10:47.320749: step 5920, loss = 1.15 (47.1 examples/sec; 0.679 sec/batch)
2018-10-16 12:10:54.458811: step 5930, loss = 1.25 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 12:11:01.664600: step 5940, loss = 1.07 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 12:11:08.840471: step 5950, loss = 1.43 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 12:11:16.009713: step 5960, loss = 1.06 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 12:11:23.197403: step 5970, loss = 1.36 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 12:11:30.337958: step 5980, loss = 1.16 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 12:11:37.581036: step 5990, loss = 1.26 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 12:11:44.655256: step 6000, loss = 1.27 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 12:11:54.370018: step 6010, loss = 1.14 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 12:12:01.550764: step 6020, loss = 1.15 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 12:12:08.639042: step 6030, loss = 1.07 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 12:12:15.747478: step 6040, loss = 1.21 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 12:12:22.928059: step 6050, loss = 1.10 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 12:12:30.059280: step 6060, loss = 1.06 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 12:12:37.299185: step 6070, loss = 1.31 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 12:12:44.405458: step 6080, loss = 1.03 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 12:12:51.592887: step 6090, loss = 1.19 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 12:12:58.785996: step 6100, loss = 1.06 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 12:13:08.675192: step 6110, loss = 1.38 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:13:15.807326: step 6120, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 12:13:22.937267: step 6130, loss = 1.02 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 12:13:30.151450: step 6140, loss = 1.11 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:13:37.296788: step 6150, loss = 1.21 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 12:13:44.376530: step 6160, loss = 1.11 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 12:13:51.503247: step 6170, loss = 1.28 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 12:13:58.663152: step 6180, loss = 1.26 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 12:14:05.810450: step 6190, loss = 1.12 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:14:12.950837: step 6200, loss = 1.15 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 12:14:22.657923: step 6210, loss = 1.59 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 12:14:29.686071: step 6220, loss = 1.08 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 12:14:36.853424: step 6230, loss = 1.10 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 12:14:44.013579: step 6240, loss = 1.19 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:14:51.118530: step 6250, loss = 1.14 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 12:14:58.264827: step 6260, loss = 1.33 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:15:05.480679: step 6270, loss = 1.11 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 12:15:12.639584: step 6280, loss = 1.18 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 12:15:19.812198: step 6290, loss = 1.14 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:15:26.942091: step 6300, loss = 1.30 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 12:15:36.932517: step 6310, loss = 1.27 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 12:15:44.073990: step 6320, loss = 1.13 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 12:15:51.245485: step 6330, loss = 1.02 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 12:15:58.367817: step 6340, loss = 1.21 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 12:16:05.584262: step 6350, loss = 1.14 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 12:16:12.713828: step 6360, loss = 1.06 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:16:19.899067: step 6370, loss = 1.20 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 12:16:27.138240: step 6380, loss = 1.09 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 12:16:34.251622: step 6390, loss = 1.28 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 12:16:41.422842: step 6400, loss = 1.19 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 12:16:51.589015: step 6410, loss = 1.19 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 12:16:58.697544: step 6420, loss = 1.23 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:17:05.845988: step 6430, loss = 1.22 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 12:17:13.097211: step 6440, loss = 1.11 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 12:17:20.230428: step 6450, loss = 1.31 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 12:17:27.323994: step 6460, loss = 1.21 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:17:34.460015: step 6470, loss = 1.07 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 12:17:41.698426: step 6480, loss = 1.19 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 12:17:48.901870: step 6490, loss = 1.17 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 12:17:56.128494: step 6500, loss = 1.14 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 12:18:05.733533: step 6510, loss = 1.11 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 12:18:12.884124: step 6520, loss = 1.13 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 12:18:20.047053: step 6530, loss = 1.27 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 12:18:27.354591: step 6540, loss = 1.32 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 12:18:34.417669: step 6550, loss = 1.08 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 12:18:41.617518: step 6560, loss = 1.11 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 12:18:48.713139: step 6570, loss = 1.21 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 12:18:55.894828: step 6580, loss = 1.15 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:19:03.045021: step 6590, loss = 1.12 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:19:10.177519: step 6600, loss = 1.17 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 12:19:19.886787: step 6610, loss = 1.71 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 12:19:27.045501: step 6620, loss = 1.08 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 12:19:34.207679: step 6630, loss = 1.26 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 12:19:41.365618: step 6640, loss = 1.06 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 12:19:48.517499: step 6650, loss = 1.24 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:19:55.650464: step 6660, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 12:20:02.847202: step 6670, loss = 1.20 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 12:20:09.989138: step 6680, loss = 1.04 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:20:17.187695: step 6690, loss = 1.16 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:20:24.308752: step 6700, loss = 1.03 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 12:20:34.344248: step 6710, loss = 1.02 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 12:20:41.483223: step 6720, loss = 1.14 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 12:20:48.665083: step 6730, loss = 1.07 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 12:20:55.819764: step 6740, loss = 1.11 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 12:21:03.021984: step 6750, loss = 1.08 (43.2 examples/sec; 0.742 sec/batch)
2018-10-16 12:21:10.264343: step 6760, loss = 1.24 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 12:21:17.361333: step 6770, loss = 1.22 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 12:21:24.514103: step 6780, loss = 1.07 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 12:21:31.706179: step 6790, loss = 1.08 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 12:21:38.878908: step 6800, loss = 1.02 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 12:21:48.533460: step 6810, loss = 1.13 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 12:21:55.670803: step 6820, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 12:22:02.808945: step 6830, loss = 1.09 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 12:22:10.010759: step 6840, loss = 1.14 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:22:17.179118: step 6850, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 12:22:24.297179: step 6860, loss = 1.21 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 12:22:31.386257: step 6870, loss = 1.17 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:22:38.590289: step 6880, loss = 1.10 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 12:22:45.710302: step 6890, loss = 1.10 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 12:22:52.907046: step 6900, loss = 1.18 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 12:23:02.632482: step 6910, loss = 1.06 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 12:23:09.735697: step 6920, loss = 1.09 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:23:16.858034: step 6930, loss = 1.26 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 12:23:23.968125: step 6940, loss = 1.17 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 12:23:31.081090: step 6950, loss = 1.12 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 12:23:38.301584: step 6960, loss = 1.28 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 12:23:45.382208: step 6970, loss = 1.07 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 12:23:52.539372: step 6980, loss = 1.08 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 12:23:59.655177: step 6990, loss = 1.27 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 12:24:06.849559: step 7000, loss = 1.07 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 12:24:16.813639: step 7010, loss = 1.23 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 12:24:24.011575: step 7020, loss = 1.12 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 12:24:31.177720: step 7030, loss = 1.08 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 12:24:38.381654: step 7040, loss = 1.15 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 12:24:45.558613: step 7050, loss = 1.12 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 12:24:52.713022: step 7060, loss = 1.16 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:24:59.907058: step 7070, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:25:07.096905: step 7080, loss = 1.07 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 12:25:14.353206: step 7090, loss = 1.08 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:25:21.604755: step 7100, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 12:25:31.426733: step 7110, loss = 1.21 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:25:38.550813: step 7120, loss = 1.33 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 12:25:45.751896: step 7130, loss = 1.06 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 12:25:52.947967: step 7140, loss = 1.14 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 12:26:00.140461: step 7150, loss = 1.09 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 12:26:07.433435: step 7160, loss = 1.07 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 12:26:14.586694: step 7170, loss = 1.06 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 12:26:21.743665: step 7180, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 12:26:28.985211: step 7190, loss = 1.07 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 12:26:36.152672: step 7200, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:26:45.934172: step 7210, loss = 1.11 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 12:26:53.013885: step 7220, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 12:27:00.214147: step 7230, loss = 1.10 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 12:27:07.381436: step 7240, loss = 1.09 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 12:27:14.436058: step 7250, loss = 1.34 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 12:27:21.608098: step 7260, loss = 1.04 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 12:27:28.857689: step 7270, loss = 1.14 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 12:27:36.093620: step 7280, loss = 1.27 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 12:27:43.338718: step 7290, loss = 1.08 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 12:27:50.537341: step 7300, loss = 1.07 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:28:00.493573: step 7310, loss = 1.07 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 12:28:07.557387: step 7320, loss = 1.08 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 12:28:14.764325: step 7330, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:28:22.024375: step 7340, loss = 1.19 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 12:28:29.235169: step 7350, loss = 1.07 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:28:36.339477: step 7360, loss = 1.14 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 12:28:43.593622: step 7370, loss = 1.06 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 12:28:50.749658: step 7380, loss = 1.15 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:28:57.861649: step 7390, loss = 1.09 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 12:29:05.024879: step 7400, loss = 1.20 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:29:15.302111: step 7410, loss = 1.21 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 12:29:22.435121: step 7420, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 12:29:29.586904: step 7430, loss = 1.19 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 12:29:36.795005: step 7440, loss = 1.15 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:29:43.980376: step 7450, loss = 1.42 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 12:29:51.173001: step 7460, loss = 1.30 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 12:29:58.343567: step 7470, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:30:05.518410: step 7480, loss = 1.08 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 12:30:12.662359: step 7490, loss = 1.15 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:30:19.854411: step 7500, loss = 1.05 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 12:30:30.042070: step 7510, loss = 1.03 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 12:30:37.242981: step 7520, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 12:30:44.454328: step 7530, loss = 1.06 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 12:30:51.637331: step 7540, loss = 1.24 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 12:30:58.696212: step 7550, loss = 1.19 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 12:31:05.942886: step 7560, loss = 1.13 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 12:31:13.113806: step 7570, loss = 1.07 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 12:31:20.254010: step 7580, loss = 1.08 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:31:27.366363: step 7590, loss = 1.16 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 12:31:34.577789: step 7600, loss = 1.03 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 12:31:44.343551: step 7610, loss = 1.19 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 12:31:51.487947: step 7620, loss = 1.06 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 12:31:58.708028: step 7630, loss = 1.10 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 12:32:05.867694: step 7640, loss = 1.11 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:32:13.057370: step 7650, loss = 1.16 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 12:32:20.291893: step 7660, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 12:32:27.495373: step 7670, loss = 1.19 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:32:34.625112: step 7680, loss = 1.06 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 12:32:41.843796: step 7690, loss = 1.27 (43.7 examples/sec; 0.731 sec/batch)
2018-10-16 12:32:49.026893: step 7700, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 12:32:58.701431: step 7710, loss = 1.25 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 12:33:05.875394: step 7720, loss = 1.00 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 12:33:13.027877: step 7730, loss = 1.15 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 12:33:20.215910: step 7740, loss = 1.04 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 12:33:27.382472: step 7750, loss = 1.19 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 12:33:34.538137: step 7760, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 12:33:41.784963: step 7770, loss = 1.09 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 12:33:48.961015: step 7780, loss = 1.11 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 12:33:56.145567: step 7790, loss = 1.13 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 12:34:03.389854: step 7800, loss = 1.23 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 12:34:13.290507: step 7810, loss = 1.08 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 12:34:20.428843: step 7820, loss = 1.19 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 12:34:27.578861: step 7830, loss = 1.11 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 12:34:34.794266: step 7840, loss = 1.07 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 12:34:42.045408: step 7850, loss = 1.08 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 12:34:49.255832: step 7860, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 12:34:56.515283: step 7870, loss = 1.17 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 12:35:03.620287: step 7880, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 12:35:10.811913: step 7890, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:35:17.955409: step 7900, loss = 1.12 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 12:35:27.730735: step 7910, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 12:35:34.872686: step 7920, loss = 1.16 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 12:35:42.079185: step 7930, loss = 1.22 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 12:35:49.289066: step 7940, loss = 1.28 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 12:35:56.423410: step 7950, loss = 1.30 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:36:03.580754: step 7960, loss = 1.26 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 12:36:10.769440: step 7970, loss = 1.12 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 12:36:17.920360: step 7980, loss = 1.15 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 12:36:25.188739: step 7990, loss = 1.11 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 12:36:32.424681: step 8000, loss = 1.02 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 12:36:42.180465: step 8010, loss = 1.11 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:36:49.352442: step 8020, loss = 1.09 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 12:36:56.530264: step 8030, loss = 1.17 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 12:37:03.724406: step 8040, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 12:37:10.903403: step 8050, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:37:18.067452: step 8060, loss = 1.10 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 12:37:25.287218: step 8070, loss = 1.15 (43.7 examples/sec; 0.731 sec/batch)
2018-10-16 12:37:32.374782: step 8080, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:37:39.417604: step 8090, loss = 1.17 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 12:37:46.635985: step 8100, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:37:56.307017: step 8110, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 12:38:03.502604: step 8120, loss = 1.03 (43.0 examples/sec; 0.743 sec/batch)
2018-10-16 12:38:10.666401: step 8130, loss = 1.05 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 12:38:17.818235: step 8140, loss = 1.14 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 12:38:24.988578: step 8150, loss = 1.16 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 12:38:32.110590: step 8160, loss = 1.19 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:38:39.312103: step 8170, loss = 1.15 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 12:38:46.509285: step 8180, loss = 1.30 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 12:38:53.713185: step 8190, loss = 1.16 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 12:39:00.943331: step 8200, loss = 1.03 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 12:39:10.741109: step 8210, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 12:39:17.863235: step 8220, loss = 1.02 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 12:39:24.942242: step 8230, loss = 1.14 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 12:39:32.137097: step 8240, loss = 1.22 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 12:39:39.340007: step 8250, loss = 1.27 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 12:39:46.575922: step 8260, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 12:39:53.752658: step 8270, loss = 1.00 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 12:40:00.913462: step 8280, loss = 1.06 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:40:08.115335: step 8290, loss = 1.13 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 12:40:15.281474: step 8300, loss = 1.11 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:40:25.033609: step 8310, loss = 1.10 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:40:32.265919: step 8320, loss = 1.33 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 12:40:39.420578: step 8330, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 12:40:46.552212: step 8340, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 12:40:53.700725: step 8350, loss = 1.04 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 12:41:00.816415: step 8360, loss = 1.07 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 12:41:07.948227: step 8370, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 12:41:15.177922: step 8380, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 12:41:22.328306: step 8390, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 12:41:29.563072: step 8400, loss = 1.34 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 12:41:39.223492: step 8410, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 12:41:46.332280: step 8420, loss = 1.07 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 12:41:53.516651: step 8430, loss = 1.07 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 12:42:00.731045: step 8440, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:42:07.889028: step 8450, loss = 1.18 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 12:42:15.034757: step 8460, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 12:42:22.227484: step 8470, loss = 1.02 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 12:42:29.475165: step 8480, loss = 1.16 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 12:42:36.709598: step 8490, loss = 1.03 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 12:42:43.909868: step 8500, loss = 1.10 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 12:42:53.679242: step 8510, loss = 1.04 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:43:00.856912: step 8520, loss = 1.08 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:43:08.041596: step 8530, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:43:15.262533: step 8540, loss = 1.08 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 12:43:22.430498: step 8550, loss = 1.13 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 12:43:29.607368: step 8560, loss = 1.12 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 12:43:36.791956: step 8570, loss = 1.11 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 12:43:43.982982: step 8580, loss = 1.07 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 12:43:51.117460: step 8590, loss = 1.04 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 12:43:58.394794: step 8600, loss = 1.10 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 12:44:08.173629: step 8610, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:44:15.272454: step 8620, loss = 1.15 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 12:44:22.445642: step 8630, loss = 1.16 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:44:29.675685: step 8640, loss = 1.04 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 12:44:36.848465: step 8650, loss = 1.08 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 12:44:43.958820: step 8660, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:44:51.082463: step 8670, loss = 1.14 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:44:58.235189: step 8680, loss = 1.05 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 12:45:05.337948: step 8690, loss = 1.06 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 12:45:12.585027: step 8700, loss = 1.39 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 12:45:22.494035: step 8710, loss = 1.07 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 12:45:29.586864: step 8720, loss = 1.04 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 12:45:36.804087: step 8730, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:45:43.983480: step 8740, loss = 1.12 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 12:45:51.115269: step 8750, loss = 1.06 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 12:45:58.263167: step 8760, loss = 1.05 (44.0 examples/sec; 0.726 sec/batch)
2018-10-16 12:46:05.366946: step 8770, loss = 1.06 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 12:46:12.664385: step 8780, loss = 1.11 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 12:46:19.850667: step 8790, loss = 1.09 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 12:46:27.028570: step 8800, loss = 1.09 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 12:46:36.811078: step 8810, loss = 1.25 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 12:46:43.930701: step 8820, loss = 1.07 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 12:46:51.179483: step 8830, loss = 1.24 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 12:46:58.370005: step 8840, loss = 1.14 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 12:47:05.559188: step 8850, loss = 1.10 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 12:47:12.729720: step 8860, loss = 1.22 (42.4 examples/sec; 0.756 sec/batch)
2018-10-16 12:47:19.890733: step 8870, loss = 1.08 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 12:47:27.124168: step 8880, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:47:34.326703: step 8890, loss = 1.02 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 12:47:41.571963: step 8900, loss = 1.15 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 12:47:51.307159: step 8910, loss = 1.15 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 12:47:58.417793: step 8920, loss = 1.14 (46.9 examples/sec; 0.682 sec/batch)
2018-10-16 12:48:05.568610: step 8930, loss = 1.03 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 12:48:12.770822: step 8940, loss = 1.16 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 12:48:19.970708: step 8950, loss = 1.20 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:48:27.188428: step 8960, loss = 1.09 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 12:48:34.379110: step 8970, loss = 1.10 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:48:41.624268: step 8980, loss = 1.02 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 12:48:48.791617: step 8990, loss = 1.20 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 12:48:55.985155: step 9000, loss = 1.03 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 12:49:05.677632: step 9010, loss = 1.03 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 12:49:12.824013: step 9020, loss = 1.07 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 12:49:20.009529: step 9030, loss = 1.18 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:49:27.239721: step 9040, loss = 1.19 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 12:49:34.452337: step 9050, loss = 1.04 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 12:49:41.584826: step 9060, loss = 1.11 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 12:49:48.837325: step 9070, loss = 1.15 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 12:49:56.043991: step 9080, loss = 1.11 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 12:50:03.238904: step 9090, loss = 1.14 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:50:10.500894: step 9100, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 12:50:20.242629: step 9110, loss = 1.04 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 12:50:27.353448: step 9120, loss = 1.28 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 12:50:34.551522: step 9130, loss = 1.11 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 12:50:41.741483: step 9140, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 12:50:48.901658: step 9150, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 12:50:56.140389: step 9160, loss = 1.13 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 12:51:03.264367: step 9170, loss = 1.20 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 12:51:10.452426: step 9180, loss = 1.08 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 12:51:17.638486: step 9190, loss = 1.06 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 12:51:24.803413: step 9200, loss = 1.06 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:51:34.609937: step 9210, loss = 1.04 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 12:51:41.711654: step 9220, loss = 1.14 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 12:51:48.919480: step 9230, loss = 1.05 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 12:51:56.154642: step 9240, loss = 1.20 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:52:03.422369: step 9250, loss = 1.04 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 12:52:10.606171: step 9260, loss = 1.09 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 12:52:17.841078: step 9270, loss = 1.08 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 12:52:25.070732: step 9280, loss = 1.04 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 12:52:32.242648: step 9290, loss = 1.23 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 12:52:39.381514: step 9300, loss = 1.23 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 12:52:50.026790: step 9310, loss = 1.07 (46.6 examples/sec; 0.686 sec/batch)
2018-10-16 12:52:57.213566: step 9320, loss = 1.07 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 12:53:04.442476: step 9330, loss = 1.16 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 12:53:11.560068: step 9340, loss = 1.02 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 12:53:18.787673: step 9350, loss = 1.21 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 12:53:26.000650: step 9360, loss = 1.29 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 12:53:33.181957: step 9370, loss = 1.15 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 12:53:40.441331: step 9380, loss = 1.24 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 12:53:47.584202: step 9390, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 12:53:54.741297: step 9400, loss = 1.26 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:54:04.640253: step 9410, loss = 1.04 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 12:54:11.846983: step 9420, loss = 1.20 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 12:54:18.983576: step 9430, loss = 1.06 (46.4 examples/sec; 0.689 sec/batch)
2018-10-16 12:54:26.165549: step 9440, loss = 1.10 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 12:54:33.324798: step 9450, loss = 1.14 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 12:54:40.510991: step 9460, loss = 1.06 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 12:54:47.657800: step 9470, loss = 1.16 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 12:54:54.919135: step 9480, loss = 1.07 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 12:55:02.045847: step 9490, loss = 1.21 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 12:55:09.274593: step 9500, loss = 1.09 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 12:55:19.626663: step 9510, loss = 1.09 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 12:55:26.783916: step 9520, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 12:55:33.997435: step 9530, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:55:41.285097: step 9540, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 12:55:48.475327: step 9550, loss = 1.07 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 12:55:55.652364: step 9560, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 12:56:02.829851: step 9570, loss = 1.19 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 12:56:10.087979: step 9580, loss = 1.07 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 12:56:17.290929: step 9590, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:56:24.404583: step 9600, loss = 1.14 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 12:56:34.102010: step 9610, loss = 1.14 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:56:41.225887: step 9620, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 12:56:48.335744: step 9630, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 12:56:55.548991: step 9640, loss = 1.15 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 12:57:02.729898: step 9650, loss = 1.19 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 12:57:09.955011: step 9660, loss = 1.08 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 12:57:17.138278: step 9670, loss = 1.05 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 12:57:24.404193: step 9680, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 12:57:31.557176: step 9690, loss = 1.20 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 12:57:38.781791: step 9700, loss = 1.08 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 12:57:48.597362: step 9710, loss = 1.13 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 12:57:55.786685: step 9720, loss = 1.05 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 12:58:02.987596: step 9730, loss = 1.03 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 12:58:10.338569: step 9740, loss = 1.16 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 12:58:17.529895: step 9750, loss = 1.05 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 12:58:24.813374: step 9760, loss = 1.25 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 12:58:32.016566: step 9770, loss = 1.20 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 12:58:39.273938: step 9780, loss = 1.08 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 12:58:46.452797: step 9790, loss = 1.15 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 12:58:53.659437: step 9800, loss = 1.09 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 12:59:03.742239: step 9810, loss = 1.22 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 12:59:10.991224: step 9820, loss = 1.17 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 12:59:18.235949: step 9830, loss = 1.08 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 12:59:25.405962: step 9840, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 12:59:32.559694: step 9850, loss = 1.06 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 12:59:39.704341: step 9860, loss = 1.00 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 12:59:46.894394: step 9870, loss = 1.11 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 12:59:54.028597: step 9880, loss = 1.00 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 13:00:01.209978: step 9890, loss = 1.11 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 13:00:08.316038: step 9900, loss = 1.08 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 13:00:17.982243: step 9910, loss = 1.12 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 13:00:25.156264: step 9920, loss = 1.05 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 13:00:32.379820: step 9930, loss = 1.12 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 13:00:39.551614: step 9940, loss = 1.18 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 13:00:46.784279: step 9950, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 13:00:53.989512: step 9960, loss = 1.13 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 13:01:01.195715: step 9970, loss = 1.12 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 13:01:08.392757: step 9980, loss = 1.09 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 13:01:15.593029: step 9990, loss = 1.14 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 13:01:22.863592: step 10000, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 13:01:35.973154: step 10010, loss = 1.15 (47.4 examples/sec; 0.675 sec/batch)
2018-10-16 13:01:42.924437: step 10020, loss = 1.05 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 13:01:50.104664: step 10030, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 13:01:57.363496: step 10040, loss = 1.24 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 13:02:04.528173: step 10050, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 13:02:11.815920: step 10060, loss = 1.05 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 13:02:19.052709: step 10070, loss = 1.11 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 13:02:26.264427: step 10080, loss = 1.07 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 13:02:33.546377: step 10090, loss = 1.06 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 13:02:40.681204: step 10100, loss = 1.11 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 13:02:50.363479: step 10110, loss = 1.10 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 13:02:57.523018: step 10120, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:03:04.792466: step 10130, loss = 1.19 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 13:03:11.883171: step 10140, loss = 1.10 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 13:03:19.104603: step 10150, loss = 1.08 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 13:03:26.317583: step 10160, loss = 1.28 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 13:03:33.539396: step 10170, loss = 1.20 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 13:03:40.730400: step 10180, loss = 1.28 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 13:03:47.965039: step 10190, loss = 1.19 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 13:03:55.199152: step 10200, loss = 1.15 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 13:04:05.056950: step 10210, loss = 1.06 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 13:04:12.213502: step 10220, loss = 1.23 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 13:04:19.391811: step 10230, loss = 1.17 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 13:04:26.532005: step 10240, loss = 1.11 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:04:33.728721: step 10250, loss = 1.22 (42.3 examples/sec; 0.756 sec/batch)
2018-10-16 13:04:40.857777: step 10260, loss = 1.05 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 13:04:48.031284: step 10270, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 13:04:55.279065: step 10280, loss = 1.34 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 13:05:02.526358: step 10290, loss = 1.03 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 13:05:09.732765: step 10300, loss = 1.15 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:05:19.563237: step 10310, loss = 1.10 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 13:05:26.812197: step 10320, loss = 1.33 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 13:05:34.050977: step 10330, loss = 1.12 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 13:05:41.306309: step 10340, loss = 1.04 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 13:05:48.428661: step 10350, loss = 1.24 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 13:05:55.719801: step 10360, loss = 1.25 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 13:06:03.021600: step 10370, loss = 1.00 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 13:06:10.271556: step 10380, loss = 1.08 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 13:06:17.521653: step 10390, loss = 1.04 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 13:06:24.771359: step 10400, loss = 1.19 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 13:06:34.906278: step 10410, loss = 1.16 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 13:06:42.025281: step 10420, loss = 1.19 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 13:06:49.258599: step 10430, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 13:06:56.424878: step 10440, loss = 1.15 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:07:03.616933: step 10450, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 13:07:10.907627: step 10460, loss = 1.12 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 13:07:18.187834: step 10470, loss = 1.01 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 13:07:25.391892: step 10480, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 13:07:32.597903: step 10490, loss = 1.06 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 13:07:39.772622: step 10500, loss = 1.22 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 13:07:49.574392: step 10510, loss = 1.04 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 13:07:56.797879: step 10520, loss = 1.15 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 13:08:04.024110: step 10530, loss = 1.16 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 13:08:11.185823: step 10540, loss = 1.11 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:08:18.423232: step 10550, loss = 1.15 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 13:08:25.685576: step 10560, loss = 1.19 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 13:08:32.814212: step 10570, loss = 1.04 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 13:08:40.102131: step 10580, loss = 1.02 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 13:08:47.280521: step 10590, loss = 1.13 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 13:08:54.532493: step 10600, loss = 1.11 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 13:09:04.634215: step 10610, loss = 1.14 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 13:09:11.815445: step 10620, loss = 1.12 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 13:09:19.066231: step 10630, loss = 1.04 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 13:09:26.216830: step 10640, loss = 1.05 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 13:09:33.369336: step 10650, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 13:09:40.718982: step 10660, loss = 1.09 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 13:09:47.856102: step 10670, loss = 1.14 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 13:09:55.068052: step 10680, loss = 1.15 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 13:10:02.234724: step 10690, loss = 1.04 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 13:10:09.368163: step 10700, loss = 1.17 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:10:19.209240: step 10710, loss = 1.40 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 13:10:26.367015: step 10720, loss = 1.12 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 13:10:33.518044: step 10730, loss = 1.02 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 13:10:40.743430: step 10740, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:10:47.971653: step 10750, loss = 1.02 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 13:10:55.187430: step 10760, loss = 1.06 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 13:11:02.421937: step 10770, loss = 1.02 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 13:11:09.633826: step 10780, loss = 1.18 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:11:16.816135: step 10790, loss = 1.00 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 13:11:24.100343: step 10800, loss = 1.15 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 13:11:33.907434: step 10810, loss = 1.05 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 13:11:41.038356: step 10820, loss = 1.10 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:11:48.209885: step 10830, loss = 1.08 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 13:11:55.455664: step 10840, loss = 1.06 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 13:12:02.609336: step 10850, loss = 1.15 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 13:12:09.885602: step 10860, loss = 1.24 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 13:12:17.045003: step 10870, loss = 1.22 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 13:12:24.252583: step 10880, loss = 1.02 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 13:12:31.436725: step 10890, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 13:12:38.611925: step 10900, loss = 1.07 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 13:12:48.436657: step 10910, loss = 1.20 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 13:12:55.653465: step 10920, loss = 1.22 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 13:13:02.871649: step 10930, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:13:10.050586: step 10940, loss = 1.01 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 13:13:17.239256: step 10950, loss = 1.10 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:13:24.460915: step 10960, loss = 1.10 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 13:13:31.657212: step 10970, loss = 1.25 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 13:13:38.808650: step 10980, loss = 1.07 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 13:13:46.017220: step 10990, loss = 1.04 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 13:13:53.197983: step 11000, loss = 1.22 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:14:03.693054: step 11010, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 13:14:10.834808: step 11020, loss = 1.04 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 13:14:18.061484: step 11030, loss = 1.10 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 13:14:25.246657: step 11040, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 13:14:32.450490: step 11050, loss = 1.06 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 13:14:39.639368: step 11060, loss = 1.02 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 13:14:46.798806: step 11070, loss = 1.12 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:14:53.990561: step 11080, loss = 1.10 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 13:15:01.233376: step 11090, loss = 1.11 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 13:15:08.400119: step 11100, loss = 1.12 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:15:18.269000: step 11110, loss = 1.07 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 13:15:25.384918: step 11120, loss = 1.23 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 13:15:32.476471: step 11130, loss = 1.09 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:15:39.678060: step 11140, loss = 1.11 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 13:15:46.852344: step 11150, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 13:15:53.984314: step 11160, loss = 1.08 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 13:16:01.169391: step 11170, loss = 1.14 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 13:16:08.286678: step 11180, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:16:15.440772: step 11190, loss = 1.10 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 13:16:22.725497: step 11200, loss = 1.05 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 13:16:32.542179: step 11210, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 13:16:39.679763: step 11220, loss = 1.01 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 13:16:46.825050: step 11230, loss = 1.07 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:16:53.998058: step 11240, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 13:17:01.205546: step 11250, loss = 1.14 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 13:17:08.383509: step 11260, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:17:15.613582: step 11270, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:17:22.804840: step 11280, loss = 1.04 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 13:17:29.972300: step 11290, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 13:17:37.154402: step 11300, loss = 1.25 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 13:17:46.935050: step 11310, loss = 1.12 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 13:17:54.116389: step 11320, loss = 1.22 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 13:18:01.344661: step 11330, loss = 1.15 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 13:18:08.482813: step 11340, loss = 1.01 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 13:18:15.637623: step 11350, loss = 1.15 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 13:18:22.849695: step 11360, loss = 1.01 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 13:18:30.034239: step 11370, loss = 1.04 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 13:18:37.144883: step 11380, loss = 1.11 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 13:18:44.373496: step 11390, loss = 1.06 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 13:18:51.674071: step 11400, loss = 1.30 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 13:19:01.731768: step 11410, loss = 1.09 (46.9 examples/sec; 0.682 sec/batch)
2018-10-16 13:19:08.960136: step 11420, loss = 1.23 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 13:19:16.098593: step 11430, loss = 1.09 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:19:23.290073: step 11440, loss = 1.04 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 13:19:30.509726: step 11450, loss = 1.21 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 13:19:37.674398: step 11460, loss = 1.01 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 13:19:44.909363: step 11470, loss = 1.02 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 13:19:52.043764: step 11480, loss = 1.16 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 13:19:59.314199: step 11490, loss = 1.08 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 13:20:06.537373: step 11500, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:20:16.311692: step 11510, loss = 1.13 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 13:20:23.495285: step 11520, loss = 1.10 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 13:20:30.651037: step 11530, loss = 1.12 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:20:37.823447: step 11540, loss = 1.06 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 13:20:45.116481: step 11550, loss = 1.08 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 13:20:52.308265: step 11560, loss = 1.06 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 13:20:59.490851: step 11570, loss = 1.07 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 13:21:06.641088: step 11580, loss = 1.09 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 13:21:13.811033: step 11590, loss = 1.07 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 13:21:20.911431: step 11600, loss = 1.05 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 13:21:31.186269: step 11610, loss = 1.21 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 13:21:38.398153: step 11620, loss = 1.13 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:21:45.688437: step 11630, loss = 1.08 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 13:21:52.876370: step 11640, loss = 1.08 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 13:22:00.175580: step 11650, loss = 1.13 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 13:22:07.320623: step 11660, loss = 1.12 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 13:22:14.514873: step 11670, loss = 1.21 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 13:22:21.699912: step 11680, loss = 1.10 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 13:22:28.967740: step 11690, loss = 1.13 (42.2 examples/sec; 0.758 sec/batch)
2018-10-16 13:22:36.224474: step 11700, loss = 1.07 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 13:22:45.969494: step 11710, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 13:22:53.105120: step 11720, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 13:23:00.314372: step 11730, loss = 1.06 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 13:23:07.484300: step 11740, loss = 1.07 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 13:23:14.721359: step 11750, loss = 1.16 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 13:23:21.931848: step 11760, loss = 1.13 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 13:23:29.060964: step 11770, loss = 1.23 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 13:23:36.386152: step 11780, loss = 1.01 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 13:23:43.537686: step 11790, loss = 1.28 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 13:23:50.703023: step 11800, loss = 1.13 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 13:24:00.605181: step 11810, loss = 1.14 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 13:24:07.799017: step 11820, loss = 1.06 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 13:24:14.945253: step 11830, loss = 1.15 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 13:24:22.024619: step 11840, loss = 1.12 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 13:24:29.247962: step 11850, loss = 1.11 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 13:24:36.506597: step 11860, loss = 1.03 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 13:24:43.706595: step 11870, loss = 1.14 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 13:24:50.999122: step 11880, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 13:24:58.195955: step 11890, loss = 1.00 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 13:25:05.398607: step 11900, loss = 1.12 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 13:25:15.132837: step 11910, loss = 1.19 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 13:25:22.262620: step 11920, loss = 1.11 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 13:25:29.490241: step 11930, loss = 1.24 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 13:25:36.730024: step 11940, loss = 1.11 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 13:25:43.945312: step 11950, loss = 1.22 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:25:51.164236: step 11960, loss = 1.11 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 13:25:58.353918: step 11970, loss = 1.12 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 13:26:05.518414: step 11980, loss = 1.08 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:26:12.711952: step 11990, loss = 1.08 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 13:26:19.861656: step 12000, loss = 1.05 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 13:26:29.555746: step 12010, loss = 1.07 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 13:26:36.689608: step 12020, loss = 1.04 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 13:26:43.939706: step 12030, loss = 1.04 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 13:26:51.122178: step 12040, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 13:26:58.318444: step 12050, loss = 1.12 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 13:27:05.513448: step 12060, loss = 1.12 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 13:27:12.651267: step 12070, loss = 1.01 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 13:27:19.877205: step 12080, loss = 1.06 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 13:27:27.081285: step 12090, loss = 1.11 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 13:27:34.301968: step 12100, loss = 1.01 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 13:27:44.070179: step 12110, loss = 1.06 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 13:27:51.251790: step 12120, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 13:27:58.360830: step 12130, loss = 1.04 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 13:28:05.581835: step 12140, loss = 1.06 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 13:28:12.735616: step 12150, loss = 1.03 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 13:28:19.974227: step 12160, loss = 1.02 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 13:28:27.118917: step 12170, loss = 1.10 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 13:28:34.313263: step 12180, loss = 1.01 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 13:28:41.485994: step 12190, loss = 1.02 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 13:28:48.746532: step 12200, loss = 1.05 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:28:58.456252: step 12210, loss = 1.09 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 13:29:05.579110: step 12220, loss = 1.03 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 13:29:12.815431: step 12230, loss = 1.09 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 13:29:19.947152: step 12240, loss = 1.13 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 13:29:27.161159: step 12250, loss = 1.18 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 13:29:34.254781: step 12260, loss = 1.05 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 13:29:41.453469: step 12270, loss = 1.12 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 13:29:48.623563: step 12280, loss = 1.08 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 13:29:55.863387: step 12290, loss = 1.09 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 13:30:03.001034: step 12300, loss = 1.15 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 13:30:12.948206: step 12310, loss = 1.09 (47.2 examples/sec; 0.678 sec/batch)
2018-10-16 13:30:20.107450: step 12320, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 13:30:27.309391: step 12330, loss = 1.08 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 13:30:34.537635: step 12340, loss = 1.06 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 13:30:41.742722: step 12350, loss = 1.21 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 13:30:48.877026: step 12360, loss = 1.08 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 13:30:56.104080: step 12370, loss = 1.03 (43.2 examples/sec; 0.742 sec/batch)
2018-10-16 13:31:03.358020: step 12380, loss = 1.11 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 13:31:10.541154: step 12390, loss = 1.10 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 13:31:17.775501: step 12400, loss = 1.04 (43.0 examples/sec; 0.743 sec/batch)
2018-10-16 13:31:27.722869: step 12410, loss = 1.06 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 13:31:34.883670: step 12420, loss = 1.00 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 13:31:42.028164: step 12430, loss = 1.03 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 13:31:49.231372: step 12440, loss = 1.01 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 13:31:56.430340: step 12450, loss = 1.05 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 13:32:03.649133: step 12460, loss = 1.11 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 13:32:10.758848: step 12470, loss = 1.13 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 13:32:18.039543: step 12480, loss = 1.04 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 13:32:25.279390: step 12490, loss = 1.06 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 13:32:32.544760: step 12500, loss = 1.11 (42.9 examples/sec; 0.747 sec/batch)
2018-10-16 13:32:42.284890: step 12510, loss = 1.04 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 13:32:49.441004: step 12520, loss = 1.10 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 13:32:56.615139: step 12530, loss = 1.17 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 13:33:03.747319: step 12540, loss = 1.08 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 13:33:10.912591: step 12550, loss = 1.13 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 13:33:18.157401: step 12560, loss = 1.10 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 13:33:25.349746: step 12570, loss = 1.06 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:33:32.508204: step 12580, loss = 1.06 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 13:33:39.747881: step 12590, loss = 1.03 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 13:33:46.906118: step 12600, loss = 1.05 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 13:33:56.667599: step 12610, loss = 1.09 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 13:34:03.810144: step 12620, loss = 1.02 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 13:34:10.987456: step 12630, loss = 1.09 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 13:34:18.160393: step 12640, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 13:34:25.351065: step 12650, loss = 1.17 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 13:34:32.518282: step 12660, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 13:34:39.647812: step 12670, loss = 1.16 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 13:34:46.863167: step 12680, loss = 1.04 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 13:34:54.040664: step 12690, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 13:35:01.207601: step 12700, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 13:35:10.855430: step 12710, loss = 1.05 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 13:35:18.050894: step 12720, loss = 1.14 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:35:25.264094: step 12730, loss = 1.03 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 13:35:32.457822: step 12740, loss = 1.17 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 13:35:39.596290: step 12750, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:35:46.744654: step 12760, loss = 1.09 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 13:35:53.956625: step 12770, loss = 1.05 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 13:36:01.163581: step 12780, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 13:36:08.323872: step 12790, loss = 1.01 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 13:36:15.505455: step 12800, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:36:25.190158: step 12810, loss = 1.28 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 13:36:32.292659: step 12820, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 13:36:39.433590: step 12830, loss = 1.12 (44.0 examples/sec; 0.726 sec/batch)
2018-10-16 13:36:46.664498: step 12840, loss = 1.06 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 13:36:53.868050: step 12850, loss = 1.03 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 13:37:01.076929: step 12860, loss = 0.99 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 13:37:08.272982: step 12870, loss = 1.14 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 13:37:15.490178: step 12880, loss = 1.10 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 13:37:22.793114: step 12890, loss = 1.05 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 13:37:29.989571: step 12900, loss = 1.05 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 13:37:39.925916: step 12910, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 13:37:47.134376: step 12920, loss = 1.29 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 13:37:54.379810: step 12930, loss = 1.03 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 13:38:01.551346: step 12940, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 13:38:08.741330: step 12950, loss = 1.07 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 13:38:15.886809: step 12960, loss = 1.10 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 13:38:23.019203: step 12970, loss = 1.01 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 13:38:30.184911: step 12980, loss = 1.06 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 13:38:37.437408: step 12990, loss = 1.03 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 13:38:44.695779: step 13000, loss = 1.08 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 13:38:54.347239: step 13010, loss = 1.09 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 13:39:01.576724: step 13020, loss = 1.06 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 13:39:08.700169: step 13030, loss = 1.13 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 13:39:15.930552: step 13040, loss = 1.07 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 13:39:23.091513: step 13050, loss = 1.15 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 13:39:30.317107: step 13060, loss = 1.20 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 13:39:37.504001: step 13070, loss = 1.09 (46.6 examples/sec; 0.687 sec/batch)
2018-10-16 13:39:44.722287: step 13080, loss = 1.04 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 13:39:51.846069: step 13090, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 13:39:59.104321: step 13100, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 13:40:09.800603: step 13110, loss = 1.13 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 13:40:16.977931: step 13120, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:40:24.174158: step 13130, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 13:40:31.342560: step 13140, loss = 1.09 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 13:40:38.513887: step 13150, loss = 1.06 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 13:40:45.770666: step 13160, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:40:53.000978: step 13170, loss = 1.01 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 13:41:00.215852: step 13180, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:41:07.436263: step 13190, loss = 1.06 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 13:41:14.628018: step 13200, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:41:24.754456: step 13210, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 13:41:31.959366: step 13220, loss = 1.00 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 13:41:39.240318: step 13230, loss = 1.18 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 13:41:46.374648: step 13240, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:41:53.480430: step 13250, loss = 1.03 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 13:42:00.676152: step 13260, loss = 1.02 (43.2 examples/sec; 0.742 sec/batch)
2018-10-16 13:42:07.830802: step 13270, loss = 1.10 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 13:42:15.052857: step 13280, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 13:42:22.199662: step 13290, loss = 1.02 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 13:42:29.418713: step 13300, loss = 1.04 (42.4 examples/sec; 0.754 sec/batch)
2018-10-16 13:42:39.220151: step 13310, loss = 1.15 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 13:42:46.454184: step 13320, loss = 1.07 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 13:42:53.709552: step 13330, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 13:43:00.881819: step 13340, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 13:43:08.118369: step 13350, loss = 1.14 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 13:43:15.301734: step 13360, loss = 1.19 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:43:22.500195: step 13370, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 13:43:29.658094: step 13380, loss = 1.02 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 13:43:36.868808: step 13390, loss = 1.14 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 13:43:44.074105: step 13400, loss = 1.14 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 13:43:53.871224: step 13410, loss = 1.08 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 13:44:01.009824: step 13420, loss = 1.07 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 13:44:08.273232: step 13430, loss = 1.12 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 13:44:15.537977: step 13440, loss = 1.16 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 13:44:22.802398: step 13450, loss = 1.09 (45.9 examples/sec; 0.696 sec/batch)
2018-10-16 13:44:30.018856: step 13460, loss = 1.06 (42.5 examples/sec; 0.752 sec/batch)
2018-10-16 13:44:37.180522: step 13470, loss = 1.20 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 13:44:44.363590: step 13480, loss = 1.04 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 13:44:51.589951: step 13490, loss = 1.12 (46.6 examples/sec; 0.687 sec/batch)
2018-10-16 13:44:58.839120: step 13500, loss = 1.02 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 13:45:08.612842: step 13510, loss = 1.00 (45.9 examples/sec; 0.696 sec/batch)
2018-10-16 13:45:15.841098: step 13520, loss = 1.12 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 13:45:23.029247: step 13530, loss = 1.18 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 13:45:30.256428: step 13540, loss = 1.18 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 13:45:37.456342: step 13550, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 13:45:44.720042: step 13560, loss = 1.03 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 13:45:51.956309: step 13570, loss = 1.02 (42.5 examples/sec; 0.752 sec/batch)
2018-10-16 13:45:59.065915: step 13580, loss = 1.22 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 13:46:06.263624: step 13590, loss = 1.11 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 13:46:13.457957: step 13600, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 13:46:23.528804: step 13610, loss = 1.06 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 13:46:30.700761: step 13620, loss = 0.99 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 13:46:37.815860: step 13630, loss = 1.18 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:46:45.102594: step 13640, loss = 1.02 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 13:46:52.375102: step 13650, loss = 1.04 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 13:46:59.517241: step 13660, loss = 1.10 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 13:47:06.753617: step 13670, loss = 1.09 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:47:14.011439: step 13680, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 13:47:21.259879: step 13690, loss = 1.15 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 13:47:28.454297: step 13700, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 13:47:38.285977: step 13710, loss = 1.32 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 13:47:45.405968: step 13720, loss = 1.18 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 13:47:52.600011: step 13730, loss = 1.14 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 13:47:59.839705: step 13740, loss = 1.09 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 13:48:07.018244: step 13750, loss = 1.07 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:48:14.207584: step 13760, loss = 1.06 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 13:48:21.369002: step 13770, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 13:48:28.548084: step 13780, loss = 1.09 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 13:48:35.842089: step 13790, loss = 1.09 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 13:48:42.997914: step 13800, loss = 1.03 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 13:48:53.291612: step 13810, loss = 1.02 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 13:49:00.545814: step 13820, loss = 1.08 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 13:49:07.754673: step 13830, loss = 1.04 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 13:49:15.012430: step 13840, loss = 1.08 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 13:49:22.222236: step 13850, loss = 1.23 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 13:49:29.365547: step 13860, loss = 1.09 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 13:49:36.575260: step 13870, loss = 1.10 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 13:49:43.859201: step 13880, loss = 1.07 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 13:49:51.078137: step 13890, loss = 1.18 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:49:58.246421: step 13900, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 13:50:07.976270: step 13910, loss = 1.00 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 13:50:15.143231: step 13920, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 13:50:22.353006: step 13930, loss = 1.06 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 13:50:29.507184: step 13940, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 13:50:36.669704: step 13950, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 13:50:43.851959: step 13960, loss = 1.01 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 13:50:51.086947: step 13970, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 13:50:58.303730: step 13980, loss = 1.21 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 13:51:05.532527: step 13990, loss = 1.22 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 13:51:12.705577: step 14000, loss = 1.12 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 13:51:22.452570: step 14010, loss = 1.01 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 13:51:29.685857: step 14020, loss = 0.99 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 13:51:36.872302: step 14030, loss = 1.14 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 13:51:43.990422: step 14040, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 13:51:51.198556: step 14050, loss = 1.07 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 13:51:58.356136: step 14060, loss = 1.03 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 13:52:05.497438: step 14070, loss = 1.04 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 13:52:12.752950: step 14080, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 13:52:19.939420: step 14090, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 13:52:27.214312: step 14100, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 13:52:37.179499: step 14110, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 13:52:44.418393: step 14120, loss = 1.02 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 13:52:51.673009: step 14130, loss = 1.00 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 13:52:58.881182: step 14140, loss = 1.10 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 13:53:06.045189: step 14150, loss = 1.13 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:53:13.214112: step 14160, loss = 1.13 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 13:53:20.330308: step 14170, loss = 1.27 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 13:53:27.534894: step 14180, loss = 1.00 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 13:53:34.782733: step 14190, loss = 1.09 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 13:53:41.972132: step 14200, loss = 1.15 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 13:53:51.860861: step 14210, loss = 1.23 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 13:53:58.995713: step 14220, loss = 1.10 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 13:54:06.226570: step 14230, loss = 1.09 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 13:54:13.375980: step 14240, loss = 1.11 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 13:54:20.506533: step 14250, loss = 1.05 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 13:54:27.730360: step 14260, loss = 1.04 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 13:54:34.939060: step 14270, loss = 1.22 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 13:54:42.173753: step 14280, loss = 1.11 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 13:54:49.326222: step 14290, loss = 1.02 (43.0 examples/sec; 0.743 sec/batch)
2018-10-16 13:54:56.547800: step 14300, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 13:55:06.270356: step 14310, loss = 1.08 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 13:55:13.546730: step 14320, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 13:55:20.691689: step 14330, loss = 1.02 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 13:55:27.856102: step 14340, loss = 1.04 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 13:55:35.031005: step 14350, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 13:55:42.200827: step 14360, loss = 1.12 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 13:55:49.438759: step 14370, loss = 1.05 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 13:55:56.696336: step 14380, loss = 1.08 (44.4 examples/sec; 0.722 sec/batch)
2018-10-16 13:56:03.792770: step 14390, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 13:56:10.937281: step 14400, loss = 1.13 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 13:56:20.819113: step 14410, loss = 1.06 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 13:56:27.933367: step 14420, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 13:56:35.224246: step 14430, loss = 1.07 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 13:56:42.454822: step 14440, loss = 1.11 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 13:56:49.558543: step 14450, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 13:56:56.766276: step 14460, loss = 1.06 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 13:57:03.991736: step 14470, loss = 1.01 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 13:57:11.202208: step 14480, loss = 1.18 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 13:57:18.364692: step 14490, loss = 1.02 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 13:57:25.576135: step 14500, loss = 1.08 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 13:57:35.332310: step 14510, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:57:42.529640: step 14520, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 13:57:49.759168: step 14530, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 13:57:56.990710: step 14540, loss = 1.07 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 13:58:04.200686: step 14550, loss = 1.05 (42.9 examples/sec; 0.747 sec/batch)
2018-10-16 13:58:11.361224: step 14560, loss = 0.99 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 13:58:18.591302: step 14570, loss = 1.03 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 13:58:25.863256: step 14580, loss = 1.08 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 13:58:33.126825: step 14590, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 13:58:40.212743: step 14600, loss = 1.04 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 13:58:49.874549: step 14610, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 13:58:57.088401: step 14620, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 13:59:04.261560: step 14630, loss = 1.05 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 13:59:11.461685: step 14640, loss = 1.07 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 13:59:18.647819: step 14650, loss = 1.12 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 13:59:25.784693: step 14660, loss = 1.05 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 13:59:32.991016: step 14670, loss = 1.05 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 13:59:40.193519: step 14680, loss = 1.00 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 13:59:47.401559: step 14690, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 13:59:54.535020: step 14700, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 14:00:04.252384: step 14710, loss = 1.01 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 14:00:11.495606: step 14720, loss = 1.22 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 14:00:18.716389: step 14730, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:00:25.986096: step 14740, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:00:33.247659: step 14750, loss = 1.00 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 14:00:40.444606: step 14760, loss = 1.07 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 14:00:47.716049: step 14770, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:00:54.936097: step 14780, loss = 1.07 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 14:01:02.163671: step 14790, loss = 1.01 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 14:01:09.427535: step 14800, loss = 1.13 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 14:01:19.195154: step 14810, loss = 1.06 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 14:01:26.380696: step 14820, loss = 1.20 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 14:01:33.598216: step 14830, loss = 1.18 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 14:01:40.754397: step 14840, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:01:47.984122: step 14850, loss = 1.15 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:01:55.178614: step 14860, loss = 1.04 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 14:02:02.405675: step 14870, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 14:02:09.677596: step 14880, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 14:02:16.940134: step 14890, loss = 1.10 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:02:24.189700: step 14900, loss = 1.01 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 14:02:33.924081: step 14910, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 14:02:41.052881: step 14920, loss = 1.11 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 14:02:48.310089: step 14930, loss = 1.02 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 14:02:55.534563: step 14940, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:03:02.713344: step 14950, loss = 1.14 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 14:03:09.946279: step 14960, loss = 1.00 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 14:03:17.128286: step 14970, loss = 1.01 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 14:03:24.336303: step 14980, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 14:03:31.571637: step 14990, loss = 1.18 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:03:38.772561: step 15000, loss = 1.12 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 14:03:51.927495: step 15010, loss = 1.01 (47.1 examples/sec; 0.679 sec/batch)
2018-10-16 14:03:58.940959: step 15020, loss = 1.11 (42.5 examples/sec; 0.753 sec/batch)
2018-10-16 14:04:06.227577: step 15030, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 14:04:13.494624: step 15040, loss = 1.03 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 14:04:20.748667: step 15050, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:04:27.934217: step 15060, loss = 1.07 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 14:04:35.159285: step 15070, loss = 1.23 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:04:42.340015: step 15080, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 14:04:49.652346: step 15090, loss = 1.11 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 14:04:56.951573: step 15100, loss = 1.09 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:05:06.765563: step 15110, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:05:13.885625: step 15120, loss = 1.10 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 14:05:21.169382: step 15130, loss = 1.18 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 14:05:28.345248: step 15140, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:05:35.505655: step 15150, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 14:05:42.646570: step 15160, loss = 1.24 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:05:49.847491: step 15170, loss = 1.02 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 14:05:57.122705: step 15180, loss = 1.03 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 14:06:04.302165: step 15190, loss = 1.06 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 14:06:11.443771: step 15200, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:06:21.219613: step 15210, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 14:06:28.414305: step 15220, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 14:06:35.592192: step 15230, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 14:06:42.837863: step 15240, loss = 1.20 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 14:06:50.058821: step 15250, loss = 1.25 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 14:06:57.209623: step 15260, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 14:07:04.432258: step 15270, loss = 1.06 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 14:07:11.688225: step 15280, loss = 1.07 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 14:07:18.932895: step 15290, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 14:07:26.098615: step 15300, loss = 1.09 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 14:07:36.254063: step 15310, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 14:07:43.429619: step 15320, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:07:50.539774: step 15330, loss = 1.02 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 14:07:57.731317: step 15340, loss = 1.03 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 14:08:04.886157: step 15350, loss = 1.05 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:08:12.019278: step 15360, loss = 1.25 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 14:08:19.257177: step 15370, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 14:08:26.473565: step 15380, loss = 1.26 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 14:08:33.754893: step 15390, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:08:40.981832: step 15400, loss = 1.18 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 14:08:50.653870: step 15410, loss = 1.01 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 14:08:57.921023: step 15420, loss = 1.05 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 14:09:05.067838: step 15430, loss = 1.07 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 14:09:12.340392: step 15440, loss = 1.06 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:09:19.543564: step 15450, loss = 1.03 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 14:09:26.718696: step 15460, loss = 1.30 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 14:09:33.862200: step 15470, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:09:41.219068: step 15480, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:09:48.428567: step 15490, loss = 1.01 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 14:09:55.631060: step 15500, loss = 1.15 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 14:10:05.335611: step 15510, loss = 1.21 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 14:10:12.519965: step 15520, loss = 1.01 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 14:10:19.714233: step 15530, loss = 1.02 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 14:10:27.003454: step 15540, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 14:10:34.191535: step 15550, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:10:41.364034: step 15560, loss = 1.08 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 14:10:48.561193: step 15570, loss = 1.14 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 14:10:55.768250: step 15580, loss = 1.11 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 14:11:02.931107: step 15590, loss = 1.13 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 14:11:10.299132: step 15600, loss = 1.17 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 14:11:20.128216: step 15610, loss = 1.13 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 14:11:27.329391: step 15620, loss = 1.04 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 14:11:34.569740: step 15630, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 14:11:41.833217: step 15640, loss = 1.13 (42.2 examples/sec; 0.759 sec/batch)
2018-10-16 14:11:48.993707: step 15650, loss = 1.06 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 14:11:56.228568: step 15660, loss = 1.09 (42.5 examples/sec; 0.753 sec/batch)
2018-10-16 14:12:03.418169: step 15670, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:12:10.747250: step 15680, loss = 1.08 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 14:12:17.966074: step 15690, loss = 1.09 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 14:12:25.078431: step 15700, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 14:12:34.906594: step 15710, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:12:42.062164: step 15720, loss = 0.99 (46.6 examples/sec; 0.687 sec/batch)
2018-10-16 14:12:49.128160: step 15730, loss = 1.05 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 14:12:56.319137: step 15740, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 14:13:03.509108: step 15750, loss = 1.08 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 14:13:10.824863: step 15760, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 14:13:18.048725: step 15770, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 14:13:25.268337: step 15780, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 14:13:32.566812: step 15790, loss = 1.08 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 14:13:39.776894: step 15800, loss = 1.16 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:13:50.206634: step 15810, loss = 1.23 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 14:13:57.321877: step 15820, loss = 1.03 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 14:14:04.566494: step 15830, loss = 1.05 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 14:14:11.820955: step 15840, loss = 1.26 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 14:14:19.048124: step 15850, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:14:26.263886: step 15860, loss = 1.01 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 14:14:33.431861: step 15870, loss = 1.02 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 14:14:40.627803: step 15880, loss = 1.03 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 14:14:47.801092: step 15890, loss = 1.19 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:14:55.007373: step 15900, loss = 1.02 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 14:15:04.757499: step 15910, loss = 1.10 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 14:15:11.930977: step 15920, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:15:19.112689: step 15930, loss = 1.08 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 14:15:26.347694: step 15940, loss = 1.11 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 14:15:33.541138: step 15950, loss = 1.02 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 14:15:40.810003: step 15960, loss = 1.01 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 14:15:48.032045: step 15970, loss = 1.02 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 14:15:55.331382: step 15980, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 14:16:02.550978: step 15990, loss = 1.09 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:16:09.792118: step 16000, loss = 1.13 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 14:16:19.556278: step 16010, loss = 1.18 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 14:16:26.652241: step 16020, loss = 1.12 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:16:33.842801: step 16030, loss = 1.02 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:16:41.091282: step 16040, loss = 1.29 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 14:16:48.291425: step 16050, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 14:16:55.530000: step 16060, loss = 1.03 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 14:17:02.727452: step 16070, loss = 1.14 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:17:09.902326: step 16080, loss = 1.12 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:17:17.133258: step 16090, loss = 1.26 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 14:17:24.334268: step 16100, loss = 1.11 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 14:17:34.114499: step 16110, loss = 1.04 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 14:17:41.405829: step 16120, loss = 1.00 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 14:17:48.554719: step 16130, loss = 1.05 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 14:17:55.775978: step 16140, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:18:03.054494: step 16150, loss = 1.11 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 14:18:10.258686: step 16160, loss = 1.04 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 14:18:17.522902: step 16170, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:18:24.654922: step 16180, loss = 1.08 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:18:31.919391: step 16190, loss = 1.18 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 14:18:39.058037: step 16200, loss = 1.19 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 14:18:49.093043: step 16210, loss = 1.01 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 14:18:56.262044: step 16220, loss = 1.05 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 14:19:03.494054: step 16230, loss = 1.19 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 14:19:10.778416: step 16240, loss = 1.02 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 14:19:18.011027: step 16250, loss = 1.05 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 14:19:25.262531: step 16260, loss = 1.02 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 14:19:32.493907: step 16270, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 14:19:39.702238: step 16280, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 14:19:46.904620: step 16290, loss = 1.18 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 14:19:54.063782: step 16300, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:20:03.906889: step 16310, loss = 1.12 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 14:20:11.182022: step 16320, loss = 1.02 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 14:20:18.418856: step 16330, loss = 1.04 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 14:20:25.606753: step 16340, loss = 1.03 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 14:20:32.898693: step 16350, loss = 1.01 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 14:20:40.148822: step 16360, loss = 1.08 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 14:20:47.387383: step 16370, loss = 1.10 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 14:20:54.572040: step 16380, loss = 1.09 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:21:01.772109: step 16390, loss = 1.05 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 14:21:09.006381: step 16400, loss = 1.03 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 14:21:18.855477: step 16410, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:21:25.997876: step 16420, loss = 1.05 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 14:21:33.265566: step 16430, loss = 1.13 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:21:40.414201: step 16440, loss = 1.21 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 14:21:47.607662: step 16450, loss = 1.11 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 14:21:54.888604: step 16460, loss = 1.01 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 14:22:02.123260: step 16470, loss = 1.08 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 14:22:09.326245: step 16480, loss = 1.04 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 14:22:16.469531: step 16490, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 14:22:23.694205: step 16500, loss = 1.01 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 14:22:33.790919: step 16510, loss = 1.16 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:22:40.985890: step 16520, loss = 1.09 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 14:22:48.129576: step 16530, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 14:22:55.413582: step 16540, loss = 1.02 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:23:02.589745: step 16550, loss = 1.18 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 14:23:09.835798: step 16560, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 14:23:17.043593: step 16570, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:23:24.176911: step 16580, loss = 1.39 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 14:23:31.376028: step 16590, loss = 1.07 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 14:23:38.634275: step 16600, loss = 1.04 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 14:23:48.497626: step 16610, loss = 1.06 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:23:55.616642: step 16620, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 14:24:02.877678: step 16630, loss = 1.13 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 14:24:10.150125: step 16640, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:24:17.290272: step 16650, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 14:24:24.564239: step 16660, loss = 1.12 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 14:24:31.721559: step 16670, loss = 1.13 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 14:24:38.930178: step 16680, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 14:24:46.212283: step 16690, loss = 1.03 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 14:24:53.405934: step 16700, loss = 1.07 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 14:25:03.153779: step 16710, loss = 1.12 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 14:25:10.353092: step 16720, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 14:25:17.497547: step 16730, loss = 1.28 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 14:25:24.619296: step 16740, loss = 1.05 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 14:25:31.762682: step 16750, loss = 1.10 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:25:38.889797: step 16760, loss = 1.22 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 14:25:46.138065: step 16770, loss = 1.04 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 14:25:53.273938: step 16780, loss = 1.00 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 14:26:00.535969: step 16790, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 14:26:07.770663: step 16800, loss = 1.06 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:26:17.546632: step 16810, loss = 1.01 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 14:26:24.755880: step 16820, loss = 1.03 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 14:26:31.945983: step 16830, loss = 1.18 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 14:26:39.120699: step 16840, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 14:26:46.309436: step 16850, loss = 1.11 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 14:26:53.551045: step 16860, loss = 1.17 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:27:00.824827: step 16870, loss = 1.12 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 14:27:08.113225: step 16880, loss = 1.12 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 14:27:15.300356: step 16890, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:27:22.461811: step 16900, loss = 1.04 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 14:27:32.238023: step 16910, loss = 1.21 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 14:27:39.458914: step 16920, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:27:46.608642: step 16930, loss = 1.01 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 14:27:53.775136: step 16940, loss = 1.01 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 14:28:00.912958: step 16950, loss = 1.04 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:28:08.188391: step 16960, loss = 1.03 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 14:28:15.403971: step 16970, loss = 1.12 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 14:28:22.634223: step 16980, loss = 1.16 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 14:28:29.758771: step 16990, loss = 1.02 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 14:28:37.029835: step 17000, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:28:47.056121: step 17010, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 14:28:54.156761: step 17020, loss = 1.02 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 14:29:01.428934: step 17030, loss = 1.21 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 14:29:08.602493: step 17040, loss = 1.10 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 14:29:15.817424: step 17050, loss = 1.02 (42.6 examples/sec; 0.750 sec/batch)
2018-10-16 14:29:22.950545: step 17060, loss = 1.16 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 14:29:30.153980: step 17070, loss = 1.18 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 14:29:37.484579: step 17080, loss = 1.12 (42.5 examples/sec; 0.753 sec/batch)
2018-10-16 14:29:44.700227: step 17090, loss = 1.00 (42.5 examples/sec; 0.753 sec/batch)
2018-10-16 14:29:51.899209: step 17100, loss = 1.08 (42.5 examples/sec; 0.752 sec/batch)
2018-10-16 14:30:01.701931: step 17110, loss = 1.06 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 14:30:08.935525: step 17120, loss = 1.18 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 14:30:16.103230: step 17130, loss = 1.02 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 14:30:23.355289: step 17140, loss = 1.02 (41.8 examples/sec; 0.766 sec/batch)
2018-10-16 14:30:30.537257: step 17150, loss = 1.05 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 14:30:37.721973: step 17160, loss = 1.09 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 14:30:44.941359: step 17170, loss = 1.10 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 14:30:52.136968: step 17180, loss = 1.04 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 14:30:59.305503: step 17190, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:31:06.592206: step 17200, loss = 1.08 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 14:31:16.381923: step 17210, loss = 1.18 (47.6 examples/sec; 0.672 sec/batch)
2018-10-16 14:31:23.559000: step 17220, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 14:31:30.759228: step 17230, loss = 1.28 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 14:31:37.950608: step 17240, loss = 1.02 (42.4 examples/sec; 0.754 sec/batch)
2018-10-16 14:31:45.263246: step 17250, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 14:31:52.428227: step 17260, loss = 1.13 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 14:31:59.653928: step 17270, loss = 1.18 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 14:32:06.946623: step 17280, loss = 1.23 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 14:32:14.083959: step 17290, loss = 1.11 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:32:21.305669: step 17300, loss = 1.03 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 14:32:31.414061: step 17310, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:32:38.551453: step 17320, loss = 1.10 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 14:32:45.812344: step 17330, loss = 1.13 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 14:32:53.009476: step 17340, loss = 0.99 (42.9 examples/sec; 0.747 sec/batch)
2018-10-16 14:33:00.253526: step 17350, loss = 1.20 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 14:33:07.429889: step 17360, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 14:33:14.583289: step 17370, loss = 0.99 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 14:33:21.832402: step 17380, loss = 1.04 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:33:29.038407: step 17390, loss = 1.12 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:33:36.281432: step 17400, loss = 1.20 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:33:46.240472: step 17410, loss = 1.04 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 14:33:53.430485: step 17420, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 14:34:00.664987: step 17430, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 14:34:07.832984: step 17440, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 14:34:15.152400: step 17450, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:34:22.335263: step 17460, loss = 1.04 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 14:34:29.586718: step 17470, loss = 0.99 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 14:34:36.788756: step 17480, loss = 1.12 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 14:34:43.941300: step 17490, loss = 1.07 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 14:34:51.086500: step 17500, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 14:35:00.975462: step 17510, loss = 1.04 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 14:35:08.180240: step 17520, loss = 1.10 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:35:15.416132: step 17530, loss = 1.26 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:35:22.607721: step 17540, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 14:35:29.829416: step 17550, loss = 1.05 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 14:35:37.001341: step 17560, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 14:35:44.320452: step 17570, loss = 1.03 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 14:35:51.546133: step 17580, loss = 1.05 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 14:35:58.829933: step 17590, loss = 1.09 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 14:36:06.024437: step 17600, loss = 1.09 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 14:36:15.872133: step 17610, loss = 1.10 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 14:36:23.057561: step 17620, loss = 1.05 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 14:36:30.298193: step 17630, loss = 1.13 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 14:36:37.451799: step 17640, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 14:36:44.664539: step 17650, loss = 1.09 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 14:36:51.910227: step 17660, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 14:36:59.163559: step 17670, loss = 1.02 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 14:37:06.389051: step 17680, loss = 1.14 (42.4 examples/sec; 0.756 sec/batch)
2018-10-16 14:37:13.556537: step 17690, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 14:37:20.830140: step 17700, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 14:37:30.668233: step 17710, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 14:37:37.845967: step 17720, loss = 1.01 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 14:37:45.099153: step 17730, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 14:37:52.321261: step 17740, loss = 1.08 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 14:37:59.647079: step 17750, loss = 1.12 (42.2 examples/sec; 0.758 sec/batch)
2018-10-16 14:38:06.888347: step 17760, loss = 1.02 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 14:38:14.007198: step 17770, loss = 1.00 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 14:38:21.234901: step 17780, loss = 1.17 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:38:28.486677: step 17790, loss = 1.01 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 14:38:35.792056: step 17800, loss = 1.06 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 14:38:45.536195: step 17810, loss = 1.20 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:38:52.777499: step 17820, loss = 1.17 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 14:38:59.989654: step 17830, loss = 1.08 (42.2 examples/sec; 0.758 sec/batch)
2018-10-16 14:39:07.256770: step 17840, loss = 1.00 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 14:39:14.404444: step 17850, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:39:21.650412: step 17860, loss = 1.04 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 14:39:28.802679: step 17870, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 14:39:36.069881: step 17880, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:39:43.302831: step 17890, loss = 1.02 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 14:39:50.569327: step 17900, loss = 1.00 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 14:40:00.377938: step 17910, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 14:40:07.571777: step 17920, loss = 1.04 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 14:40:14.753810: step 17930, loss = 1.02 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 14:40:22.017822: step 17940, loss = 1.15 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 14:40:29.278693: step 17950, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 14:40:36.458261: step 17960, loss = 1.10 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 14:40:43.753656: step 17970, loss = 0.99 (42.1 examples/sec; 0.761 sec/batch)
2018-10-16 14:40:50.990139: step 17980, loss = 1.21 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 14:40:58.116521: step 17990, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:41:05.423097: step 18000, loss = 1.13 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 14:41:15.266549: step 18010, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:41:22.352918: step 18020, loss = 1.06 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 14:41:29.535213: step 18030, loss = 0.99 (42.6 examples/sec; 0.750 sec/batch)
2018-10-16 14:41:36.821415: step 18040, loss = 1.14 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 14:41:44.030849: step 18050, loss = 1.02 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 14:41:51.265018: step 18060, loss = 1.06 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:41:58.405343: step 18070, loss = 1.04 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 14:42:05.514608: step 18080, loss = 1.10 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 14:42:12.763821: step 18090, loss = 1.14 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 14:42:19.901040: step 18100, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:42:30.470638: step 18110, loss = 1.04 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 14:42:37.644450: step 18120, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 14:42:44.811725: step 18130, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 14:42:52.032032: step 18140, loss = 1.05 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 14:42:59.255366: step 18150, loss = 1.02 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 14:43:06.472073: step 18160, loss = 1.05 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:43:13.719388: step 18170, loss = 1.09 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:43:20.854386: step 18180, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:43:28.057723: step 18190, loss = 1.07 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 14:43:35.190102: step 18200, loss = 1.11 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 14:43:45.155200: step 18210, loss = 1.03 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 14:43:52.351142: step 18220, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 14:43:59.588809: step 18230, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:44:06.864538: step 18240, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 14:44:14.057309: step 18250, loss = 1.14 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:44:21.304717: step 18260, loss = 1.00 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 14:44:28.492231: step 18270, loss = 1.04 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 14:44:35.742325: step 18280, loss = 1.11 (42.3 examples/sec; 0.756 sec/batch)
2018-10-16 14:44:43.063643: step 18290, loss = 1.12 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 14:44:50.266107: step 18300, loss = 1.03 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 14:45:00.352176: step 18310, loss = 1.12 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 14:45:07.574457: step 18320, loss = 1.11 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 14:45:14.741518: step 18330, loss = 1.13 (42.5 examples/sec; 0.753 sec/batch)
2018-10-16 14:45:21.999133: step 18340, loss = 1.00 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 14:45:29.222644: step 18350, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:45:36.495612: step 18360, loss = 1.04 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 14:45:43.698232: step 18370, loss = 1.02 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 14:45:50.942287: step 18380, loss = 1.02 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 14:45:58.084119: step 18390, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 14:46:05.245569: step 18400, loss = 1.01 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 14:46:15.269673: step 18410, loss = 1.15 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 14:46:22.468501: step 18420, loss = 1.18 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 14:46:29.614842: step 18430, loss = 0.99 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 14:46:36.886220: step 18440, loss = 1.09 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 14:46:44.081676: step 18450, loss = 1.05 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 14:46:51.349380: step 18460, loss = 1.04 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:46:58.573851: step 18470, loss = 1.16 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 14:47:05.760580: step 18480, loss = 0.99 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 14:47:13.000489: step 18490, loss = 1.07 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 14:47:20.193225: step 18500, loss = 1.04 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 14:47:30.240244: step 18510, loss = 1.04 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 14:47:37.336583: step 18520, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 14:47:44.556022: step 18530, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:47:51.823703: step 18540, loss = 1.13 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 14:47:59.056715: step 18550, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 14:48:06.216029: step 18560, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 14:48:13.401035: step 18570, loss = 1.05 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 14:48:20.736160: step 18580, loss = 1.01 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 14:48:27.965730: step 18590, loss = 1.10 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 14:48:35.101404: step 18600, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 14:48:44.853851: step 18610, loss = 1.09 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 14:48:52.140985: step 18620, loss = 1.04 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 14:48:59.260997: step 18630, loss = 1.07 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 14:49:06.455614: step 18640, loss = 1.04 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 14:49:13.659075: step 18650, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:49:20.852215: step 18660, loss = 1.05 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 14:49:28.027690: step 18670, loss = 1.01 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 14:49:35.169483: step 18680, loss = 1.04 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 14:49:42.369624: step 18690, loss = 1.05 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 14:49:49.586540: step 18700, loss = 0.99 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 14:49:59.401044: step 18710, loss = 1.10 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 14:50:06.470264: step 18720, loss = 0.99 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 14:50:13.787557: step 18730, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 14:50:20.899257: step 18740, loss = 1.19 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 14:50:28.081764: step 18750, loss = 1.22 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:50:35.241728: step 18760, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:50:42.449117: step 18770, loss = 1.14 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 14:50:49.656881: step 18780, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 14:50:56.885702: step 18790, loss = 1.01 (42.3 examples/sec; 0.756 sec/batch)
2018-10-16 14:51:04.084891: step 18800, loss = 1.01 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 14:51:13.852165: step 18810, loss = 1.08 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 14:51:21.021756: step 18820, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:51:28.285731: step 18830, loss = 1.00 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 14:51:35.495776: step 18840, loss = 1.04 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 14:51:42.780810: step 18850, loss = 1.05 (42.5 examples/sec; 0.753 sec/batch)
2018-10-16 14:51:50.000383: step 18860, loss = 1.00 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 14:51:57.199291: step 18870, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 14:52:04.390743: step 18880, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 14:52:11.608048: step 18890, loss = 1.00 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 14:52:18.855488: step 18900, loss = 1.04 (42.3 examples/sec; 0.756 sec/batch)
2018-10-16 14:52:28.696490: step 18910, loss = 1.06 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 14:52:35.866997: step 18920, loss = 1.18 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 14:52:43.010304: step 18930, loss = 1.04 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:52:50.323467: step 18940, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 14:52:57.513442: step 18950, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 14:53:04.697451: step 18960, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 14:53:11.907767: step 18970, loss = 1.02 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 14:53:19.138164: step 18980, loss = 1.05 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 14:53:26.321846: step 18990, loss = 1.08 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 14:53:33.572422: step 19000, loss = 1.18 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 14:53:43.289303: step 19010, loss = 1.09 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 14:53:50.417904: step 19020, loss = 1.03 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 14:53:57.568641: step 19030, loss = 1.05 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 14:54:04.803530: step 19040, loss = 1.03 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 14:54:12.062838: step 19050, loss = 1.12 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 14:54:19.265737: step 19060, loss = 1.01 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 14:54:26.488745: step 19070, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 14:54:33.692657: step 19080, loss = 1.15 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:54:40.840688: step 19090, loss = 1.09 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 14:54:47.994626: step 19100, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:54:57.844672: step 19110, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 14:55:04.973965: step 19120, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 14:55:12.165062: step 19130, loss = 0.99 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 14:55:19.349664: step 19140, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:55:26.453388: step 19150, loss = 1.19 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 14:55:33.666132: step 19160, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 14:55:40.923898: step 19170, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:55:48.074396: step 19180, loss = 1.09 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 14:55:55.335887: step 19190, loss = 1.17 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:56:02.498054: step 19200, loss = 1.13 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 14:56:12.175186: step 19210, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 14:56:19.345706: step 19220, loss = 1.10 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 14:56:26.609471: step 19230, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:56:33.790120: step 19240, loss = 1.17 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 14:56:41.034622: step 19250, loss = 1.15 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 14:56:48.194291: step 19260, loss = 1.01 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 14:56:55.416092: step 19270, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 14:57:02.719202: step 19280, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 14:57:09.952918: step 19290, loss = 1.03 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 14:57:17.187385: step 19300, loss = 1.09 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 14:57:26.902436: step 19310, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 14:57:34.041640: step 19320, loss = 1.09 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 14:57:41.326943: step 19330, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 14:57:48.563783: step 19340, loss = 1.00 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 14:57:55.715870: step 19350, loss = 1.13 (42.1 examples/sec; 0.761 sec/batch)
2018-10-16 14:58:02.885956: step 19360, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 14:58:10.110490: step 19370, loss = 1.13 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 14:58:17.360499: step 19380, loss = 1.13 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 14:58:24.562974: step 19390, loss = 1.05 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 14:58:31.834394: step 19400, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 14:58:41.782500: step 19410, loss = 1.18 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 14:58:48.999359: step 19420, loss = 1.08 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 14:58:56.228654: step 19430, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 14:59:03.452716: step 19440, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 14:59:10.710873: step 19450, loss = 1.15 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 14:59:17.856369: step 19460, loss = 1.02 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 14:59:25.111319: step 19470, loss = 1.07 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 14:59:32.311805: step 19480, loss = 1.25 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 14:59:39.537299: step 19490, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 14:59:46.683137: step 19500, loss = 1.10 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 14:59:56.452788: step 19510, loss = 1.04 (47.3 examples/sec; 0.677 sec/batch)
2018-10-16 15:00:03.577272: step 19520, loss = 1.21 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 15:00:10.739158: step 19530, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 15:00:17.961388: step 19540, loss = 0.99 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 15:00:25.172855: step 19550, loss = 1.04 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 15:00:32.383102: step 19560, loss = 1.12 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 15:00:39.582224: step 19570, loss = 1.01 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 15:00:46.804691: step 19580, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 15:00:53.993831: step 19590, loss = 1.11 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 15:01:01.171349: step 19600, loss = 1.00 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 15:01:10.870945: step 19610, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 15:01:18.040342: step 19620, loss = 1.07 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 15:01:25.243704: step 19630, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 15:01:32.535508: step 19640, loss = 1.13 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 15:01:39.761648: step 19650, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 15:01:46.983199: step 19660, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 15:01:54.189390: step 19670, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 15:02:01.433790: step 19680, loss = 1.05 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:02:08.593240: step 19690, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 15:02:15.888123: step 19700, loss = 1.03 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 15:02:25.677778: step 19710, loss = 1.08 (42.5 examples/sec; 0.752 sec/batch)
2018-10-16 15:02:32.876856: step 19720, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 15:02:40.129035: step 19730, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 15:02:47.439044: step 19740, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:02:54.602352: step 19750, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:03:01.877365: step 19760, loss = 1.09 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 15:03:09.199986: step 19770, loss = 1.00 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 15:03:16.449493: step 19780, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 15:03:23.974880: step 19790, loss = 1.02 (41.1 examples/sec; 0.779 sec/batch)
2018-10-16 15:03:31.530128: step 19800, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 15:03:41.684647: step 19810, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 15:03:48.970229: step 19820, loss = 1.10 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 15:03:56.535240: step 19830, loss = 1.10 (41.5 examples/sec; 0.771 sec/batch)
2018-10-16 15:04:03.776334: step 19840, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 15:04:11.035085: step 19850, loss = 1.11 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 15:04:18.632352: step 19860, loss = 1.16 (38.4 examples/sec; 0.833 sec/batch)
2018-10-16 15:04:25.897052: step 19870, loss = 1.13 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 15:04:33.185459: step 19880, loss = 1.20 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 15:04:40.844622: step 19890, loss = 1.05 (38.1 examples/sec; 0.840 sec/batch)
2018-10-16 15:04:48.451007: step 19900, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 15:04:58.366785: step 19910, loss = 1.06 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 15:05:05.512620: step 19920, loss = 1.24 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:05:12.939038: step 19930, loss = 1.04 (40.0 examples/sec; 0.799 sec/batch)
2018-10-16 15:05:20.301103: step 19940, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:05:27.570679: step 19950, loss = 1.07 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 15:05:34.732998: step 19960, loss = 1.08 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 15:05:41.924828: step 19970, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 15:05:49.077417: step 19980, loss = 0.99 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 15:05:56.288401: step 19990, loss = 1.02 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 15:06:03.467213: step 20000, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 15:06:16.044948: step 20010, loss = 1.00 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 15:06:22.993505: step 20020, loss = 1.17 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 15:06:30.255035: step 20030, loss = 1.07 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 15:06:37.469371: step 20040, loss = 1.07 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 15:06:44.745581: step 20050, loss = 1.07 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 15:06:51.922667: step 20060, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 15:06:59.194000: step 20070, loss = 1.02 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 15:07:06.360618: step 20080, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 15:07:13.559736: step 20090, loss = 1.09 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 15:07:20.765435: step 20100, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 15:07:30.938857: step 20110, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:07:38.166143: step 20120, loss = 1.15 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 15:07:45.406495: step 20130, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 15:07:52.721102: step 20140, loss = 1.14 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 15:07:59.917458: step 20150, loss = 1.13 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 15:08:07.093902: step 20160, loss = 1.08 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 15:08:14.282206: step 20170, loss = 1.13 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 15:08:21.515945: step 20180, loss = 1.22 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 15:08:28.781841: step 20190, loss = 1.02 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 15:08:35.940804: step 20200, loss = 1.01 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 15:08:46.212606: step 20210, loss = 1.00 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 15:08:53.385558: step 20220, loss = 1.04 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 15:09:00.536773: step 20230, loss = 1.32 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 15:09:07.739174: step 20240, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 15:09:14.983288: step 20250, loss = 1.10 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 15:09:22.227606: step 20260, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 15:09:29.439692: step 20270, loss = 1.03 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 15:09:36.568382: step 20280, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 15:09:43.802961: step 20290, loss = 1.13 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 15:09:50.994108: step 20300, loss = 1.21 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:10:00.782467: step 20310, loss = 1.06 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 15:10:07.955395: step 20320, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 15:10:15.217742: step 20330, loss = 1.02 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 15:10:22.462368: step 20340, loss = 1.11 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 15:10:29.722380: step 20350, loss = 1.09 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 15:10:37.053130: step 20360, loss = 1.02 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 15:10:44.220943: step 20370, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 15:10:51.480515: step 20380, loss = 1.02 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:10:58.721854: step 20390, loss = 1.06 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 15:11:05.952682: step 20400, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 15:11:16.371576: step 20410, loss = 1.05 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:11:23.626503: step 20420, loss = 1.00 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 15:11:30.849775: step 20430, loss = 1.09 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 15:11:38.138818: step 20440, loss = 1.20 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 15:11:45.400160: step 20450, loss = 1.08 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:11:52.606937: step 20460, loss = 1.00 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 15:11:59.868247: step 20470, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 15:12:07.055678: step 20480, loss = 1.00 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 15:12:14.228232: step 20490, loss = 1.06 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 15:12:21.377803: step 20500, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 15:12:31.144511: step 20510, loss = 1.32 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 15:12:38.460530: step 20520, loss = 1.09 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 15:12:45.675383: step 20530, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 15:12:52.925153: step 20540, loss = 1.10 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 15:13:00.172143: step 20550, loss = 1.01 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 15:13:07.331247: step 20560, loss = 1.04 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 15:13:14.569724: step 20570, loss = 1.01 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 15:13:21.712059: step 20580, loss = 1.16 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 15:13:28.945605: step 20590, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 15:13:36.138565: step 20600, loss = 1.03 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 15:13:45.894229: step 20610, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 15:13:53.137391: step 20620, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:14:00.293522: step 20630, loss = 1.06 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 15:14:07.466743: step 20640, loss = 1.01 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 15:14:14.672536: step 20650, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 15:14:21.970012: step 20660, loss = 1.07 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 15:14:29.204529: step 20670, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:14:36.435763: step 20680, loss = 1.06 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 15:14:43.657119: step 20690, loss = 1.19 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 15:14:50.918478: step 20700, loss = 1.06 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 15:15:00.996022: step 20710, loss = 1.05 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 15:15:08.226883: step 20720, loss = 1.09 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 15:15:15.372770: step 20730, loss = 1.22 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 15:15:22.524689: step 20740, loss = 1.03 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 15:15:29.679163: step 20750, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 15:15:36.903169: step 20760, loss = 1.09 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 15:15:43.993558: step 20770, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 15:15:51.238547: step 20780, loss = 1.25 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 15:15:58.433009: step 20790, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 15:16:05.646148: step 20800, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 15:16:15.360122: step 20810, loss = 1.01 (47.1 examples/sec; 0.680 sec/batch)
2018-10-16 15:16:22.610902: step 20820, loss = 1.04 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 15:16:29.785459: step 20830, loss = 1.04 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:16:37.039433: step 20840, loss = 1.16 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 15:16:44.296510: step 20850, loss = 0.99 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 15:16:51.432424: step 20860, loss = 1.08 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 15:16:58.610895: step 20870, loss = 1.08 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 15:17:05.748501: step 20880, loss = 1.02 (42.5 examples/sec; 0.753 sec/batch)
2018-10-16 15:17:12.987979: step 20890, loss = 1.04 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 15:17:20.126114: step 20900, loss = 1.11 (44.4 examples/sec; 0.722 sec/batch)
2018-10-16 15:17:29.757093: step 20910, loss = 1.10 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 15:17:36.851310: step 20920, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:17:44.019609: step 20930, loss = 1.16 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 15:17:51.294469: step 20940, loss = 1.16 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 15:17:58.431445: step 20950, loss = 1.08 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 15:18:05.604659: step 20960, loss = 1.03 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 15:18:12.775448: step 20970, loss = 1.01 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 15:18:19.997410: step 20980, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 15:18:27.136310: step 20990, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 15:18:34.357887: step 21000, loss = 1.05 (42.1 examples/sec; 0.760 sec/batch)
2018-10-16 15:18:43.956179: step 21010, loss = 1.08 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 15:18:51.183753: step 21020, loss = 1.04 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 15:18:58.314803: step 21030, loss = 1.01 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 15:19:05.457886: step 21040, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 15:19:12.607486: step 21050, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 15:19:19.749013: step 21060, loss = 1.15 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 15:19:27.003828: step 21070, loss = 1.06 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 15:19:34.237149: step 21080, loss = 1.03 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 15:19:41.489928: step 21090, loss = 1.05 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 15:19:48.764431: step 21100, loss = 1.07 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 15:19:58.408677: step 21110, loss = 1.03 (47.6 examples/sec; 0.672 sec/batch)
2018-10-16 15:20:05.518303: step 21120, loss = 1.00 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 15:20:12.712693: step 21130, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 15:20:19.932077: step 21140, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 15:20:27.108787: step 21150, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 15:20:34.367781: step 21160, loss = 1.02 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 15:20:41.537680: step 21170, loss = 1.07 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:20:48.634092: step 21180, loss = 1.10 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 15:20:55.905248: step 21190, loss = 1.03 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 15:21:02.963304: step 21200, loss = 1.08 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 15:21:12.659369: step 21210, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 15:21:19.859199: step 21220, loss = 1.01 (43.7 examples/sec; 0.731 sec/batch)
2018-10-16 15:21:27.093148: step 21230, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:21:34.288686: step 21240, loss = 1.01 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 15:21:41.484608: step 21250, loss = 1.08 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 15:21:48.553140: step 21260, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 15:21:55.728715: step 21270, loss = 1.02 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 15:22:02.950123: step 21280, loss = 1.07 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 15:22:10.255184: step 21290, loss = 1.04 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 15:22:17.381263: step 21300, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:22:27.135072: step 21310, loss = 1.09 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 15:22:34.323364: step 21320, loss = 1.15 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 15:22:41.512305: step 21330, loss = 1.04 (43.0 examples/sec; 0.743 sec/batch)
2018-10-16 15:22:48.679177: step 21340, loss = 1.12 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 15:22:55.854599: step 21350, loss = 1.11 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 15:23:03.078032: step 21360, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 15:23:10.282399: step 21370, loss = 1.01 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 15:23:17.512932: step 21380, loss = 1.00 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 15:23:24.684560: step 21390, loss = 1.11 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 15:23:31.900151: step 21400, loss = 0.99 (42.5 examples/sec; 0.753 sec/batch)
2018-10-16 15:23:42.023337: step 21410, loss = 1.06 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 15:23:49.212797: step 21420, loss = 1.03 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 15:23:56.450437: step 21430, loss = 1.04 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 15:24:03.628346: step 21440, loss = 1.16 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 15:24:10.783110: step 21450, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 15:24:17.871738: step 21460, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 15:24:25.133238: step 21470, loss = 1.08 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 15:24:32.332164: step 21480, loss = 1.04 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 15:24:39.509421: step 21490, loss = 1.23 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 15:24:46.749695: step 21500, loss = 1.02 (42.2 examples/sec; 0.759 sec/batch)
2018-10-16 15:24:56.529355: step 21510, loss = 1.08 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 15:25:03.641234: step 21520, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 15:25:10.849198: step 21530, loss = 1.01 (45.7 examples/sec; 0.699 sec/batch)
2018-10-16 15:25:17.948298: step 21540, loss = 1.13 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 15:25:25.220804: step 21550, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 15:25:32.370272: step 21560, loss = 1.03 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 15:25:39.622672: step 21570, loss = 1.21 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 15:25:46.723985: step 21580, loss = 1.12 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 15:25:53.984570: step 21590, loss = 1.05 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 15:26:01.173548: step 21600, loss = 1.05 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 15:26:10.986121: step 21610, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 15:26:18.272804: step 21620, loss = 1.00 (43.0 examples/sec; 0.743 sec/batch)
2018-10-16 15:26:25.442804: step 21630, loss = 1.00 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 15:26:32.640068: step 21640, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 15:26:39.833459: step 21650, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 15:26:47.021241: step 21660, loss = 1.03 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 15:26:54.152125: step 21670, loss = 1.08 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 15:27:01.314977: step 21680, loss = 1.10 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 15:27:08.503654: step 21690, loss = 1.10 (42.3 examples/sec; 0.756 sec/batch)
2018-10-16 15:27:15.686257: step 21700, loss = 1.07 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 15:27:25.380017: step 21710, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 15:27:32.553997: step 21720, loss = 1.15 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 15:27:39.709747: step 21730, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:27:46.840794: step 21740, loss = 1.08 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 15:27:54.002267: step 21750, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 15:28:01.188793: step 21760, loss = 1.04 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 15:28:08.335987: step 21770, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 15:28:15.470092: step 21780, loss = 1.02 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 15:28:22.636832: step 21790, loss = 1.06 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 15:28:29.783879: step 21800, loss = 1.10 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 15:28:39.756582: step 21810, loss = 1.07 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 15:28:46.893495: step 21820, loss = 1.11 (45.9 examples/sec; 0.696 sec/batch)
2018-10-16 15:28:54.131344: step 21830, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 15:29:01.282733: step 21840, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 15:29:08.447354: step 21850, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 15:29:15.623167: step 21860, loss = 1.01 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 15:29:22.721549: step 21870, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 15:29:29.881253: step 21880, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 15:29:37.084829: step 21890, loss = 1.04 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 15:29:44.269659: step 21900, loss = 1.12 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 15:29:54.362878: step 21910, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 15:30:01.456698: step 21920, loss = 1.00 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 15:30:08.710116: step 21930, loss = 1.06 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 15:30:15.880020: step 21940, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 15:30:23.136256: step 21950, loss = 1.10 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 15:30:30.294830: step 21960, loss = 1.09 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 15:30:37.475977: step 21970, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:30:44.732288: step 21980, loss = 1.01 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 15:30:51.987432: step 21990, loss = 1.10 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 15:30:59.089923: step 22000, loss = 1.02 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 15:31:08.872424: step 22010, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 15:31:15.993546: step 22020, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 15:31:23.112483: step 22030, loss = 1.17 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 15:31:30.285023: step 22040, loss = 1.04 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 15:31:37.422187: step 22050, loss = 1.11 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 15:31:44.495391: step 22060, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 15:31:51.699879: step 22070, loss = 1.14 (43.0 examples/sec; 0.743 sec/batch)
2018-10-16 15:31:58.885612: step 22080, loss = 1.23 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 15:32:06.083084: step 22090, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 15:32:13.226547: step 22100, loss = 1.10 (43.7 examples/sec; 0.731 sec/batch)
2018-10-16 15:32:23.378737: step 22110, loss = 1.13 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 15:32:30.581246: step 22120, loss = 1.04 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:32:37.767382: step 22130, loss = 0.99 (46.4 examples/sec; 0.689 sec/batch)
2018-10-16 15:32:44.920053: step 22140, loss = 1.00 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 15:32:52.093106: step 22150, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 15:32:59.254033: step 22160, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 15:33:06.478838: step 22170, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 15:33:13.667092: step 22180, loss = 1.02 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:33:20.865977: step 22190, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:33:27.995501: step 22200, loss = 1.02 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 15:33:37.759901: step 22210, loss = 0.99 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 15:33:44.905959: step 22220, loss = 1.02 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 15:33:52.121947: step 22230, loss = 1.09 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 15:33:59.311683: step 22240, loss = 1.01 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 15:34:06.463140: step 22250, loss = 1.17 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 15:34:13.743643: step 22260, loss = 1.00 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 15:34:20.933395: step 22270, loss = 1.16 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 15:34:28.145273: step 22280, loss = 1.11 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 15:34:35.285300: step 22290, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 15:34:42.463772: step 22300, loss = 1.09 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 15:34:52.210664: step 22310, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 15:34:59.367600: step 22320, loss = 1.03 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 15:35:06.537469: step 22330, loss = 1.14 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 15:35:13.714299: step 22340, loss = 1.16 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 15:35:20.904055: step 22350, loss = 1.08 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 15:35:28.114539: step 22360, loss = 1.10 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 15:35:35.336485: step 22370, loss = 1.07 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 15:35:42.527107: step 22380, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 15:35:49.665876: step 22390, loss = 1.08 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 15:35:56.965896: step 22400, loss = 1.00 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 15:36:06.737358: step 22410, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 15:36:13.908691: step 22420, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 15:36:21.082558: step 22430, loss = 1.04 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 15:36:28.295373: step 22440, loss = 1.07 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 15:36:35.456634: step 22450, loss = 1.28 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 15:36:42.699594: step 22460, loss = 1.05 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 15:36:49.857624: step 22470, loss = 1.15 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 15:36:57.036537: step 22480, loss = 1.05 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 15:37:04.271683: step 22490, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 15:37:11.473069: step 22500, loss = 1.02 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 15:37:21.265273: step 22510, loss = 1.11 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 15:37:28.439732: step 22520, loss = 0.99 (44.4 examples/sec; 0.722 sec/batch)
2018-10-16 15:37:35.661284: step 22530, loss = 1.09 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 15:37:42.891849: step 22540, loss = 1.09 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 15:37:50.069525: step 22550, loss = 1.13 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 15:37:57.242294: step 22560, loss = 1.16 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 15:38:04.443557: step 22570, loss = 1.03 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 15:38:11.665335: step 22580, loss = 1.08 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 15:38:18.906981: step 22590, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 15:38:26.203266: step 22600, loss = 1.02 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 15:38:36.317377: step 22610, loss = 1.16 (46.9 examples/sec; 0.682 sec/batch)
2018-10-16 15:38:43.605281: step 22620, loss = 1.05 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 15:38:50.839932: step 22630, loss = 1.07 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 15:38:57.953120: step 22640, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 15:39:05.208481: step 22650, loss = 1.14 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 15:39:12.416421: step 22660, loss = 1.15 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 15:39:19.630764: step 22670, loss = 1.00 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 15:39:26.842122: step 22680, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 15:39:34.017866: step 22690, loss = 1.14 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 15:39:41.308194: step 22700, loss = 1.13 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 15:39:51.275950: step 22710, loss = 1.04 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 15:39:58.445350: step 22720, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 15:40:05.744789: step 22730, loss = 1.02 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 15:40:12.974328: step 22740, loss = 1.03 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 15:40:20.223302: step 22750, loss = 0.99 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 15:40:27.446421: step 22760, loss = 1.01 (42.0 examples/sec; 0.761 sec/batch)
2018-10-16 15:40:34.580304: step 22770, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 15:40:41.837967: step 22780, loss = 1.18 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 15:40:49.067870: step 22790, loss = 1.05 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 15:40:56.235196: step 22800, loss = 0.99 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 15:41:06.325777: step 22810, loss = 1.16 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 15:41:13.478391: step 22820, loss = 1.12 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 15:41:20.728423: step 22830, loss = 1.13 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 15:41:27.995544: step 22840, loss = 1.06 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 15:41:35.292008: step 22850, loss = 1.07 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:41:42.486042: step 22860, loss = 1.05 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 15:41:49.760324: step 22870, loss = 1.07 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 15:41:56.983626: step 22880, loss = 1.12 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 15:42:04.148399: step 22890, loss = 1.12 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:42:11.385371: step 22900, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 15:42:21.179471: step 22910, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:42:28.341739: step 22920, loss = 0.99 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 15:42:35.503201: step 22930, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 15:42:42.791877: step 22940, loss = 1.04 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 15:42:50.129555: step 22950, loss = 1.06 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 15:42:57.371295: step 22960, loss = 1.07 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 15:43:04.551814: step 22970, loss = 1.10 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 15:43:11.729804: step 22980, loss = 1.07 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 15:43:18.868692: step 22990, loss = 1.00 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 15:43:26.128865: step 23000, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 15:43:35.813954: step 23010, loss = 1.04 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 15:43:43.000698: step 23020, loss = 1.12 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 15:43:50.229036: step 23030, loss = 1.12 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 15:43:57.415797: step 23040, loss = 1.05 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 15:44:04.687181: step 23050, loss = 1.19 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 15:44:11.874284: step 23060, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 15:44:19.218492: step 23070, loss = 1.00 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 15:44:26.408983: step 23080, loss = 1.00 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 15:44:33.642754: step 23090, loss = 1.01 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 15:44:40.815136: step 23100, loss = 1.05 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 15:44:50.652177: step 23110, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 15:44:57.776341: step 23120, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 15:45:05.029599: step 23130, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 15:45:12.200440: step 23140, loss = 1.08 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 15:45:19.403522: step 23150, loss = 1.17 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 15:45:26.591587: step 23160, loss = 1.08 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 15:45:33.810559: step 23170, loss = 1.01 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 15:45:41.038659: step 23180, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 15:45:48.283956: step 23190, loss = 1.12 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 15:45:55.442771: step 23200, loss = 1.00 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 15:46:05.335166: step 23210, loss = 1.22 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 15:46:12.531478: step 23220, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 15:46:19.672877: step 23230, loss = 1.19 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 15:46:26.920495: step 23240, loss = 1.01 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 15:46:34.191283: step 23250, loss = 1.11 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 15:46:41.410442: step 23260, loss = 1.08 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 15:46:48.678194: step 23270, loss = 1.09 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 15:46:55.906736: step 23280, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 15:47:03.204529: step 23290, loss = 1.02 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 15:47:10.496492: step 23300, loss = 1.04 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 15:47:20.322887: step 23310, loss = 1.01 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 15:47:27.527128: step 23320, loss = 1.10 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 15:47:34.729368: step 23330, loss = 1.18 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 15:47:41.915180: step 23340, loss = 1.20 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 15:47:49.222656: step 23350, loss = 1.07 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 15:47:56.428990: step 23360, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 15:48:03.618243: step 23370, loss = 1.10 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 15:48:10.832183: step 23380, loss = 1.04 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 15:48:18.002598: step 23390, loss = 1.07 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 15:48:25.156507: step 23400, loss = 1.21 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 15:48:34.988117: step 23410, loss = 1.04 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 15:48:42.146224: step 23420, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 15:48:49.377629: step 23430, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:48:56.636872: step 23440, loss = 1.13 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 15:49:03.861007: step 23450, loss = 1.17 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 15:49:11.107113: step 23460, loss = 1.09 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 15:49:18.309478: step 23470, loss = 1.09 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 15:49:25.592021: step 23480, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 15:49:32.857836: step 23490, loss = 1.01 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 15:49:40.023845: step 23500, loss = 1.15 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 15:49:49.845573: step 23510, loss = 1.03 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 15:49:56.980589: step 23520, loss = 1.01 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 15:50:04.133843: step 23530, loss = 1.10 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 15:50:11.276392: step 23540, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 15:50:18.564123: step 23550, loss = 1.09 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 15:50:25.799289: step 23560, loss = 1.19 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 15:50:33.051370: step 23570, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 15:50:40.303685: step 23580, loss = 1.10 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:50:47.478010: step 23590, loss = 1.07 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 15:50:54.667447: step 23600, loss = 1.08 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 15:51:04.591247: step 23610, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 15:51:11.838573: step 23620, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 15:51:18.996250: step 23630, loss = 1.21 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 15:51:26.092316: step 23640, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 15:51:33.365412: step 23650, loss = 1.04 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 15:51:40.478110: step 23660, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 15:51:47.692942: step 23670, loss = 1.21 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 15:51:54.963760: step 23680, loss = 1.11 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 15:52:02.217913: step 23690, loss = 1.05 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 15:52:09.498972: step 23700, loss = 1.05 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 15:52:19.319406: step 23710, loss = 1.05 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 15:52:26.491102: step 23720, loss = 1.11 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 15:52:33.629788: step 23730, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 15:52:40.844432: step 23740, loss = 1.09 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 15:52:48.102143: step 23750, loss = 1.02 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 15:52:55.290593: step 23760, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 15:53:02.513720: step 23770, loss = 1.13 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 15:53:09.765342: step 23780, loss = 1.06 (42.6 examples/sec; 0.750 sec/batch)
2018-10-16 15:53:16.923585: step 23790, loss = 1.05 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 15:53:24.164271: step 23800, loss = 1.04 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 15:53:33.915537: step 23810, loss = 1.13 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 15:53:41.135035: step 23820, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 15:53:48.391997: step 23830, loss = 1.23 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 15:53:55.696948: step 23840, loss = 1.09 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 15:54:02.848931: step 23850, loss = 1.24 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 15:54:10.052007: step 23860, loss = 1.01 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 15:54:17.294554: step 23870, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 15:54:24.601711: step 23880, loss = 1.04 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 15:54:31.788550: step 23890, loss = 1.02 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 15:54:39.037408: step 23900, loss = 1.04 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 15:54:48.741430: step 23910, loss = 1.10 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 15:54:55.986687: step 23920, loss = 1.16 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 15:55:03.210771: step 23930, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 15:55:10.414616: step 23940, loss = 1.09 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 15:55:17.615992: step 23950, loss = 1.13 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 15:55:24.812206: step 23960, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 15:55:31.959537: step 23970, loss = 1.11 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 15:55:39.124535: step 23980, loss = 1.15 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 15:55:46.332075: step 23990, loss = 1.09 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 15:55:53.538140: step 24000, loss = 1.08 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:56:03.213593: step 24010, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:56:10.312484: step 24020, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 15:56:17.525120: step 24030, loss = 1.03 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 15:56:24.700842: step 24040, loss = 1.06 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 15:56:31.892219: step 24050, loss = 1.00 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 15:56:39.097868: step 24060, loss = 1.03 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 15:56:46.391181: step 24070, loss = 1.00 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 15:56:53.535191: step 24080, loss = 1.01 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 15:57:00.836087: step 24090, loss = 1.01 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 15:57:08.038800: step 24100, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 15:57:17.799780: step 24110, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 15:57:25.017578: step 24120, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 15:57:32.321890: step 24130, loss = 1.05 (43.2 examples/sec; 0.742 sec/batch)
2018-10-16 15:57:39.584646: step 24140, loss = 1.06 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 15:57:46.788063: step 24150, loss = 1.01 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 15:57:54.003922: step 24160, loss = 1.09 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 15:58:01.169711: step 24170, loss = 1.19 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 15:58:08.378231: step 24180, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 15:58:15.583361: step 24190, loss = 1.00 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 15:58:22.805459: step 24200, loss = 0.99 (44.0 examples/sec; 0.726 sec/batch)
2018-10-16 15:58:32.578100: step 24210, loss = 1.05 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 15:58:39.771080: step 24220, loss = 1.00 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 15:58:46.912949: step 24230, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 15:58:54.084140: step 24240, loss = 1.01 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 15:59:01.255896: step 24250, loss = 1.11 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 15:59:08.472559: step 24260, loss = 0.99 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 15:59:15.639165: step 24270, loss = 1.02 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 15:59:22.856523: step 24280, loss = 1.06 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 15:59:30.153184: step 24290, loss = 0.99 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 15:59:37.265818: step 24300, loss = 1.11 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 15:59:47.051448: step 24310, loss = 1.05 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 15:59:54.144102: step 24320, loss = 1.15 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 16:00:01.347133: step 24330, loss = 1.07 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 16:00:08.565495: step 24340, loss = 1.08 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 16:00:15.790528: step 24350, loss = 1.12 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 16:00:23.137777: step 24360, loss = 1.11 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 16:00:30.354733: step 24370, loss = 1.01 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 16:00:37.638618: step 24380, loss = 1.18 (43.0 examples/sec; 0.743 sec/batch)
2018-10-16 16:00:44.867963: step 24390, loss = 1.02 (46.4 examples/sec; 0.689 sec/batch)
2018-10-16 16:00:51.984526: step 24400, loss = 1.02 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 16:01:02.652886: step 24410, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 16:01:09.913022: step 24420, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 16:01:17.075140: step 24430, loss = 1.11 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 16:01:24.222048: step 24440, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 16:01:31.423571: step 24450, loss = 1.04 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 16:01:38.640066: step 24460, loss = 1.17 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 16:01:45.814857: step 24470, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 16:01:53.045659: step 24480, loss = 1.17 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 16:02:00.274241: step 24490, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 16:02:07.512559: step 24500, loss = 1.25 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 16:02:17.332290: step 24510, loss = 1.07 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 16:02:24.522696: step 24520, loss = 1.06 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 16:02:31.756384: step 24530, loss = 0.99 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 16:02:38.952521: step 24540, loss = 1.01 (47.1 examples/sec; 0.680 sec/batch)
2018-10-16 16:02:46.170617: step 24550, loss = 1.23 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:02:53.372360: step 24560, loss = 1.03 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 16:03:00.528104: step 24570, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 16:03:07.722141: step 24580, loss = 1.10 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 16:03:14.992723: step 24590, loss = 1.15 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 16:03:22.253367: step 24600, loss = 0.99 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 16:03:32.709286: step 24610, loss = 1.03 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 16:03:39.984544: step 24620, loss = 1.11 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 16:03:47.155528: step 24630, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:03:54.480692: step 24640, loss = 1.18 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 16:04:01.687616: step 24650, loss = 0.99 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 16:04:08.844631: step 24660, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 16:04:16.012965: step 24670, loss = 1.10 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 16:04:23.259245: step 24680, loss = 1.00 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 16:04:30.481517: step 24690, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 16:04:37.808868: step 24700, loss = 1.06 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 16:04:47.627654: step 24710, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 16:04:54.740752: step 24720, loss = 1.05 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 16:05:01.941635: step 24730, loss = 1.09 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 16:05:09.153325: step 24740, loss = 1.26 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 16:05:16.401071: step 24750, loss = 1.00 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 16:05:23.568856: step 24760, loss = 1.03 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 16:05:30.728808: step 24770, loss = 1.02 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 16:05:37.967075: step 24780, loss = 1.02 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 16:05:45.153067: step 24790, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:05:52.449621: step 24800, loss = 1.01 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 16:06:02.273517: step 24810, loss = 1.13 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:06:09.437343: step 24820, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 16:06:16.735916: step 24830, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 16:06:23.893077: step 24840, loss = 1.07 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 16:06:31.116108: step 24850, loss = 1.10 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 16:06:38.312858: step 24860, loss = 0.99 (42.2 examples/sec; 0.759 sec/batch)
2018-10-16 16:06:45.529113: step 24870, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:06:52.766224: step 24880, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 16:07:00.045738: step 24890, loss = 1.15 (42.6 examples/sec; 0.750 sec/batch)
2018-10-16 16:07:07.251166: step 24900, loss = 1.01 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 16:07:17.005282: step 24910, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:07:24.200531: step 24920, loss = 1.02 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 16:07:31.426077: step 24930, loss = 1.06 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:07:38.573952: step 24940, loss = 1.23 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 16:07:45.850858: step 24950, loss = 1.00 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 16:07:53.072694: step 24960, loss = 1.07 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 16:08:00.403891: step 24970, loss = 1.01 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 16:08:07.533883: step 24980, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:08:14.746712: step 24990, loss = 1.05 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 16:08:21.988559: step 25000, loss = 1.10 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:08:35.182537: step 25010, loss = 1.08 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 16:08:42.026805: step 25020, loss = 1.13 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 16:08:49.246874: step 25030, loss = 1.47 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 16:08:56.485474: step 25040, loss = 1.07 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 16:09:03.705028: step 25050, loss = 1.01 (42.2 examples/sec; 0.758 sec/batch)
2018-10-16 16:09:11.006093: step 25060, loss = 1.02 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 16:09:18.204209: step 25070, loss = 1.07 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 16:09:25.442274: step 25080, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 16:09:32.665902: step 25090, loss = 1.03 (42.3 examples/sec; 0.756 sec/batch)
2018-10-16 16:09:39.963893: step 25100, loss = 1.00 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 16:09:49.758041: step 25110, loss = 1.06 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 16:09:56.905488: step 25120, loss = 1.04 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 16:10:04.042076: step 25130, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 16:10:11.173775: step 25140, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 16:10:18.331715: step 25150, loss = 1.12 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 16:10:25.625251: step 25160, loss = 1.05 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:10:32.816304: step 25170, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:10:39.943364: step 25180, loss = 1.05 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 16:10:47.193537: step 25190, loss = 1.14 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 16:10:54.382785: step 25200, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 16:11:04.737946: step 25210, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 16:11:11.877749: step 25220, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 16:11:19.073134: step 25230, loss = 0.99 (43.2 examples/sec; 0.742 sec/batch)
2018-10-16 16:11:26.318817: step 25240, loss = 1.00 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 16:11:33.507117: step 25250, loss = 1.19 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 16:11:40.757437: step 25260, loss = 1.04 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 16:11:47.923424: step 25270, loss = 1.01 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 16:11:55.113684: step 25280, loss = 0.99 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 16:12:02.296991: step 25290, loss = 1.19 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 16:12:09.514636: step 25300, loss = 1.12 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 16:12:19.233226: step 25310, loss = 1.15 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 16:12:26.462920: step 25320, loss = 1.16 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 16:12:33.634288: step 25330, loss = 0.99 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 16:12:40.882174: step 25340, loss = 1.08 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 16:12:48.066037: step 25350, loss = 1.08 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 16:12:55.314324: step 25360, loss = 1.00 (42.3 examples/sec; 0.756 sec/batch)
2018-10-16 16:13:02.515161: step 25370, loss = 1.04 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 16:13:09.799324: step 25380, loss = 1.00 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 16:13:16.996907: step 25390, loss = 1.13 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:13:24.270791: step 25400, loss = 1.18 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 16:13:34.828088: step 25410, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 16:13:41.981488: step 25420, loss = 1.03 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 16:13:49.218424: step 25430, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:13:56.370836: step 25440, loss = 1.06 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:14:03.593338: step 25450, loss = 1.01 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:14:10.853976: step 25460, loss = 1.06 (42.2 examples/sec; 0.759 sec/batch)
2018-10-16 16:14:18.126300: step 25470, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 16:14:25.427646: step 25480, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 16:14:32.614459: step 25490, loss = 1.06 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 16:14:39.788618: step 25500, loss = 1.12 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 16:14:49.781448: step 25510, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 16:14:56.963511: step 25520, loss = 1.05 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 16:15:04.106333: step 25530, loss = 1.06 (46.6 examples/sec; 0.686 sec/batch)
2018-10-16 16:15:11.287041: step 25540, loss = 1.02 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 16:15:18.597680: step 25550, loss = 1.18 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 16:15:25.753312: step 25560, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:15:32.982496: step 25570, loss = 1.00 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 16:15:40.126187: step 25580, loss = 1.16 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 16:15:47.326880: step 25590, loss = 1.11 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 16:15:54.506827: step 25600, loss = 1.11 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 16:16:04.323601: step 25610, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 16:16:11.517950: step 25620, loss = 1.13 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 16:16:18.733363: step 25630, loss = 1.18 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 16:16:25.983584: step 25640, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 16:16:33.144590: step 25650, loss = 1.06 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 16:16:40.434390: step 25660, loss = 1.13 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 16:16:47.618681: step 25670, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:16:54.862742: step 25680, loss = 1.13 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 16:17:02.015591: step 25690, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 16:17:09.268495: step 25700, loss = 1.10 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 16:17:19.014627: step 25710, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 16:17:26.163875: step 25720, loss = 1.12 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 16:17:33.355058: step 25730, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 16:17:40.619221: step 25740, loss = 1.07 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 16:17:47.779436: step 25750, loss = 1.21 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 16:17:54.991075: step 25760, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 16:18:02.297987: step 25770, loss = 1.10 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 16:18:09.502185: step 25780, loss = 1.09 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:18:16.697097: step 25790, loss = 1.02 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 16:18:23.964573: step 25800, loss = 1.07 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 16:18:33.687604: step 25810, loss = 1.02 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 16:18:40.934563: step 25820, loss = 1.03 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 16:18:48.170225: step 25830, loss = 1.03 (42.3 examples/sec; 0.756 sec/batch)
2018-10-16 16:18:55.330793: step 25840, loss = 1.20 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 16:19:02.496715: step 25850, loss = 1.06 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:19:09.657951: step 25860, loss = 1.09 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 16:19:16.830952: step 25870, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:19:24.046457: step 25880, loss = 1.12 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 16:19:31.238948: step 25890, loss = 1.01 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 16:19:38.504407: step 25900, loss = 1.13 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:19:48.372197: step 25910, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 16:19:55.540411: step 25920, loss = 0.99 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 16:20:02.736947: step 25930, loss = 1.06 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 16:20:09.935981: step 25940, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 16:20:17.177385: step 25950, loss = 1.05 (42.5 examples/sec; 0.754 sec/batch)
2018-10-16 16:20:24.374362: step 25960, loss = 1.01 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 16:20:31.513023: step 25970, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:20:38.805064: step 25980, loss = 1.16 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 16:20:46.098514: step 25990, loss = 1.09 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 16:20:53.330619: step 26000, loss = 1.10 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 16:21:04.037513: step 26010, loss = 1.17 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 16:21:11.269200: step 26020, loss = 1.05 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:21:18.439818: step 26030, loss = 1.05 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 16:21:25.602220: step 26040, loss = 1.17 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 16:21:32.848117: step 26050, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 16:21:40.071483: step 26060, loss = 1.04 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 16:21:47.292682: step 26070, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 16:21:54.679710: step 26080, loss = 1.10 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 16:22:01.960032: step 26090, loss = 1.05 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 16:22:09.082838: step 26100, loss = 1.02 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 16:22:18.918156: step 26110, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 16:22:26.083374: step 26120, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 16:22:33.288471: step 26130, loss = 1.10 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 16:22:40.519790: step 26140, loss = 1.12 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 16:22:47.806940: step 26150, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 16:22:55.038787: step 26160, loss = 1.04 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 16:23:02.278701: step 26170, loss = 1.17 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 16:23:09.529712: step 26180, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 16:23:16.803300: step 26190, loss = 0.99 (43.7 examples/sec; 0.731 sec/batch)
2018-10-16 16:23:23.964105: step 26200, loss = 1.19 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 16:23:33.747443: step 26210, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 16:23:40.949694: step 26220, loss = 1.03 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 16:23:48.209131: step 26230, loss = 1.01 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 16:23:55.379949: step 26240, loss = 1.01 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 16:24:02.605086: step 26250, loss = 1.16 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 16:24:09.834876: step 26260, loss = 1.01 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 16:24:17.030194: step 26270, loss = 1.07 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 16:24:24.309949: step 26280, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 16:24:31.502752: step 26290, loss = 1.04 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 16:24:38.614038: step 26300, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 16:24:48.268640: step 26310, loss = 1.08 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 16:24:55.457446: step 26320, loss = 1.11 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 16:25:02.723985: step 26330, loss = 1.03 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 16:25:09.878446: step 26340, loss = 1.20 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 16:25:16.998544: step 26350, loss = 1.26 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:25:24.337549: step 26360, loss = 1.05 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 16:25:31.522450: step 26370, loss = 1.04 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 16:25:38.750344: step 26380, loss = 1.13 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 16:25:46.004279: step 26390, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:25:53.252681: step 26400, loss = 1.02 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 16:26:03.444842: step 26410, loss = 1.10 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 16:26:10.602380: step 26420, loss = 1.07 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 16:26:17.735929: step 26430, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:26:24.928813: step 26440, loss = 1.10 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 16:26:32.183172: step 26450, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 16:26:39.409838: step 26460, loss = 1.01 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 16:26:46.615376: step 26470, loss = 1.02 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 16:26:53.826357: step 26480, loss = 1.14 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 16:27:01.054626: step 26490, loss = 1.38 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 16:27:08.291168: step 26500, loss = 1.07 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 16:27:18.412642: step 26510, loss = 1.23 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:27:25.614098: step 26520, loss = 1.19 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 16:27:32.840452: step 26530, loss = 1.00 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 16:27:40.019578: step 26540, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 16:27:47.259670: step 26550, loss = 0.99 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 16:27:54.507753: step 26560, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 16:28:01.742305: step 26570, loss = 1.01 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 16:28:08.907921: step 26580, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 16:28:16.124799: step 26590, loss = 1.09 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 16:28:23.288670: step 26600, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 16:28:32.946388: step 26610, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:28:40.117983: step 26620, loss = 0.99 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 16:28:47.360270: step 26630, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 16:28:54.553453: step 26640, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 16:29:01.740156: step 26650, loss = 1.11 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:29:08.962883: step 26660, loss = 1.00 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 16:29:16.172948: step 26670, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 16:29:23.389807: step 26680, loss = 1.03 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 16:29:30.636890: step 26690, loss = 0.99 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 16:29:37.820999: step 26700, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 16:29:47.643176: step 26710, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 16:29:54.877675: step 26720, loss = 1.06 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 16:30:02.044839: step 26730, loss = 1.05 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 16:30:09.331519: step 26740, loss = 1.05 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 16:30:16.541935: step 26750, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:30:23.765245: step 26760, loss = 1.08 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 16:30:31.026917: step 26770, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:30:38.255770: step 26780, loss = 1.03 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 16:30:45.482460: step 26790, loss = 1.00 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 16:30:52.731071: step 26800, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 16:31:02.470623: step 26810, loss = 1.04 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 16:31:09.623736: step 26820, loss = 1.03 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 16:31:16.775336: step 26830, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 16:31:24.057421: step 26840, loss = 1.03 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 16:31:31.238241: step 26850, loss = 1.12 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 16:31:38.457598: step 26860, loss = 1.05 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 16:31:45.665528: step 26870, loss = 1.03 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 16:31:52.956219: step 26880, loss = 1.08 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 16:32:00.156912: step 26890, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 16:32:07.343609: step 26900, loss = 1.10 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:32:17.119069: step 26910, loss = 1.02 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 16:32:24.230437: step 26920, loss = 1.03 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 16:32:31.438658: step 26930, loss = 1.03 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 16:32:38.669331: step 26940, loss = 1.11 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 16:32:45.892221: step 26950, loss = 1.06 (42.5 examples/sec; 0.752 sec/batch)
2018-10-16 16:32:53.052277: step 26960, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 16:33:00.185513: step 26970, loss = 1.03 (46.7 examples/sec; 0.686 sec/batch)
2018-10-16 16:33:07.392545: step 26980, loss = 1.22 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 16:33:14.578134: step 26990, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 16:33:21.847848: step 27000, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 16:33:31.761708: step 27010, loss = 1.03 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 16:33:38.970444: step 27020, loss = 1.08 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 16:33:46.273912: step 27030, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 16:33:53.370624: step 27040, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 16:34:00.613023: step 27050, loss = 1.04 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 16:34:07.856337: step 27060, loss = 1.06 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 16:34:15.065553: step 27070, loss = 1.12 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 16:34:22.321391: step 27080, loss = 0.99 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 16:34:29.579777: step 27090, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:34:36.748896: step 27100, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 16:34:46.435971: step 27110, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 16:34:53.618087: step 27120, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 16:35:00.746017: step 27130, loss = 1.13 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 16:35:07.945992: step 27140, loss = 1.20 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 16:35:15.094908: step 27150, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 16:35:22.250453: step 27160, loss = 1.01 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 16:35:29.411221: step 27170, loss = 1.20 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 16:35:36.601315: step 27180, loss = 1.09 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 16:35:43.778772: step 27190, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 16:35:50.968856: step 27200, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:36:00.713476: step 27210, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 16:36:07.872420: step 27220, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 16:36:15.101754: step 27230, loss = 1.00 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 16:36:22.353246: step 27240, loss = 1.15 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 16:36:29.573737: step 27250, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 16:36:36.812494: step 27260, loss = 1.03 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 16:36:43.963408: step 27270, loss = 1.10 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 16:36:51.136608: step 27280, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 16:36:58.414601: step 27290, loss = 0.99 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 16:37:05.657728: step 27300, loss = 1.08 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 16:37:15.370535: step 27310, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 16:37:22.506301: step 27320, loss = 1.11 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 16:37:29.809892: step 27330, loss = 1.11 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 16:37:37.010209: step 27340, loss = 1.09 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 16:37:44.240899: step 27350, loss = 1.03 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 16:37:51.386764: step 27360, loss = 1.04 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 16:37:58.587301: step 27370, loss = 1.04 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 16:38:05.768122: step 27380, loss = 1.06 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 16:38:12.938336: step 27390, loss = 1.04 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 16:38:20.155301: step 27400, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 16:38:29.909767: step 27410, loss = 1.02 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 16:38:37.155744: step 27420, loss = 1.06 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 16:38:44.349748: step 27430, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 16:38:51.459688: step 27440, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 16:38:58.716662: step 27450, loss = 0.99 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 16:39:05.940656: step 27460, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 16:39:13.103237: step 27470, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:39:20.350997: step 27480, loss = 1.07 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 16:39:27.582688: step 27490, loss = 0.99 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 16:39:34.762151: step 27500, loss = 1.04 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 16:39:44.670659: step 27510, loss = 1.02 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 16:39:51.917130: step 27520, loss = 1.21 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 16:39:59.185076: step 27530, loss = 1.11 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 16:40:06.422933: step 27540, loss = 1.07 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:40:13.575885: step 27550, loss = 1.10 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:40:20.801282: step 27560, loss = 1.23 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 16:40:27.978557: step 27570, loss = 1.09 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 16:40:35.113527: step 27580, loss = 1.04 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 16:40:42.333619: step 27590, loss = 1.19 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 16:40:49.546749: step 27600, loss = 1.12 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 16:40:59.343690: step 27610, loss = 1.18 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 16:41:06.477059: step 27620, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 16:41:13.722205: step 27630, loss = 1.10 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 16:41:20.951828: step 27640, loss = 1.00 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 16:41:28.213279: step 27650, loss = 0.99 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 16:41:35.410176: step 27660, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:41:42.632548: step 27670, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:41:49.842982: step 27680, loss = 1.08 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 16:41:57.012441: step 27690, loss = 1.11 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 16:42:04.215064: step 27700, loss = 1.11 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 16:42:13.964499: step 27710, loss = 1.02 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 16:42:21.148525: step 27720, loss = 1.12 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 16:42:28.281965: step 27730, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:42:35.574151: step 27740, loss = 1.08 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 16:42:42.724549: step 27750, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 16:42:49.857725: step 27760, loss = 1.13 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:42:57.008977: step 27770, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 16:43:04.207097: step 27780, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 16:43:11.450376: step 27790, loss = 1.14 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 16:43:18.592517: step 27800, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 16:43:28.353016: step 27810, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 16:43:35.509440: step 27820, loss = 0.99 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 16:43:42.770984: step 27830, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:43:49.925388: step 27840, loss = 1.16 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:43:57.098725: step 27850, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 16:44:04.287086: step 27860, loss = 1.08 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 16:44:11.519369: step 27870, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 16:44:18.677017: step 27880, loss = 1.08 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 16:44:25.897108: step 27890, loss = 1.12 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 16:44:33.132069: step 27900, loss = 1.03 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 16:44:42.897417: step 27910, loss = 1.20 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 16:44:50.057779: step 27920, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 16:44:57.264895: step 27930, loss = 1.20 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 16:45:04.389829: step 27940, loss = 1.04 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 16:45:11.622427: step 27950, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 16:45:18.764916: step 27960, loss = 1.06 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 16:45:25.885791: step 27970, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:45:33.055672: step 27980, loss = 1.10 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:45:40.246838: step 27990, loss = 1.13 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 16:45:47.418798: step 28000, loss = 1.07 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:45:57.148368: step 28010, loss = 1.05 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 16:46:04.340609: step 28020, loss = 1.10 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 16:46:11.495172: step 28030, loss = 1.04 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 16:46:18.692662: step 28040, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 16:46:25.974829: step 28050, loss = 1.08 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 16:46:33.197027: step 28060, loss = 1.00 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 16:46:40.513604: step 28070, loss = 1.01 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 16:46:47.713136: step 28080, loss = 1.13 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 16:46:54.956964: step 28090, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 16:47:02.139313: step 28100, loss = 1.00 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 16:47:11.890448: step 28110, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:47:19.090164: step 28120, loss = 1.08 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 16:47:26.243407: step 28130, loss = 1.09 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:47:33.453223: step 28140, loss = 1.12 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 16:47:40.660862: step 28150, loss = 1.00 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 16:47:47.846166: step 28160, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 16:47:55.114254: step 28170, loss = 1.08 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 16:48:02.317579: step 28180, loss = 1.04 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 16:48:09.495148: step 28190, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 16:48:16.695435: step 28200, loss = 1.13 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:48:26.653325: step 28210, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 16:48:33.830419: step 28220, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 16:48:41.024186: step 28230, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 16:48:48.237664: step 28240, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 16:48:55.467475: step 28250, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 16:49:02.654524: step 28260, loss = 0.99 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 16:49:09.850731: step 28270, loss = 1.10 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 16:49:17.109939: step 28280, loss = 1.13 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 16:49:24.261686: step 28290, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 16:49:31.434902: step 28300, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 16:49:41.172378: step 28310, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 16:49:48.346518: step 28320, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 16:49:55.476022: step 28330, loss = 1.12 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 16:50:02.731421: step 28340, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 16:50:09.919235: step 28350, loss = 1.07 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 16:50:17.106348: step 28360, loss = 1.06 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 16:50:24.343666: step 28370, loss = 1.03 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 16:50:31.553972: step 28380, loss = 1.04 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 16:50:38.689061: step 28390, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 16:50:45.955364: step 28400, loss = 1.07 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 16:50:56.007105: step 28410, loss = 1.10 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 16:51:03.147709: step 28420, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 16:51:10.371954: step 28430, loss = 1.06 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 16:51:17.556530: step 28440, loss = 1.20 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 16:51:24.751198: step 28450, loss = 1.13 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 16:51:31.936461: step 28460, loss = 1.06 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 16:51:39.236996: step 28470, loss = 1.17 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 16:51:46.451186: step 28480, loss = 1.10 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 16:51:53.740888: step 28490, loss = 1.16 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 16:52:01.009351: step 28500, loss = 1.03 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 16:52:10.750788: step 28510, loss = 1.10 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 16:52:17.859547: step 28520, loss = 1.07 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 16:52:25.067432: step 28530, loss = 1.05 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 16:52:32.344170: step 28540, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 16:52:39.466697: step 28550, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 16:52:46.692037: step 28560, loss = 1.03 (42.4 examples/sec; 0.754 sec/batch)
2018-10-16 16:52:53.978324: step 28570, loss = 1.05 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 16:53:01.210792: step 28580, loss = 1.06 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 16:53:08.333581: step 28590, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 16:53:15.525248: step 28600, loss = 1.01 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 16:53:25.785895: step 28610, loss = 0.99 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 16:53:33.051603: step 28620, loss = 1.10 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 16:53:40.243935: step 28630, loss = 1.06 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 16:53:47.501779: step 28640, loss = 1.07 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 16:53:54.695101: step 28650, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:54:01.977967: step 28660, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 16:54:09.137195: step 28670, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 16:54:16.321225: step 28680, loss = 1.08 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 16:54:23.472825: step 28690, loss = 1.03 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 16:54:30.663372: step 28700, loss = 1.04 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 16:54:40.419172: step 28710, loss = 1.06 (46.6 examples/sec; 0.686 sec/batch)
2018-10-16 16:54:47.500716: step 28720, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 16:54:54.770574: step 28730, loss = 1.01 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 16:55:01.953597: step 28740, loss = 1.03 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 16:55:09.115385: step 28750, loss = 1.25 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 16:55:16.292484: step 28760, loss = 1.22 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 16:55:23.463237: step 28770, loss = 1.06 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 16:55:30.676403: step 28780, loss = 1.04 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 16:55:37.841777: step 28790, loss = 1.02 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 16:55:44.989444: step 28800, loss = 1.08 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 16:55:54.877379: step 28810, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 16:56:02.143644: step 28820, loss = 1.06 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 16:56:09.291607: step 28830, loss = 1.04 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 16:56:16.606572: step 28840, loss = 1.01 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:56:23.747888: step 28850, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 16:56:31.023131: step 28860, loss = 1.22 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 16:56:38.244153: step 28870, loss = 1.04 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 16:56:45.504636: step 28880, loss = 1.10 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 16:56:52.778876: step 28890, loss = 1.20 (44.4 examples/sec; 0.722 sec/batch)
2018-10-16 16:56:59.956589: step 28900, loss = 1.24 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 16:57:09.795965: step 28910, loss = 1.16 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 16:57:16.946901: step 28920, loss = 1.04 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 16:57:24.120117: step 28930, loss = 1.00 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 16:57:31.303728: step 28940, loss = 1.14 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 16:57:38.581241: step 28950, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 16:57:45.715218: step 28960, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 16:57:52.863520: step 28970, loss = 1.05 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 16:58:00.043264: step 28980, loss = 1.05 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 16:58:07.236974: step 28990, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 16:58:14.486212: step 29000, loss = 1.02 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 16:58:24.118741: step 29010, loss = 1.00 (47.3 examples/sec; 0.677 sec/batch)
2018-10-16 16:58:31.290052: step 29020, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 16:58:38.397471: step 29030, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 16:58:45.643840: step 29040, loss = 1.05 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 16:58:52.830273: step 29050, loss = 1.08 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 16:59:00.028377: step 29060, loss = 1.11 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 16:59:07.184532: step 29070, loss = 0.99 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 16:59:14.307225: step 29080, loss = 1.18 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 16:59:21.495489: step 29090, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 16:59:28.681970: step 29100, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 16:59:38.387677: step 29110, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 16:59:45.499209: step 29120, loss = 1.20 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 16:59:52.674698: step 29130, loss = 1.07 (42.5 examples/sec; 0.754 sec/batch)
2018-10-16 16:59:59.837273: step 29140, loss = 0.99 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 17:00:07.136006: step 29150, loss = 1.01 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 17:00:14.346606: step 29160, loss = 1.02 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 17:00:21.587530: step 29170, loss = 1.16 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 17:00:28.828313: step 29180, loss = 1.12 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 17:00:36.057763: step 29190, loss = 1.12 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:00:43.285245: step 29200, loss = 1.02 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 17:00:53.980645: step 29210, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 17:01:01.208413: step 29220, loss = 1.06 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 17:01:08.352454: step 29230, loss = 1.05 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 17:01:15.621978: step 29240, loss = 1.00 (42.3 examples/sec; 0.756 sec/batch)
2018-10-16 17:01:22.869058: step 29250, loss = 1.19 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 17:01:30.035565: step 29260, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 17:01:37.201555: step 29270, loss = 1.14 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 17:01:44.452350: step 29280, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 17:01:51.608475: step 29290, loss = 1.06 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 17:01:58.830848: step 29300, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 17:02:08.554442: step 29310, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 17:02:15.665845: step 29320, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 17:02:22.890719: step 29330, loss = 1.09 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 17:02:30.134214: step 29340, loss = 1.04 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 17:02:37.266043: step 29350, loss = 1.06 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 17:02:44.478618: step 29360, loss = 1.15 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 17:02:51.697116: step 29370, loss = 1.13 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 17:02:58.890517: step 29380, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:03:06.081327: step 29390, loss = 1.07 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 17:03:13.218464: step 29400, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 17:03:22.937085: step 29410, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 17:03:29.966787: step 29420, loss = 1.03 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 17:03:37.162947: step 29430, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 17:03:44.343052: step 29440, loss = 1.01 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 17:03:51.570927: step 29450, loss = 0.99 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 17:03:58.769036: step 29460, loss = 1.15 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:04:05.998805: step 29470, loss = 1.01 (46.4 examples/sec; 0.689 sec/batch)
2018-10-16 17:04:13.164599: step 29480, loss = 1.13 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:04:20.347980: step 29490, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:04:27.582288: step 29500, loss = 1.12 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 17:04:38.058211: step 29510, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 17:04:45.268778: step 29520, loss = 1.17 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 17:04:52.435737: step 29530, loss = 1.20 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 17:04:59.695933: step 29540, loss = 1.16 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 17:05:06.916387: step 29550, loss = 1.23 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:05:14.170602: step 29560, loss = 1.14 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 17:05:21.439910: step 29570, loss = 1.15 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 17:05:28.693760: step 29580, loss = 1.06 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 17:05:35.908889: step 29590, loss = 1.09 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 17:05:43.105884: step 29600, loss = 1.00 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 17:05:52.992447: step 29610, loss = 1.02 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 17:06:00.178045: step 29620, loss = 1.01 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 17:06:07.304953: step 29630, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 17:06:14.510306: step 29640, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 17:06:21.701035: step 29650, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 17:06:28.998252: step 29660, loss = 1.10 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 17:06:36.253800: step 29670, loss = 1.06 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:06:43.384292: step 29680, loss = 1.07 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 17:06:50.582829: step 29690, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 17:06:57.794609: step 29700, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:07:07.967824: step 29710, loss = 0.99 (46.3 examples/sec; 0.690 sec/batch)
2018-10-16 17:07:15.198289: step 29720, loss = 1.21 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 17:07:22.338284: step 29730, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 17:07:29.503189: step 29740, loss = 1.03 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 17:07:36.703309: step 29750, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 17:07:43.916406: step 29760, loss = 1.09 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 17:07:51.132822: step 29770, loss = 1.00 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 17:07:58.335163: step 29780, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 17:08:05.461766: step 29790, loss = 0.99 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 17:08:12.649963: step 29800, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:08:22.518875: step 29810, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 17:08:29.697286: step 29820, loss = 1.00 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 17:08:36.846382: step 29830, loss = 1.17 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 17:08:44.055999: step 29840, loss = 1.03 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 17:08:51.243126: step 29850, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 17:08:58.409131: step 29860, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 17:09:05.636065: step 29870, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 17:09:12.829343: step 29880, loss = 0.99 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 17:09:19.974714: step 29890, loss = 1.08 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 17:09:27.106342: step 29900, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 17:09:37.856253: step 29910, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 17:09:45.064115: step 29920, loss = 1.27 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 17:09:52.274661: step 29930, loss = 0.99 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 17:09:59.428979: step 29940, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:10:06.542408: step 29950, loss = 1.01 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 17:10:13.744718: step 29960, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 17:10:20.885993: step 29970, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 17:10:28.003968: step 29980, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 17:10:35.204436: step 29990, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:10:42.265236: step 30000, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 17:10:55.441400: step 30010, loss = 0.99 (48.1 examples/sec; 0.665 sec/batch)
2018-10-16 17:11:02.298879: step 30020, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:11:09.550742: step 30030, loss = 1.04 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 17:11:16.766891: step 30040, loss = 1.09 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 17:11:23.984336: step 30050, loss = 1.08 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 17:11:31.186214: step 30060, loss = 1.00 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 17:11:38.353634: step 30070, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:11:45.532012: step 30080, loss = 1.06 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:11:52.688322: step 30090, loss = 1.14 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 17:11:59.924299: step 30100, loss = 1.00 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 17:12:09.703961: step 30110, loss = 1.07 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 17:12:16.894295: step 30120, loss = 1.04 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 17:12:24.104311: step 30130, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 17:12:31.296892: step 30140, loss = 1.08 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 17:12:38.577316: step 30150, loss = 1.11 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 17:12:45.737799: step 30160, loss = 1.06 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 17:12:52.937801: step 30170, loss = 1.00 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 17:13:00.192616: step 30180, loss = 1.10 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 17:13:07.326777: step 30190, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 17:13:14.505823: step 30200, loss = 1.06 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 17:13:24.278369: step 30210, loss = 1.03 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 17:13:31.546914: step 30220, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 17:13:38.723995: step 30230, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:13:45.890214: step 30240, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:13:53.147914: step 30250, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 17:14:00.319716: step 30260, loss = 1.04 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 17:14:07.516074: step 30270, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 17:14:14.699891: step 30280, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 17:14:21.902988: step 30290, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 17:14:29.053962: step 30300, loss = 1.13 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:14:39.433930: step 30310, loss = 1.08 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 17:14:46.568685: step 30320, loss = 1.09 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 17:14:53.787057: step 30330, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 17:15:00.954705: step 30340, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 17:15:08.176551: step 30350, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 17:15:15.405420: step 30360, loss = 1.09 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 17:15:22.581781: step 30370, loss = 1.04 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 17:15:29.760147: step 30380, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 17:15:36.920004: step 30390, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 17:15:44.096940: step 30400, loss = 1.05 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 17:15:53.924972: step 30410, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:16:01.086678: step 30420, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 17:16:08.285664: step 30430, loss = 1.03 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 17:16:15.446346: step 30440, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 17:16:22.649051: step 30450, loss = 1.09 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 17:16:29.886272: step 30460, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 17:16:37.130980: step 30470, loss = 1.03 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 17:16:44.405427: step 30480, loss = 1.20 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 17:16:51.627372: step 30490, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 17:16:58.814751: step 30500, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 17:17:08.598256: step 30510, loss = 1.11 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:17:15.752734: step 30520, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 17:17:22.935851: step 30530, loss = 1.08 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 17:17:30.162276: step 30540, loss = 1.02 (42.6 examples/sec; 0.752 sec/batch)
2018-10-16 17:17:37.335502: step 30550, loss = 1.11 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 17:17:44.493383: step 30560, loss = 1.10 (47.4 examples/sec; 0.675 sec/batch)
2018-10-16 17:17:51.648711: step 30570, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 17:17:58.834093: step 30580, loss = 1.00 (42.9 examples/sec; 0.747 sec/batch)
2018-10-16 17:18:05.998375: step 30590, loss = 1.01 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 17:18:13.150283: step 30600, loss = 0.99 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 17:18:23.190065: step 30610, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 17:18:30.396833: step 30620, loss = 1.10 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 17:18:37.569231: step 30630, loss = 1.07 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 17:18:44.717414: step 30640, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 17:18:51.893412: step 30650, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:18:59.066826: step 30660, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 17:19:06.231333: step 30670, loss = 1.06 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 17:19:13.457696: step 30680, loss = 1.06 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 17:19:20.671355: step 30690, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:19:27.869453: step 30700, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 17:19:38.111197: step 30710, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 17:19:45.269606: step 30720, loss = 1.06 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:19:52.403675: step 30730, loss = 1.05 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 17:19:59.572514: step 30740, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 17:20:06.746790: step 30750, loss = 1.03 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 17:20:13.996539: step 30760, loss = 0.99 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 17:20:21.166302: step 30770, loss = 1.00 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 17:20:28.417771: step 30780, loss = 0.99 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 17:20:35.574935: step 30790, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:20:42.706775: step 30800, loss = 0.99 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 17:20:52.397831: step 30810, loss = 1.01 (47.8 examples/sec; 0.669 sec/batch)
2018-10-16 17:20:59.515593: step 30820, loss = 1.03 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 17:21:06.714789: step 30830, loss = 1.02 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 17:21:13.874534: step 30840, loss = 1.05 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 17:21:21.155053: step 30850, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 17:21:28.373749: step 30860, loss = 1.08 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:21:35.568149: step 30870, loss = 1.08 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 17:21:42.791649: step 30880, loss = 0.99 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 17:21:49.977201: step 30890, loss = 1.12 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 17:21:57.219616: step 30900, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 17:22:07.074299: step 30910, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 17:22:14.213980: step 30920, loss = 1.12 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 17:22:21.384094: step 30930, loss = 1.06 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 17:22:28.567281: step 30940, loss = 1.18 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 17:22:35.678106: step 30950, loss = 1.01 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 17:22:42.941572: step 30960, loss = 0.99 (41.9 examples/sec; 0.764 sec/batch)
2018-10-16 17:22:50.121431: step 30970, loss = 1.08 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 17:22:57.363903: step 30980, loss = 1.03 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 17:23:04.480349: step 30990, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 17:23:11.709073: step 31000, loss = 1.07 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 17:23:21.453734: step 31010, loss = 1.13 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:23:28.623245: step 31020, loss = 1.15 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 17:23:35.779248: step 31030, loss = 1.04 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 17:23:42.906310: step 31040, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 17:23:50.104646: step 31050, loss = 1.07 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:23:57.332587: step 31060, loss = 1.14 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 17:24:04.508432: step 31070, loss = 0.99 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 17:24:11.624894: step 31080, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 17:24:18.783554: step 31090, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 17:24:25.977717: step 31100, loss = 1.17 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 17:24:35.947356: step 31110, loss = 1.08 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 17:24:43.106540: step 31120, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 17:24:50.236817: step 31130, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 17:24:57.434793: step 31140, loss = 1.03 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 17:25:04.546435: step 31150, loss = 1.09 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:25:11.704235: step 31160, loss = 1.17 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 17:25:18.819247: step 31170, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:25:26.047398: step 31180, loss = 1.14 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 17:25:33.319074: step 31190, loss = 1.00 (42.5 examples/sec; 0.753 sec/batch)
2018-10-16 17:25:40.508727: step 31200, loss = 1.03 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 17:25:50.319411: step 31210, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 17:25:57.413756: step 31220, loss = 1.11 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 17:26:04.556901: step 31230, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 17:26:11.802221: step 31240, loss = 1.11 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 17:26:18.951456: step 31250, loss = 1.05 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 17:26:26.120998: step 31260, loss = 1.04 (42.9 examples/sec; 0.747 sec/batch)
2018-10-16 17:26:33.278501: step 31270, loss = 1.19 (46.6 examples/sec; 0.687 sec/batch)
2018-10-16 17:26:40.476009: step 31280, loss = 1.17 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 17:26:47.670154: step 31290, loss = 0.99 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 17:26:54.827503: step 31300, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 17:27:04.535136: step 31310, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 17:27:11.669195: step 31320, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 17:27:18.876678: step 31330, loss = 1.07 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 17:27:26.080741: step 31340, loss = 0.99 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 17:27:33.319834: step 31350, loss = 1.20 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 17:27:40.490363: step 31360, loss = 0.99 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 17:27:47.697889: step 31370, loss = 1.11 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 17:27:54.904953: step 31380, loss = 1.01 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 17:28:02.118919: step 31390, loss = 1.06 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 17:28:09.359885: step 31400, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 17:28:19.476900: step 31410, loss = 0.99 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 17:28:26.693177: step 31420, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 17:28:33.853026: step 31430, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:28:40.953797: step 31440, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 17:28:48.206284: step 31450, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 17:28:55.356113: step 31460, loss = 1.20 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:29:02.518776: step 31470, loss = 1.09 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 17:29:09.677015: step 31480, loss = 0.99 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 17:29:16.894816: step 31490, loss = 1.01 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 17:29:24.136956: step 31500, loss = 1.01 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 17:29:33.853861: step 31510, loss = 1.05 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 17:29:41.058047: step 31520, loss = 0.99 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 17:29:48.270729: step 31530, loss = 1.17 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 17:29:55.353046: step 31540, loss = 1.10 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 17:30:02.507033: step 31550, loss = 0.99 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 17:30:09.681117: step 31560, loss = 1.01 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 17:30:16.859103: step 31570, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 17:30:24.100997: step 31580, loss = 1.05 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 17:30:31.324632: step 31590, loss = 1.08 (45.7 examples/sec; 0.699 sec/batch)
2018-10-16 17:30:38.504905: step 31600, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 17:30:48.543016: step 31610, loss = 1.04 (42.8 examples/sec; 0.749 sec/batch)
2018-10-16 17:30:55.665725: step 31620, loss = 1.07 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 17:31:02.825820: step 31630, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 17:31:10.012965: step 31640, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:31:17.207824: step 31650, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 17:31:24.417611: step 31660, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 17:31:31.614406: step 31670, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 17:31:38.856226: step 31680, loss = 1.11 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 17:31:46.036765: step 31690, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 17:31:53.208924: step 31700, loss = 1.01 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 17:32:02.864809: step 31710, loss = 1.07 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 17:32:09.982984: step 31720, loss = 1.21 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:32:17.161281: step 31730, loss = 1.00 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 17:32:24.387292: step 31740, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 17:32:31.533546: step 31750, loss = 0.99 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 17:32:38.729693: step 31760, loss = 1.02 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 17:32:45.900823: step 31770, loss = 1.02 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 17:32:53.105901: step 31780, loss = 1.05 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:33:00.359481: step 31790, loss = 1.06 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:33:07.535315: step 31800, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 17:33:17.615935: step 31810, loss = 1.11 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:33:24.765679: step 31820, loss = 1.09 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 17:33:32.041672: step 31830, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 17:33:39.266083: step 31840, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 17:33:46.460691: step 31850, loss = 1.14 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 17:33:53.526020: step 31860, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 17:34:00.821485: step 31870, loss = 1.12 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 17:34:08.039109: step 31880, loss = 1.12 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 17:34:15.245338: step 31890, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 17:34:22.392242: step 31900, loss = 1.10 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:34:32.242752: step 31910, loss = 0.99 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 17:34:39.411062: step 31920, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 17:34:46.630983: step 31930, loss = 1.11 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:34:53.921345: step 31940, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 17:35:01.062040: step 31950, loss = 1.01 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 17:35:08.246311: step 31960, loss = 1.03 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 17:35:15.443790: step 31970, loss = 1.07 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 17:35:22.642187: step 31980, loss = 1.13 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 17:35:29.889725: step 31990, loss = 1.01 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 17:35:37.053023: step 32000, loss = 1.01 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 17:35:46.740833: step 32010, loss = 1.10 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:35:53.855337: step 32020, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:36:01.036620: step 32030, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:36:08.228391: step 32040, loss = 1.18 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 17:36:15.326571: step 32050, loss = 1.08 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:36:22.517264: step 32060, loss = 1.00 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 17:36:29.679302: step 32070, loss = 1.09 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 17:36:36.896025: step 32080, loss = 1.09 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 17:36:44.061674: step 32090, loss = 1.02 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 17:36:51.211766: step 32100, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:37:00.914955: step 32110, loss = 1.07 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 17:37:08.110490: step 32120, loss = 1.16 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 17:37:15.292659: step 32130, loss = 1.04 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 17:37:22.553712: step 32140, loss = 1.08 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 17:37:29.764127: step 32150, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 17:37:36.952764: step 32160, loss = 1.16 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:37:44.100519: step 32170, loss = 1.04 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 17:37:51.268029: step 32180, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 17:37:58.437628: step 32190, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 17:38:05.581344: step 32200, loss = 1.10 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 17:38:15.890485: step 32210, loss = 1.08 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 17:38:23.054364: step 32220, loss = 0.99 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 17:38:30.293067: step 32230, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 17:38:37.463166: step 32240, loss = 1.18 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 17:38:44.572866: step 32250, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 17:38:51.787953: step 32260, loss = 1.06 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 17:38:58.897446: step 32270, loss = 1.20 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 17:39:06.028922: step 32280, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 17:39:13.245767: step 32290, loss = 1.06 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 17:39:20.379940: step 32300, loss = 1.02 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 17:39:30.091006: step 32310, loss = 1.26 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 17:39:37.243045: step 32320, loss = 1.09 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 17:39:44.475340: step 32330, loss = 1.00 (42.9 examples/sec; 0.747 sec/batch)
2018-10-16 17:39:51.604525: step 32340, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 17:39:58.800832: step 32350, loss = 1.04 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 17:40:06.016822: step 32360, loss = 1.11 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 17:40:13.215753: step 32370, loss = 1.13 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:40:20.356484: step 32380, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 17:40:27.552816: step 32390, loss = 1.01 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 17:40:34.813658: step 32400, loss = 1.09 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 17:40:44.646071: step 32410, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 17:40:51.787028: step 32420, loss = 1.19 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 17:40:58.972158: step 32430, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:41:06.097774: step 32440, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 17:41:13.297487: step 32450, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 17:41:20.529316: step 32460, loss = 1.08 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 17:41:27.779495: step 32470, loss = 1.09 (45.9 examples/sec; 0.696 sec/batch)
2018-10-16 17:41:34.937849: step 32480, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 17:41:42.054099: step 32490, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 17:41:49.230280: step 32500, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 17:41:59.035703: step 32510, loss = 1.18 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 17:42:06.172987: step 32520, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 17:42:13.438112: step 32530, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 17:42:20.598651: step 32540, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 17:42:27.858693: step 32550, loss = 1.02 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 17:42:35.084853: step 32560, loss = 1.04 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 17:42:42.280016: step 32570, loss = 1.03 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 17:42:49.412230: step 32580, loss = 1.11 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 17:42:56.655977: step 32590, loss = 1.01 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 17:43:03.798178: step 32600, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:43:13.637413: step 32610, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 17:43:20.802919: step 32620, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 17:43:27.987248: step 32630, loss = 1.19 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 17:43:35.182132: step 32640, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:43:42.476951: step 32650, loss = 1.00 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 17:43:49.680755: step 32660, loss = 1.13 (42.3 examples/sec; 0.756 sec/batch)
2018-10-16 17:43:56.801519: step 32670, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:44:04.027605: step 32680, loss = 1.07 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 17:44:11.191993: step 32690, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 17:44:18.357495: step 32700, loss = 1.29 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 17:44:27.904869: step 32710, loss = 1.03 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 17:44:35.038785: step 32720, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 17:44:42.276904: step 32730, loss = 1.01 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 17:44:49.483132: step 32740, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 17:44:56.690803: step 32750, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 17:45:03.859571: step 32760, loss = 1.09 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 17:45:11.024494: step 32770, loss = 1.04 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 17:45:18.215282: step 32780, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:45:25.415679: step 32790, loss = 1.09 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 17:45:32.669847: step 32800, loss = 1.00 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 17:45:42.817034: step 32810, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 17:45:49.998893: step 32820, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 17:45:57.231218: step 32830, loss = 1.02 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 17:46:04.411977: step 32840, loss = 1.03 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 17:46:11.604646: step 32850, loss = 0.99 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 17:46:18.793815: step 32860, loss = 1.21 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:46:25.898489: step 32870, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:46:33.087504: step 32880, loss = 1.07 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 17:46:40.272393: step 32890, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 17:46:47.379534: step 32900, loss = 1.11 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 17:46:57.316096: step 32910, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 17:47:04.448867: step 32920, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 17:47:11.630454: step 32930, loss = 1.11 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:47:18.833934: step 32940, loss = 1.09 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 17:47:25.957298: step 32950, loss = 1.16 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:47:33.151528: step 32960, loss = 1.07 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 17:47:40.321191: step 32970, loss = 1.02 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 17:47:47.563743: step 32980, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 17:47:54.715531: step 32990, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 17:48:01.847128: step 33000, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 17:48:11.635617: step 33010, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 17:48:18.814036: step 33020, loss = 1.06 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 17:48:26.024622: step 33030, loss = 1.01 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 17:48:33.209391: step 33040, loss = 1.08 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:48:40.437820: step 33050, loss = 1.00 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 17:48:47.559225: step 33060, loss = 0.99 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 17:48:54.725883: step 33070, loss = 1.02 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 17:49:01.938879: step 33080, loss = 1.00 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 17:49:09.056470: step 33090, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 17:49:16.261804: step 33100, loss = 1.10 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 17:49:25.974598: step 33110, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 17:49:33.096522: step 33120, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 17:49:40.293477: step 33130, loss = 1.06 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 17:49:47.418232: step 33140, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 17:49:54.556978: step 33150, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:50:01.844425: step 33160, loss = 1.12 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 17:50:09.015592: step 33170, loss = 1.12 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 17:50:16.106327: step 33180, loss = 1.17 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:50:23.253395: step 33190, loss = 1.16 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 17:50:30.488669: step 33200, loss = 1.16 (42.9 examples/sec; 0.747 sec/batch)
2018-10-16 17:50:40.157750: step 33210, loss = 1.08 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 17:50:47.277476: step 33220, loss = 1.09 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 17:50:54.479116: step 33230, loss = 1.04 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 17:51:01.638282: step 33240, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:51:08.888031: step 33250, loss = 1.01 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 17:51:16.079574: step 33260, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 17:51:23.241727: step 33270, loss = 1.05 (42.3 examples/sec; 0.757 sec/batch)
2018-10-16 17:51:30.362519: step 33280, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:51:37.618964: step 33290, loss = 1.15 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 17:51:44.767611: step 33300, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 17:51:55.351714: step 33310, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 17:52:02.472079: step 33320, loss = 1.05 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 17:52:09.606334: step 33330, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:52:16.794940: step 33340, loss = 1.10 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 17:52:24.024412: step 33350, loss = 1.07 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 17:52:31.095143: step 33360, loss = 1.13 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 17:52:38.268461: step 33370, loss = 0.99 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 17:52:45.460040: step 33380, loss = 1.11 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 17:52:52.623240: step 33390, loss = 1.14 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 17:52:59.764082: step 33400, loss = 1.04 (46.3 examples/sec; 0.690 sec/batch)
2018-10-16 17:53:10.037670: step 33410, loss = 1.15 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 17:53:17.170344: step 33420, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:53:24.323390: step 33430, loss = 1.06 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 17:53:31.495812: step 33440, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 17:53:38.602075: step 33450, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:53:45.796305: step 33460, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 17:53:52.994530: step 33470, loss = 1.04 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 17:54:00.118858: step 33480, loss = 1.28 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 17:54:07.364840: step 33490, loss = 1.00 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 17:54:14.496834: step 33500, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 17:54:24.092783: step 33510, loss = 1.16 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 17:54:31.158563: step 33520, loss = 1.26 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:54:38.333863: step 33530, loss = 1.05 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:54:45.475965: step 33540, loss = 1.07 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 17:54:52.569657: step 33550, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 17:54:59.828000: step 33560, loss = 1.09 (43.0 examples/sec; 0.743 sec/batch)
2018-10-16 17:55:07.057749: step 33570, loss = 1.13 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 17:55:14.205414: step 33580, loss = 1.10 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:55:21.400815: step 33590, loss = 1.08 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 17:55:28.584059: step 33600, loss = 1.05 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 17:55:38.368340: step 33610, loss = 1.03 (46.7 examples/sec; 0.686 sec/batch)
2018-10-16 17:55:45.534906: step 33620, loss = 1.00 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 17:55:52.648774: step 33630, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 17:55:59.878376: step 33640, loss = 1.07 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 17:56:07.056272: step 33650, loss = 1.02 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 17:56:14.231988: step 33660, loss = 1.12 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 17:56:21.399599: step 33670, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 17:56:28.531490: step 33680, loss = 1.01 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 17:56:35.655440: step 33690, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 17:56:42.792328: step 33700, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:56:53.105456: step 33710, loss = 1.01 (46.9 examples/sec; 0.683 sec/batch)
2018-10-16 17:57:00.203112: step 33720, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 17:57:07.322897: step 33730, loss = 1.03 (46.3 examples/sec; 0.690 sec/batch)
2018-10-16 17:57:14.458155: step 33740, loss = 1.08 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 17:57:21.635898: step 33750, loss = 1.17 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 17:57:28.750426: step 33760, loss = 1.04 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 17:57:35.970207: step 33770, loss = 1.10 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 17:57:43.182208: step 33780, loss = 1.11 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 17:57:50.212862: step 33790, loss = 1.31 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 17:57:57.416118: step 33800, loss = 1.13 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 17:58:07.079776: step 33810, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 17:58:14.196696: step 33820, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:58:21.329300: step 33830, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 17:58:28.470605: step 33840, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 17:58:35.620644: step 33850, loss = 1.05 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 17:58:42.742371: step 33860, loss = 1.16 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 17:58:49.968566: step 33870, loss = 1.07 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 17:58:57.173583: step 33880, loss = 1.09 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 17:59:04.265623: step 33890, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 17:59:11.430268: step 33900, loss = 1.05 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 17:59:21.339347: step 33910, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 17:59:28.444086: step 33920, loss = 1.13 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 17:59:35.536343: step 33930, loss = 1.06 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 17:59:42.709490: step 33940, loss = 0.99 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 17:59:49.872363: step 33950, loss = 1.01 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 17:59:56.995928: step 33960, loss = 1.04 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 18:00:04.119026: step 33970, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:00:11.322089: step 33980, loss = 1.07 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 18:00:18.504233: step 33990, loss = 1.07 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:00:25.636545: step 34000, loss = 1.13 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 18:00:35.357913: step 34010, loss = 1.07 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 18:00:42.533854: step 34020, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 18:00:49.668797: step 34030, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 18:00:56.849820: step 34040, loss = 1.12 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:01:04.029867: step 34050, loss = 1.06 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 18:01:11.130631: step 34060, loss = 1.15 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 18:01:18.358590: step 34070, loss = 1.11 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 18:01:25.568022: step 34080, loss = 1.08 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 18:01:32.715421: step 34090, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 18:01:39.926789: step 34100, loss = 1.00 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 18:01:49.723292: step 34110, loss = 1.03 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 18:01:56.829374: step 34120, loss = 1.05 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 18:02:03.939152: step 34130, loss = 1.00 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 18:02:11.114283: step 34140, loss = 1.08 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 18:02:18.222170: step 34150, loss = 1.06 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 18:02:25.312811: step 34160, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 18:02:32.576547: step 34170, loss = 1.05 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 18:02:39.783297: step 34180, loss = 1.00 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 18:02:47.016888: step 34190, loss = 1.11 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 18:02:54.138291: step 34200, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 18:03:03.917314: step 34210, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 18:03:11.026879: step 34220, loss = 1.08 (46.6 examples/sec; 0.686 sec/batch)
2018-10-16 18:03:18.165533: step 34230, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 18:03:25.318947: step 34240, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 18:03:32.614786: step 34250, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 18:03:39.705487: step 34260, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 18:03:46.888355: step 34270, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:03:53.961290: step 34280, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 18:04:01.076347: step 34290, loss = 1.16 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 18:04:08.257252: step 34300, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:04:17.975009: step 34310, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:04:25.164756: step 34320, loss = 1.03 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 18:04:32.273704: step 34330, loss = 1.08 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 18:04:39.357799: step 34340, loss = 1.27 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 18:04:46.507917: step 34350, loss = 1.07 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 18:04:53.693645: step 34360, loss = 1.05 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 18:05:00.776677: step 34370, loss = 1.17 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:05:07.946399: step 34380, loss = 1.12 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 18:05:15.107197: step 34390, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 18:05:22.248205: step 34400, loss = 1.04 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 18:05:32.080442: step 34410, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 18:05:39.182045: step 34420, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 18:05:46.334391: step 34430, loss = 1.08 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 18:05:53.426495: step 34440, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 18:06:00.559382: step 34450, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 18:06:07.724723: step 34460, loss = 1.11 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 18:06:14.886218: step 34470, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 18:06:21.990678: step 34480, loss = 1.00 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 18:06:29.178655: step 34490, loss = 1.09 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 18:06:36.365460: step 34500, loss = 1.24 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:06:46.353704: step 34510, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 18:06:53.409456: step 34520, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 18:07:00.603499: step 34530, loss = 1.17 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 18:07:07.788434: step 34540, loss = 1.02 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 18:07:14.933931: step 34550, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 18:07:22.080924: step 34560, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 18:07:29.214279: step 34570, loss = 1.03 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 18:07:36.359609: step 34580, loss = 1.40 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:07:43.472993: step 34590, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 18:07:50.610876: step 34600, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:08:00.276717: step 34610, loss = 1.03 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 18:08:07.504104: step 34620, loss = 1.13 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 18:08:14.725019: step 34630, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:08:21.919224: step 34640, loss = 1.05 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:08:29.045210: step 34650, loss = 1.07 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 18:08:36.215326: step 34660, loss = 1.01 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 18:08:43.383924: step 34670, loss = 1.03 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 18:08:50.532473: step 34680, loss = 1.03 (44.4 examples/sec; 0.722 sec/batch)
2018-10-16 18:08:57.690054: step 34690, loss = 1.14 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 18:09:04.872818: step 34700, loss = 1.09 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 18:09:14.659998: step 34710, loss = 1.16 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:09:21.772138: step 34720, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 18:09:28.966760: step 34730, loss = 1.01 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 18:09:36.100870: step 34740, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 18:09:43.316612: step 34750, loss = 1.06 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 18:09:50.519494: step 34760, loss = 1.03 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:09:57.732894: step 34770, loss = 1.17 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 18:10:04.863818: step 34780, loss = 1.14 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 18:10:12.118357: step 34790, loss = 1.02 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 18:10:19.291124: step 34800, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 18:10:28.945979: step 34810, loss = 1.19 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 18:10:36.098927: step 34820, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 18:10:43.243782: step 34830, loss = 1.12 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 18:10:50.351439: step 34840, loss = 1.08 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 18:10:57.521731: step 34850, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 18:11:04.600746: step 34860, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 18:11:11.797034: step 34870, loss = 1.08 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 18:11:19.083789: step 34880, loss = 1.12 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 18:11:26.174059: step 34890, loss = 1.05 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:11:33.346316: step 34900, loss = 1.01 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 18:11:43.007275: step 34910, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 18:11:50.187346: step 34920, loss = 1.09 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 18:11:57.356736: step 34930, loss = 1.11 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 18:12:04.495013: step 34940, loss = 1.01 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 18:12:11.726202: step 34950, loss = 1.05 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 18:12:18.878506: step 34960, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 18:12:26.027167: step 34970, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:12:33.070151: step 34980, loss = 1.03 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 18:12:40.305057: step 34990, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 18:12:47.380217: step 35000, loss = 1.02 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 18:12:59.953296: step 35010, loss = 1.02 (47.6 examples/sec; 0.672 sec/batch)
2018-10-16 18:13:06.856375: step 35020, loss = 1.01 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 18:13:14.033046: step 35030, loss = 1.09 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:13:21.206576: step 35040, loss = 1.10 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:13:28.376452: step 35050, loss = 1.04 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 18:13:35.607613: step 35060, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 18:13:42.845558: step 35070, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 18:13:49.941042: step 35080, loss = 1.00 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 18:13:57.050951: step 35090, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 18:14:04.249203: step 35100, loss = 1.02 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 18:14:14.013343: step 35110, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 18:14:21.179203: step 35120, loss = 1.17 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:14:28.396193: step 35130, loss = 1.24 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 18:14:35.613942: step 35140, loss = 1.01 (42.3 examples/sec; 0.757 sec/batch)
2018-10-16 18:14:42.749663: step 35150, loss = 1.07 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 18:14:49.903558: step 35160, loss = 1.04 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 18:14:57.032120: step 35170, loss = 1.02 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 18:15:04.299674: step 35180, loss = 1.08 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 18:15:11.506108: step 35190, loss = 1.06 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:15:18.684403: step 35200, loss = 1.21 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:15:28.536667: step 35210, loss = 0.99 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 18:15:35.598572: step 35220, loss = 1.13 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 18:15:42.738370: step 35230, loss = 1.09 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 18:15:49.918427: step 35240, loss = 1.01 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 18:15:57.049711: step 35250, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 18:16:04.259824: step 35260, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 18:16:11.495405: step 35270, loss = 1.19 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 18:16:18.656129: step 35280, loss = 1.01 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 18:16:25.854197: step 35290, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 18:16:32.966887: step 35300, loss = 1.07 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:16:42.817103: step 35310, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 18:16:49.979230: step 35320, loss = 1.05 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 18:16:57.115164: step 35330, loss = 1.13 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 18:17:04.280605: step 35340, loss = 1.02 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 18:17:11.512626: step 35350, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 18:17:18.740997: step 35360, loss = 1.01 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 18:17:25.926676: step 35370, loss = 1.06 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 18:17:33.112319: step 35380, loss = 1.04 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 18:17:40.236843: step 35390, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 18:17:47.378170: step 35400, loss = 1.04 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 18:17:57.064601: step 35410, loss = 1.19 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:18:04.239003: step 35420, loss = 1.03 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 18:18:11.338831: step 35430, loss = 1.08 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 18:18:18.536222: step 35440, loss = 1.05 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:18:25.653671: step 35450, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 18:18:32.783763: step 35460, loss = 0.99 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 18:18:39.930543: step 35470, loss = 1.01 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 18:18:47.189806: step 35480, loss = 1.09 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 18:18:54.324178: step 35490, loss = 1.12 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 18:19:01.460594: step 35500, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 18:19:11.202986: step 35510, loss = 1.12 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 18:19:18.411258: step 35520, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 18:19:25.561605: step 35530, loss = 1.06 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:19:32.765190: step 35540, loss = 1.09 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:19:39.905526: step 35550, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 18:19:47.099529: step 35560, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 18:19:54.264729: step 35570, loss = 1.14 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 18:20:01.412373: step 35580, loss = 1.01 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 18:20:08.504347: step 35590, loss = 1.00 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 18:20:15.683333: step 35600, loss = 0.99 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 18:20:25.461225: step 35610, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:20:32.580073: step 35620, loss = 1.11 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 18:20:39.785584: step 35630, loss = 1.15 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 18:20:46.928404: step 35640, loss = 1.00 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 18:20:54.052570: step 35650, loss = 1.02 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 18:21:01.270250: step 35660, loss = 0.99 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 18:21:08.399635: step 35670, loss = 1.05 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 18:21:15.567234: step 35680, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:21:22.780169: step 35690, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:21:29.987442: step 35700, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 18:21:39.675689: step 35710, loss = 1.08 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 18:21:46.781916: step 35720, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 18:21:53.884667: step 35730, loss = 1.02 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 18:22:01.031509: step 35740, loss = 1.02 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 18:22:08.145046: step 35750, loss = 1.05 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 18:22:15.282931: step 35760, loss = 1.02 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 18:22:22.449519: step 35770, loss = 1.09 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 18:22:29.578306: step 35780, loss = 1.13 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 18:22:36.756203: step 35790, loss = 0.99 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 18:22:43.889194: step 35800, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:22:53.724586: step 35810, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 18:23:00.819482: step 35820, loss = 1.15 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:23:07.996124: step 35830, loss = 1.08 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 18:23:15.136839: step 35840, loss = 1.04 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 18:23:22.274232: step 35850, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 18:23:29.379392: step 35860, loss = 0.99 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 18:23:36.491927: step 35870, loss = 1.04 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:23:43.639449: step 35880, loss = 1.12 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 18:23:50.771338: step 35890, loss = 1.04 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 18:23:57.936936: step 35900, loss = 1.01 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 18:24:07.650430: step 35910, loss = 1.12 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:24:14.726321: step 35920, loss = 0.99 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 18:24:21.871394: step 35930, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 18:24:29.028354: step 35940, loss = 1.05 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 18:24:36.199313: step 35950, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:24:43.446667: step 35960, loss = 1.02 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 18:24:50.636198: step 35970, loss = 1.14 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 18:24:57.820941: step 35980, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 18:25:04.988789: step 35990, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 18:25:12.126860: step 36000, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 18:25:21.829793: step 36010, loss = 0.99 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 18:25:28.948334: step 36020, loss = 1.08 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 18:25:36.163768: step 36030, loss = 1.07 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 18:25:43.305308: step 36040, loss = 0.99 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 18:25:50.503150: step 36050, loss = 1.04 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 18:25:57.628458: step 36060, loss = 1.03 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 18:26:04.733888: step 36070, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 18:26:11.915523: step 36080, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 18:26:18.993233: step 36090, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 18:26:26.084865: step 36100, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:26:35.795181: step 36110, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:26:42.933512: step 36120, loss = 1.13 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:26:50.119528: step 36130, loss = 1.04 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 18:26:57.231258: step 36140, loss = 1.08 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 18:27:04.398781: step 36150, loss = 1.14 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 18:27:11.551001: step 36160, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 18:27:18.733345: step 36170, loss = 0.99 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 18:27:25.861977: step 36180, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:27:32.974886: step 36190, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:27:40.155500: step 36200, loss = 1.00 (42.6 examples/sec; 0.750 sec/batch)
2018-10-16 18:27:49.949677: step 36210, loss = 1.23 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 18:27:57.010598: step 36220, loss = 1.01 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 18:28:04.114988: step 36230, loss = 1.06 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 18:28:11.292849: step 36240, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:28:18.527355: step 36250, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 18:28:25.695837: step 36260, loss = 1.00 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 18:28:32.839995: step 36270, loss = 0.99 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 18:28:39.962064: step 36280, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 18:28:47.151762: step 36290, loss = 1.00 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 18:28:54.250404: step 36300, loss = 0.99 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 18:29:04.649988: step 36310, loss = 1.07 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 18:29:11.739763: step 36320, loss = 1.11 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:29:18.953221: step 36330, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 18:29:26.115425: step 36340, loss = 1.00 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 18:29:33.220133: step 36350, loss = 1.02 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 18:29:40.352959: step 36360, loss = 1.04 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 18:29:47.498883: step 36370, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 18:29:54.649129: step 36380, loss = 1.06 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:30:01.800959: step 36390, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 18:30:08.956411: step 36400, loss = 1.06 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 18:30:18.599233: step 36410, loss = 1.17 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 18:30:25.715443: step 36420, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:30:32.877066: step 36430, loss = 1.12 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 18:30:40.054907: step 36440, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 18:30:47.335429: step 36450, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 18:30:54.534093: step 36460, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:31:01.717783: step 36470, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 18:31:08.846527: step 36480, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:31:16.021780: step 36490, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 18:31:23.316091: step 36500, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:31:32.995635: step 36510, loss = 1.05 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 18:31:40.193483: step 36520, loss = 1.21 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 18:31:47.274623: step 36530, loss = 1.04 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:31:54.384075: step 36540, loss = 1.01 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 18:32:01.526611: step 36550, loss = 1.20 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:32:08.744059: step 36560, loss = 1.10 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 18:32:15.943001: step 36570, loss = 1.09 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 18:32:23.064888: step 36580, loss = 1.06 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:32:30.219635: step 36590, loss = 1.17 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 18:32:37.368080: step 36600, loss = 0.99 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 18:32:47.463241: step 36610, loss = 1.04 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 18:32:54.571218: step 36620, loss = 1.04 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 18:33:01.838772: step 36630, loss = 1.11 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:33:08.963687: step 36640, loss = 1.02 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 18:33:16.090404: step 36650, loss = 1.06 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 18:33:23.275811: step 36660, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 18:33:30.418918: step 36670, loss = 1.14 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 18:33:37.596430: step 36680, loss = 1.02 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 18:33:44.715240: step 36690, loss = 1.06 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 18:33:51.900600: step 36700, loss = 1.11 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:34:02.081080: step 36710, loss = 1.03 (42.6 examples/sec; 0.750 sec/batch)
2018-10-16 18:34:09.181162: step 36720, loss = 1.05 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 18:34:16.311388: step 36730, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 18:34:23.579022: step 36740, loss = 1.07 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 18:34:30.781862: step 36750, loss = 1.00 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 18:34:37.910470: step 36760, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:34:45.068992: step 36770, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 18:34:52.247116: step 36780, loss = 1.00 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 18:34:59.456407: step 36790, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:35:06.639887: step 36800, loss = 1.06 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 18:35:16.318878: step 36810, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 18:35:23.454167: step 36820, loss = 1.15 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 18:35:30.556770: step 36830, loss = 1.00 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 18:35:37.762764: step 36840, loss = 1.03 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 18:35:44.866015: step 36850, loss = 1.07 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 18:35:51.972980: step 36860, loss = 1.04 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:35:59.113879: step 36870, loss = 1.11 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 18:36:06.263628: step 36880, loss = 1.18 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 18:36:13.445028: step 36890, loss = 1.03 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 18:36:20.624917: step 36900, loss = 1.02 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 18:36:30.517590: step 36910, loss = 1.33 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 18:36:37.653150: step 36920, loss = 1.09 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 18:36:44.715897: step 36930, loss = 1.20 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:36:51.856749: step 36940, loss = 1.02 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 18:36:58.983592: step 36950, loss = 1.09 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:37:06.198100: step 36960, loss = 0.99 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 18:37:13.364829: step 36970, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 18:37:20.515485: step 36980, loss = 1.11 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:37:27.775451: step 36990, loss = 1.05 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 18:37:34.902098: step 37000, loss = 1.06 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 18:37:44.917999: step 37010, loss = 0.99 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 18:37:52.098873: step 37020, loss = 1.00 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 18:37:59.227567: step 37030, loss = 1.12 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:38:06.424920: step 37040, loss = 1.08 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:38:13.604172: step 37050, loss = 0.99 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 18:38:20.773987: step 37060, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:38:27.923875: step 37070, loss = 1.03 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 18:38:35.050006: step 37080, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 18:38:42.104260: step 37090, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 18:38:49.348687: step 37100, loss = 1.03 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 18:38:59.289271: step 37110, loss = 1.09 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:39:06.427791: step 37120, loss = 1.04 (42.4 examples/sec; 0.754 sec/batch)
2018-10-16 18:39:13.523401: step 37130, loss = 1.20 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 18:39:20.708470: step 37140, loss = 0.99 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 18:39:27.924517: step 37150, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 18:39:35.054041: step 37160, loss = 1.05 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 18:39:42.265529: step 37170, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 18:39:49.372619: step 37180, loss = 1.01 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 18:39:56.544325: step 37190, loss = 1.02 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 18:40:03.723523: step 37200, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:40:13.449934: step 37210, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 18:40:20.588080: step 37220, loss = 1.11 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 18:40:27.799934: step 37230, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 18:40:34.971147: step 37240, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 18:40:42.117226: step 37250, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:40:49.337285: step 37260, loss = 1.07 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:40:56.533649: step 37270, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 18:41:03.688797: step 37280, loss = 0.99 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 18:41:10.838820: step 37290, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:41:18.022929: step 37300, loss = 1.14 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 18:41:27.727542: step 37310, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:41:34.787060: step 37320, loss = 1.16 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 18:41:41.956604: step 37330, loss = 0.99 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 18:41:49.087504: step 37340, loss = 1.10 (46.6 examples/sec; 0.686 sec/batch)
2018-10-16 18:41:56.281483: step 37350, loss = 1.15 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 18:42:03.429111: step 37360, loss = 1.01 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 18:42:10.557748: step 37370, loss = 1.10 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 18:42:17.663939: step 37380, loss = 1.24 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 18:42:24.822935: step 37390, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 18:42:31.973973: step 37400, loss = 1.01 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 18:42:41.772801: step 37410, loss = 1.16 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 18:42:48.904190: step 37420, loss = 1.03 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 18:42:56.079129: step 37430, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 18:43:03.261161: step 37440, loss = 0.99 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 18:43:10.453452: step 37450, loss = 1.25 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 18:43:17.646943: step 37460, loss = 1.04 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 18:43:24.799151: step 37470, loss = 1.00 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 18:43:31.984698: step 37480, loss = 1.07 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:43:39.169276: step 37490, loss = 1.07 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 18:43:46.338348: step 37500, loss = 1.14 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 18:43:56.182866: step 37510, loss = 1.04 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 18:44:03.245361: step 37520, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 18:44:10.423342: step 37530, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:44:17.585196: step 37540, loss = 1.06 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 18:44:24.719426: step 37550, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 18:44:31.898402: step 37560, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 18:44:39.010924: step 37570, loss = 1.04 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 18:44:46.257793: step 37580, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 18:44:53.358339: step 37590, loss = 1.08 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:45:00.443231: step 37600, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:45:10.750861: step 37610, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 18:45:17.898613: step 37620, loss = 1.06 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 18:45:25.058319: step 37630, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:45:32.266240: step 37640, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:45:39.388348: step 37650, loss = 1.04 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 18:45:46.606619: step 37660, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:45:53.778642: step 37670, loss = 1.00 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 18:46:00.953812: step 37680, loss = 1.10 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 18:46:08.088380: step 37690, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 18:46:15.316257: step 37700, loss = 1.07 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:46:25.023173: step 37710, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:46:32.184394: step 37720, loss = 1.06 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 18:46:39.268866: step 37730, loss = 1.05 (46.6 examples/sec; 0.687 sec/batch)
2018-10-16 18:46:46.517445: step 37740, loss = 1.09 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 18:46:53.666335: step 37750, loss = 1.02 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 18:47:00.831232: step 37760, loss = 1.19 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 18:47:07.975036: step 37770, loss = 1.14 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 18:47:15.108461: step 37780, loss = 1.28 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 18:47:22.281574: step 37790, loss = 1.02 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 18:47:29.506298: step 37800, loss = 1.06 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:47:39.273203: step 37810, loss = 0.99 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 18:47:46.381974: step 37820, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:47:53.531681: step 37830, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 18:48:00.734897: step 37840, loss = 1.12 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 18:48:07.923427: step 37850, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:48:15.018447: step 37860, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 18:48:22.175469: step 37870, loss = 1.11 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 18:48:29.276999: step 37880, loss = 1.00 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 18:48:36.373250: step 37890, loss = 1.01 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 18:48:43.555767: step 37900, loss = 1.11 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 18:48:53.241894: step 37910, loss = 1.06 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 18:49:00.372242: step 37920, loss = 0.99 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 18:49:07.561208: step 37930, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:49:14.670576: step 37940, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 18:49:21.818018: step 37950, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 18:49:29.019307: step 37960, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:49:36.159927: step 37970, loss = 1.03 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 18:49:43.276905: step 37980, loss = 1.10 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 18:49:50.408421: step 37990, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 18:49:57.577511: step 38000, loss = 1.19 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:50:07.562531: step 38010, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 18:50:14.563486: step 38020, loss = 1.06 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:50:21.687743: step 38030, loss = 1.00 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 18:50:28.844590: step 38040, loss = 1.01 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 18:50:36.008171: step 38050, loss = 1.06 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 18:50:43.201341: step 38060, loss = 1.06 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:50:50.380018: step 38070, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:50:57.578267: step 38080, loss = 1.02 (47.5 examples/sec; 0.673 sec/batch)
2018-10-16 18:51:04.717108: step 38090, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:51:11.902303: step 38100, loss = 1.06 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 18:51:21.694084: step 38110, loss = 1.10 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 18:51:28.778716: step 38120, loss = 1.00 (47.3 examples/sec; 0.676 sec/batch)
2018-10-16 18:51:36.028026: step 38130, loss = 1.15 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:51:43.260820: step 38140, loss = 1.01 (42.9 examples/sec; 0.747 sec/batch)
2018-10-16 18:51:50.398835: step 38150, loss = 1.00 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 18:51:57.582196: step 38160, loss = 1.02 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 18:52:04.631845: step 38170, loss = 1.00 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 18:52:11.777988: step 38180, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:52:18.890971: step 38190, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:52:26.024322: step 38200, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 18:52:35.811858: step 38210, loss = 1.04 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 18:52:42.931800: step 38220, loss = 1.15 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 18:52:50.100224: step 38230, loss = 1.02 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 18:52:57.137400: step 38240, loss = 1.06 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 18:53:04.243828: step 38250, loss = 1.05 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 18:53:11.414480: step 38260, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 18:53:18.553133: step 38270, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:53:25.751459: step 38280, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 18:53:32.901493: step 38290, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:53:39.983789: step 38300, loss = 1.05 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 18:53:49.598983: step 38310, loss = 1.03 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 18:53:56.681178: step 38320, loss = 1.20 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 18:54:03.800138: step 38330, loss = 1.00 (47.1 examples/sec; 0.680 sec/batch)
2018-10-16 18:54:11.051797: step 38340, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:54:18.233340: step 38350, loss = 1.04 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 18:54:25.464583: step 38360, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:54:32.560913: step 38370, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 18:54:39.710332: step 38380, loss = 1.02 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 18:54:46.870598: step 38390, loss = 1.04 (47.1 examples/sec; 0.680 sec/batch)
2018-10-16 18:54:54.011565: step 38400, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:55:03.761757: step 38410, loss = 1.02 (46.9 examples/sec; 0.682 sec/batch)
2018-10-16 18:55:10.913501: step 38420, loss = 1.02 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 18:55:18.018676: step 38430, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 18:55:25.147734: step 38440, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 18:55:32.333109: step 38450, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:55:39.486812: step 38460, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 18:55:46.662902: step 38470, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 18:55:53.775153: step 38480, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:56:00.964110: step 38490, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 18:56:08.122021: step 38500, loss = 0.99 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 18:56:17.876616: step 38510, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 18:56:24.982838: step 38520, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:56:32.083932: step 38530, loss = 1.00 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 18:56:39.239773: step 38540, loss = 1.05 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:56:46.327402: step 38550, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 18:56:53.514437: step 38560, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 18:57:00.638484: step 38570, loss = 1.04 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 18:57:07.801077: step 38580, loss = 0.99 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 18:57:14.938823: step 38590, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 18:57:22.086544: step 38600, loss = 1.01 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 18:57:31.952622: step 38610, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:57:39.058459: step 38620, loss = 1.11 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 18:57:46.134968: step 38630, loss = 0.99 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 18:57:53.284792: step 38640, loss = 1.02 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 18:58:00.497522: step 38650, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 18:58:07.622720: step 38660, loss = 1.14 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 18:58:14.793524: step 38670, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 18:58:22.040898: step 38680, loss = 1.30 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 18:58:29.122811: step 38690, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 18:58:36.236997: step 38700, loss = 0.99 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 18:58:45.986554: step 38710, loss = 1.10 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 18:58:53.093747: step 38720, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 18:59:00.196453: step 38730, loss = 1.18 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 18:59:07.418732: step 38740, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 18:59:14.681589: step 38750, loss = 1.05 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 18:59:21.800050: step 38760, loss = 0.99 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 18:59:29.015964: step 38770, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 18:59:36.189202: step 38780, loss = 1.12 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 18:59:43.355707: step 38790, loss = 1.09 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 18:59:50.526105: step 38800, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 19:00:00.270222: step 38810, loss = 1.05 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 19:00:07.367642: step 38820, loss = 1.10 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 19:00:14.504890: step 38830, loss = 1.08 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 19:00:21.685812: step 38840, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 19:00:28.833601: step 38850, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 19:00:35.978939: step 38860, loss = 1.03 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 19:00:43.132394: step 38870, loss = 1.02 (46.9 examples/sec; 0.683 sec/batch)
2018-10-16 19:00:50.286601: step 38880, loss = 1.21 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 19:00:57.422926: step 38890, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 19:01:04.564250: step 38900, loss = 1.02 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 19:01:14.351558: step 38910, loss = 1.12 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 19:01:21.456465: step 38920, loss = 1.12 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:01:28.593009: step 38930, loss = 1.13 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 19:01:35.730351: step 38940, loss = 1.08 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 19:01:42.772605: step 38950, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:01:49.979909: step 38960, loss = 1.10 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 19:01:57.125207: step 38970, loss = 1.03 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 19:02:04.336439: step 38980, loss = 1.05 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 19:02:11.488098: step 38990, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 19:02:18.642917: step 39000, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 19:02:28.822846: step 39010, loss = 1.13 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 19:02:35.895014: step 39020, loss = 1.13 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 19:02:43.103138: step 39030, loss = 1.07 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 19:02:50.228077: step 39040, loss = 1.12 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 19:02:57.387842: step 39050, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 19:03:04.610473: step 39060, loss = 1.00 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 19:03:11.749974: step 39070, loss = 1.01 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 19:03:18.930719: step 39080, loss = 1.19 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:03:25.983728: step 39090, loss = 1.17 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 19:03:33.186204: step 39100, loss = 1.00 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 19:03:42.934060: step 39110, loss = 1.06 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 19:03:49.984388: step 39120, loss = 1.04 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 19:03:57.148151: step 39130, loss = 1.24 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 19:04:04.319479: step 39140, loss = 1.01 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 19:04:11.501205: step 39150, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:04:18.735006: step 39160, loss = 1.07 (43.0 examples/sec; 0.743 sec/batch)
2018-10-16 19:04:25.872436: step 39170, loss = 1.04 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 19:04:33.045469: step 39180, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:04:40.269309: step 39190, loss = 1.17 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 19:04:47.386384: step 39200, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 19:04:56.991928: step 39210, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:05:04.153082: step 39220, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 19:05:11.291375: step 39230, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 19:05:18.442402: step 39240, loss = 1.08 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 19:05:25.603260: step 39250, loss = 1.07 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 19:05:32.712801: step 39260, loss = 1.00 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 19:05:39.868371: step 39270, loss = 1.10 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 19:05:47.096610: step 39280, loss = 1.02 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 19:05:54.247108: step 39290, loss = 1.05 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 19:06:01.353681: step 39300, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 19:06:11.165247: step 39310, loss = 1.17 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 19:06:18.243597: step 39320, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 19:06:25.420290: step 39330, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 19:06:32.454235: step 39340, loss = 1.03 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 19:06:39.713917: step 39350, loss = 1.04 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 19:06:46.835120: step 39360, loss = 1.00 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 19:06:53.998947: step 39370, loss = 1.11 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 19:07:01.109891: step 39380, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 19:07:08.303795: step 39390, loss = 0.99 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 19:07:15.468579: step 39400, loss = 1.07 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 19:07:25.154712: step 39410, loss = 0.99 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 19:07:32.241695: step 39420, loss = 1.13 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 19:07:39.415130: step 39430, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 19:07:46.527059: step 39440, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 19:07:53.685000: step 39450, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:08:00.865017: step 39460, loss = 1.17 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 19:08:08.050288: step 39470, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 19:08:15.243427: step 39480, loss = 1.13 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 19:08:22.445402: step 39490, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 19:08:29.548245: step 39500, loss = 1.01 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 19:08:39.265085: step 39510, loss = 1.11 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 19:08:46.386802: step 39520, loss = 1.07 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 19:08:53.535420: step 39530, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:09:00.723907: step 39540, loss = 1.09 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 19:09:07.828646: step 39550, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:09:14.948048: step 39560, loss = 1.06 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 19:09:22.145701: step 39570, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 19:09:29.334392: step 39580, loss = 1.06 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 19:09:36.512111: step 39590, loss = 1.06 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 19:09:43.630125: step 39600, loss = 1.08 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 19:09:53.410493: step 39610, loss = 1.15 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 19:10:00.516514: step 39620, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 19:10:07.662795: step 39630, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:10:14.762044: step 39640, loss = 1.02 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 19:10:21.978173: step 39650, loss = 1.11 (42.5 examples/sec; 0.753 sec/batch)
2018-10-16 19:10:29.169252: step 39660, loss = 1.09 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:10:36.332100: step 39670, loss = 1.08 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:10:43.490023: step 39680, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 19:10:50.651687: step 39690, loss = 1.01 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 19:10:57.833039: step 39700, loss = 1.03 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 19:11:07.775092: step 39710, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 19:11:14.913891: step 39720, loss = 1.04 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 19:11:22.035003: step 39730, loss = 1.13 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 19:11:29.172281: step 39740, loss = 1.08 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:11:36.356888: step 39750, loss = 1.01 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 19:11:43.554396: step 39760, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 19:11:50.695539: step 39770, loss = 1.03 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 19:11:57.833307: step 39780, loss = 1.09 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:12:04.951710: step 39790, loss = 0.99 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 19:12:12.162060: step 39800, loss = 1.01 (46.4 examples/sec; 0.689 sec/batch)
2018-10-16 19:12:21.840996: step 39810, loss = 1.04 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 19:12:29.086684: step 39820, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:12:36.212531: step 39830, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:12:43.314883: step 39840, loss = 1.03 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 19:12:50.521733: step 39850, loss = 1.08 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 19:12:57.778520: step 39860, loss = 1.22 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 19:13:04.971713: step 39870, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:13:12.089682: step 39880, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 19:13:19.234216: step 39890, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 19:13:26.370995: step 39900, loss = 1.14 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:13:36.649588: step 39910, loss = 1.12 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 19:13:43.875560: step 39920, loss = 1.06 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 19:13:51.035650: step 39930, loss = 1.13 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:13:58.215754: step 39940, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 19:14:05.362065: step 39950, loss = 0.99 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 19:14:12.572334: step 39960, loss = 1.03 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 19:14:19.733747: step 39970, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:14:26.866605: step 39980, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 19:14:34.046188: step 39990, loss = 1.11 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 19:14:41.169096: step 40000, loss = 1.10 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:14:54.470387: step 40010, loss = 1.00 (46.9 examples/sec; 0.682 sec/batch)
2018-10-16 19:15:01.249268: step 40020, loss = 1.06 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 19:15:08.418361: step 40030, loss = 1.04 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 19:15:15.550963: step 40040, loss = 1.10 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 19:15:22.699009: step 40050, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 19:15:29.906522: step 40060, loss = 1.06 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 19:15:37.194927: step 40070, loss = 1.02 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 19:15:44.334799: step 40080, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 19:15:51.572058: step 40090, loss = 1.12 (42.4 examples/sec; 0.755 sec/batch)
2018-10-16 19:15:58.591722: step 40100, loss = 1.11 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 19:16:08.558503: step 40110, loss = 1.00 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 19:16:15.700398: step 40120, loss = 1.13 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 19:16:22.805363: step 40130, loss = 1.17 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 19:16:29.969539: step 40140, loss = 1.10 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 19:16:37.166308: step 40150, loss = 1.11 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:16:44.396070: step 40160, loss = 1.02 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 19:16:51.559637: step 40170, loss = 1.08 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 19:16:58.793519: step 40180, loss = 1.06 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 19:17:05.929976: step 40190, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:17:13.065380: step 40200, loss = 1.19 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 19:17:22.790190: step 40210, loss = 1.01 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 19:17:29.990203: step 40220, loss = 1.02 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 19:17:37.235804: step 40230, loss = 1.00 (42.5 examples/sec; 0.753 sec/batch)
2018-10-16 19:17:44.323115: step 40240, loss = 1.05 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 19:17:51.432797: step 40250, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 19:17:58.671507: step 40260, loss = 1.03 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 19:18:05.809950: step 40270, loss = 1.07 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 19:18:13.002707: step 40280, loss = 1.05 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 19:18:20.150617: step 40290, loss = 1.24 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:18:27.268872: step 40300, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 19:18:36.934823: step 40310, loss = 1.11 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:18:44.042697: step 40320, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:18:51.191959: step 40330, loss = 1.05 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 19:18:58.366923: step 40340, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:19:05.383195: step 40350, loss = 1.11 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 19:19:12.601191: step 40360, loss = 0.99 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 19:19:19.846486: step 40370, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 19:19:26.990093: step 40380, loss = 1.11 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:19:34.147647: step 40390, loss = 1.08 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 19:19:41.178692: step 40400, loss = 1.19 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:19:51.204144: step 40410, loss = 1.04 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 19:19:58.387485: step 40420, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 19:20:05.521305: step 40430, loss = 1.09 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 19:20:12.638662: step 40440, loss = 1.13 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 19:20:19.738833: step 40450, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 19:20:26.896959: step 40460, loss = 1.07 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 19:20:34.149521: step 40470, loss = 0.99 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 19:20:41.363552: step 40480, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 19:20:48.541963: step 40490, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:20:55.699612: step 40500, loss = 1.10 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 19:21:05.350932: step 40510, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:21:12.477783: step 40520, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 19:21:19.551315: step 40530, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:21:26.772864: step 40540, loss = 1.12 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 19:21:33.945812: step 40550, loss = 1.15 (42.9 examples/sec; 0.747 sec/batch)
2018-10-16 19:21:41.067149: step 40560, loss = 1.03 (46.7 examples/sec; 0.686 sec/batch)
2018-10-16 19:21:48.208161: step 40570, loss = 1.10 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 19:21:55.359206: step 40580, loss = 1.05 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 19:22:02.501776: step 40590, loss = 1.07 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 19:22:09.597833: step 40600, loss = 1.01 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 19:22:19.400671: step 40610, loss = 1.07 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 19:22:26.609129: step 40620, loss = 1.13 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 19:22:33.792587: step 40630, loss = 1.12 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 19:22:40.977133: step 40640, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 19:22:48.113204: step 40650, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 19:22:55.263510: step 40660, loss = 1.11 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 19:23:02.391046: step 40670, loss = 1.14 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:23:09.556996: step 40680, loss = 1.00 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 19:23:16.601137: step 40690, loss = 1.12 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 19:23:23.694606: step 40700, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:23:33.478112: step 40710, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 19:23:40.678176: step 40720, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 19:23:47.782540: step 40730, loss = 1.02 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 19:23:54.854900: step 40740, loss = 1.01 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 19:24:02.029558: step 40750, loss = 1.02 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 19:24:09.168106: step 40760, loss = 1.02 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 19:24:16.347938: step 40770, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 19:24:23.515150: step 40780, loss = 1.01 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 19:24:30.731783: step 40790, loss = 1.03 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 19:24:37.934611: step 40800, loss = 1.06 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 19:24:47.590614: step 40810, loss = 1.09 (47.1 examples/sec; 0.680 sec/batch)
2018-10-16 19:24:54.684743: step 40820, loss = 1.12 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:25:01.767234: step 40830, loss = 1.08 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 19:25:08.950268: step 40840, loss = 1.06 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 19:25:16.149387: step 40850, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 19:25:23.320612: step 40860, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 19:25:30.413320: step 40870, loss = 1.11 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 19:25:37.574005: step 40880, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 19:25:44.728418: step 40890, loss = 1.01 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 19:25:51.793137: step 40900, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 19:26:01.527289: step 40910, loss = 1.07 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 19:26:08.660266: step 40920, loss = 1.02 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 19:26:15.858796: step 40930, loss = 1.04 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 19:26:22.956813: step 40940, loss = 1.14 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:26:30.120760: step 40950, loss = 0.99 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 19:26:37.298933: step 40960, loss = 1.00 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 19:26:44.395934: step 40970, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:26:51.591316: step 40980, loss = 0.99 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 19:26:58.702426: step 40990, loss = 1.06 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:27:05.799094: step 41000, loss = 1.11 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:27:15.659779: step 41010, loss = 1.12 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 19:27:22.808232: step 41020, loss = 1.00 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 19:27:29.964723: step 41030, loss = 1.00 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 19:27:37.183911: step 41040, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:27:44.293760: step 41050, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 19:27:51.492781: step 41060, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:27:58.603858: step 41070, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 19:28:05.803586: step 41080, loss = 1.04 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 19:28:12.963648: step 41090, loss = 1.03 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 19:28:20.153055: step 41100, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 19:28:29.852001: step 41110, loss = 1.03 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 19:28:36.909168: step 41120, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:28:44.109209: step 41130, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 19:28:51.216184: step 41140, loss = 1.13 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 19:28:58.383888: step 41150, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 19:29:05.639238: step 41160, loss = 1.05 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 19:29:12.840550: step 41170, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:29:20.036510: step 41180, loss = 1.02 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 19:29:27.174008: step 41190, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 19:29:34.405856: step 41200, loss = 1.08 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 19:29:44.324223: step 41210, loss = 1.10 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 19:29:51.383815: step 41220, loss = 1.04 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 19:29:58.547952: step 41230, loss = 1.01 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 19:30:05.688718: step 41240, loss = 1.02 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 19:30:12.870332: step 41250, loss = 1.11 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:30:20.029209: step 41260, loss = 1.00 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 19:30:27.116040: step 41270, loss = 1.00 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 19:30:34.228916: step 41280, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 19:30:41.392763: step 41290, loss = 1.06 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 19:30:48.586814: step 41300, loss = 1.26 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 19:30:58.515384: step 41310, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 19:31:05.623863: step 41320, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:31:12.720620: step 41330, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:31:19.814900: step 41340, loss = 1.05 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 19:31:27.009391: step 41350, loss = 0.99 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 19:31:34.084219: step 41360, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:31:41.317471: step 41370, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 19:31:48.401985: step 41380, loss = 1.04 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 19:31:55.582216: step 41390, loss = 1.02 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 19:32:02.745539: step 41400, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 19:32:12.770930: step 41410, loss = 1.09 (47.1 examples/sec; 0.679 sec/batch)
2018-10-16 19:32:19.866146: step 41420, loss = 1.11 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 19:32:27.062840: step 41430, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:32:34.229772: step 41440, loss = 1.03 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:32:41.373726: step 41450, loss = 1.06 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 19:32:48.471342: step 41460, loss = 1.11 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:32:55.646489: step 41470, loss = 1.08 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 19:33:02.840051: step 41480, loss = 1.06 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 19:33:10.005789: step 41490, loss = 1.04 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 19:33:17.156986: step 41500, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:33:26.912896: step 41510, loss = 0.99 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 19:33:34.001971: step 41520, loss = 1.03 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 19:33:41.218919: step 41530, loss = 0.99 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 19:33:48.301274: step 41540, loss = 1.13 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 19:33:55.449351: step 41550, loss = 1.05 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 19:34:02.601269: step 41560, loss = 1.03 (46.1 examples/sec; 0.693 sec/batch)
2018-10-16 19:34:09.730431: step 41570, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:34:16.870997: step 41580, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 19:34:24.050196: step 41590, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 19:34:31.227916: step 41600, loss = 0.99 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 19:34:40.901580: step 41610, loss = 1.10 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 19:34:48.019411: step 41620, loss = 1.01 (46.7 examples/sec; 0.686 sec/batch)
2018-10-16 19:34:55.161243: step 41630, loss = 1.00 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 19:35:02.396428: step 41640, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 19:35:09.513171: step 41650, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 19:35:16.661353: step 41660, loss = 1.02 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 19:35:23.777703: step 41670, loss = 1.03 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 19:35:30.869415: step 41680, loss = 1.17 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:35:38.096249: step 41690, loss = 1.09 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 19:35:45.315355: step 41700, loss = 1.01 (43.0 examples/sec; 0.743 sec/batch)
2018-10-16 19:35:55.234265: step 41710, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 19:36:02.339236: step 41720, loss = 1.02 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 19:36:09.474956: step 41730, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 19:36:16.622052: step 41740, loss = 1.19 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 19:36:23.834467: step 41750, loss = 1.03 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 19:36:30.983898: step 41760, loss = 1.00 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 19:36:38.068651: step 41770, loss = 1.21 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:36:45.171518: step 41780, loss = 1.11 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 19:36:52.283662: step 41790, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:36:59.379123: step 41800, loss = 1.01 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 19:37:09.358201: step 41810, loss = 1.15 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 19:37:16.495460: step 41820, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:37:23.695429: step 41830, loss = 1.09 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:37:30.894890: step 41840, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 19:37:38.009296: step 41850, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 19:37:45.178431: step 41860, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:37:52.314696: step 41870, loss = 1.12 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 19:37:59.384296: step 41880, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:38:06.567875: step 41890, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:38:13.614036: step 41900, loss = 1.08 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 19:38:23.374149: step 41910, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 19:38:30.443289: step 41920, loss = 1.04 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 19:38:37.519888: step 41930, loss = 0.99 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 19:38:44.589244: step 41940, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:38:51.690567: step 41950, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 19:38:58.917516: step 41960, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 19:39:06.106316: step 41970, loss = 1.16 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 19:39:13.284348: step 41980, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 19:39:20.419086: step 41990, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:39:27.522697: step 42000, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:39:37.262673: step 42010, loss = 1.03 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 19:39:44.424637: step 42020, loss = 1.13 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:39:51.653304: step 42030, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:39:58.776693: step 42040, loss = 1.08 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 19:40:05.948852: step 42050, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 19:40:13.088799: step 42060, loss = 1.03 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 19:40:20.246223: step 42070, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 19:40:27.357557: step 42080, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 19:40:34.558979: step 42090, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 19:40:41.698200: step 42100, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 19:40:51.367817: step 42110, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 19:40:58.491226: step 42120, loss = 1.06 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 19:41:05.680288: step 42130, loss = 1.01 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 19:41:12.792597: step 42140, loss = 1.02 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 19:41:20.012957: step 42150, loss = 1.02 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 19:41:27.170282: step 42160, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 19:41:34.253278: step 42170, loss = 1.10 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:41:41.437077: step 42180, loss = 1.16 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 19:41:48.615281: step 42190, loss = 1.11 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:41:55.717716: step 42200, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 19:42:05.407198: step 42210, loss = 1.12 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 19:42:12.500403: step 42220, loss = 0.99 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 19:42:19.606537: step 42230, loss = 1.03 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 19:42:26.723184: step 42240, loss = 0.99 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 19:42:33.847770: step 42250, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 19:42:41.066631: step 42260, loss = 0.99 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 19:42:48.177251: step 42270, loss = 1.10 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 19:42:55.318043: step 42280, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 19:43:02.451468: step 42290, loss = 1.07 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:43:09.673126: step 42300, loss = 1.14 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 19:43:19.478757: step 42310, loss = 0.99 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 19:43:26.590858: step 42320, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 19:43:33.714429: step 42330, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 19:43:40.938828: step 42340, loss = 1.12 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 19:43:48.065901: step 42350, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:43:55.203182: step 42360, loss = 1.02 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 19:44:02.312341: step 42370, loss = 1.07 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 19:44:09.504826: step 42380, loss = 1.02 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 19:44:16.659284: step 42390, loss = 1.02 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 19:44:23.919156: step 42400, loss = 1.03 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 19:44:34.045151: step 42410, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:44:41.122633: step 42420, loss = 1.09 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 19:44:48.316386: step 42430, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 19:44:55.457108: step 42440, loss = 1.11 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 19:45:02.587465: step 42450, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 19:45:09.769689: step 42460, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 19:45:16.932759: step 42470, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 19:45:24.015291: step 42480, loss = 1.03 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 19:45:31.194917: step 42490, loss = 1.09 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 19:45:38.348273: step 42500, loss = 1.25 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 19:45:48.068769: step 42510, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 19:45:55.228717: step 42520, loss = 1.09 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 19:46:02.284180: step 42530, loss = 1.06 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:46:09.415911: step 42540, loss = 0.99 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 19:46:16.613509: step 42550, loss = 1.13 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 19:46:23.739784: step 42560, loss = 1.05 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 19:46:30.968002: step 42570, loss = 0.99 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 19:46:38.166441: step 42580, loss = 0.99 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 19:46:45.287969: step 42590, loss = 0.99 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 19:46:52.446902: step 42600, loss = 1.11 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 19:47:02.502959: step 42610, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:47:09.652442: step 42620, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:47:16.849862: step 42630, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 19:47:23.966738: step 42640, loss = 1.07 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 19:47:31.085719: step 42650, loss = 1.15 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:47:38.234277: step 42660, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:47:45.371134: step 42670, loss = 0.99 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 19:47:52.486801: step 42680, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:47:59.684722: step 42690, loss = 1.12 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 19:48:06.849998: step 42700, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:48:16.492888: step 42710, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 19:48:23.561844: step 42720, loss = 1.11 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 19:48:30.679512: step 42730, loss = 1.10 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 19:48:37.772785: step 42740, loss = 1.14 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 19:48:44.955100: step 42750, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 19:48:52.178340: step 42760, loss = 1.22 (43.0 examples/sec; 0.743 sec/batch)
2018-10-16 19:48:59.317652: step 42770, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 19:49:06.564396: step 42780, loss = 1.14 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 19:49:13.671204: step 42790, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 19:49:20.765330: step 42800, loss = 1.38 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 19:49:30.404806: step 42810, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:49:37.539330: step 42820, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 19:49:44.714786: step 42830, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:49:51.890619: step 42840, loss = 1.00 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 19:49:59.065128: step 42850, loss = 1.12 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 19:50:06.199684: step 42860, loss = 1.08 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 19:50:13.280126: step 42870, loss = 1.10 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 19:50:20.438443: step 42880, loss = 1.02 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 19:50:27.629568: step 42890, loss = 1.13 (46.3 examples/sec; 0.690 sec/batch)
2018-10-16 19:50:34.783082: step 42900, loss = 1.04 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 19:50:44.458239: step 42910, loss = 1.05 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 19:50:51.524996: step 42920, loss = 1.17 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:50:58.643887: step 42930, loss = 1.18 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:51:05.837796: step 42940, loss = 1.02 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 19:51:12.951725: step 42950, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:51:20.174950: step 42960, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 19:51:27.295338: step 42970, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 19:51:34.491675: step 42980, loss = 1.07 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 19:51:41.656698: step 42990, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:51:48.721186: step 43000, loss = 1.12 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 19:51:58.488343: step 43010, loss = 1.12 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 19:52:05.601494: step 43020, loss = 1.07 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 19:52:12.705726: step 43030, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:52:19.893767: step 43040, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 19:52:27.056846: step 43050, loss = 1.01 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 19:52:34.284906: step 43060, loss = 1.03 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 19:52:41.450970: step 43070, loss = 1.16 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 19:52:48.587102: step 43080, loss = 1.00 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 19:52:55.680102: step 43090, loss = 1.06 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 19:53:02.821417: step 43100, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 19:53:12.422595: step 43110, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 19:53:19.490090: step 43120, loss = 1.10 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 19:53:26.587995: step 43130, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 19:53:33.664628: step 43140, loss = 1.16 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 19:53:40.870494: step 43150, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 19:53:48.053694: step 43160, loss = 1.30 (42.4 examples/sec; 0.754 sec/batch)
2018-10-16 19:53:55.143215: step 43170, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 19:54:02.339750: step 43180, loss = 1.08 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:54:09.440135: step 43190, loss = 1.10 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 19:54:16.551851: step 43200, loss = 1.02 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 19:54:26.241196: step 43210, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 19:54:33.405626: step 43220, loss = 1.00 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 19:54:40.530537: step 43230, loss = 1.09 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 19:54:47.704551: step 43240, loss = 1.05 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 19:54:54.864178: step 43250, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 19:55:02.008935: step 43260, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 19:55:09.202550: step 43270, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 19:55:16.334862: step 43280, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 19:55:23.498687: step 43290, loss = 1.05 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:55:30.613521: step 43300, loss = 1.21 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:55:40.550783: step 43310, loss = 1.07 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 19:55:47.691761: step 43320, loss = 0.99 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 19:55:54.807760: step 43330, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 19:56:01.950409: step 43340, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 19:56:09.125204: step 43350, loss = 1.06 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 19:56:16.263389: step 43360, loss = 1.10 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 19:56:23.363661: step 43370, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 19:56:30.535536: step 43380, loss = 1.15 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 19:56:37.655893: step 43390, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:56:44.734616: step 43400, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 19:56:54.689097: step 43410, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 19:57:01.802936: step 43420, loss = 1.24 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 19:57:09.056096: step 43430, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:57:16.244300: step 43440, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 19:57:23.365639: step 43450, loss = 1.04 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 19:57:30.525107: step 43460, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 19:57:37.681280: step 43470, loss = 1.01 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 19:57:44.805158: step 43480, loss = 1.04 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 19:57:51.960602: step 43490, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 19:57:59.077419: step 43500, loss = 1.04 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 19:58:08.775174: step 43510, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 19:58:15.910795: step 43520, loss = 1.09 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 19:58:23.032035: step 43530, loss = 1.04 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 19:58:30.175090: step 43540, loss = 1.03 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 19:58:37.341503: step 43550, loss = 1.11 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 19:58:44.500105: step 43560, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 19:58:51.658118: step 43570, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 19:58:58.816588: step 43580, loss = 1.01 (42.5 examples/sec; 0.754 sec/batch)
2018-10-16 19:59:05.934512: step 43590, loss = 1.10 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 19:59:13.133233: step 43600, loss = 1.03 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 19:59:22.783694: step 43610, loss = 1.22 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 19:59:29.905224: step 43620, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 19:59:36.961293: step 43630, loss = 1.01 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 19:59:44.143889: step 43640, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 19:59:51.146338: step 43650, loss = 1.03 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 19:59:58.281981: step 43660, loss = 1.06 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 20:00:05.397302: step 43670, loss = 1.06 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 20:00:12.583793: step 43680, loss = 1.05 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 20:00:19.696515: step 43690, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 20:00:26.858878: step 43700, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 20:00:36.613847: step 43710, loss = 1.09 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 20:00:43.681246: step 43720, loss = 1.04 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 20:00:50.799791: step 43730, loss = 1.04 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 20:00:57.921420: step 43740, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:01:05.069422: step 43750, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:01:12.247285: step 43760, loss = 1.09 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 20:01:19.353061: step 43770, loss = 1.06 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 20:01:26.480298: step 43780, loss = 0.99 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 20:01:33.631604: step 43790, loss = 1.11 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 20:01:40.775154: step 43800, loss = 1.00 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 20:01:50.519866: step 43810, loss = 1.03 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 20:01:57.706890: step 43820, loss = 1.02 (42.6 examples/sec; 0.750 sec/batch)
2018-10-16 20:02:04.779911: step 43830, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:02:11.975216: step 43840, loss = 1.06 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 20:02:19.047966: step 43850, loss = 1.11 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 20:02:26.177946: step 43860, loss = 1.00 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 20:02:33.288119: step 43870, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:02:40.432086: step 43880, loss = 1.00 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 20:02:47.567359: step 43890, loss = 1.05 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:02:54.706524: step 43900, loss = 1.54 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 20:03:04.357745: step 43910, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 20:03:11.462112: step 43920, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:03:18.672126: step 43930, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 20:03:25.865456: step 43940, loss = 1.08 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:03:32.967275: step 43950, loss = 1.17 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 20:03:40.063543: step 43960, loss = 0.99 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 20:03:47.228945: step 43970, loss = 1.22 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 20:03:54.330589: step 43980, loss = 1.02 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 20:04:01.506419: step 43990, loss = 1.06 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 20:04:08.672967: step 44000, loss = 1.02 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 20:04:18.373148: step 44010, loss = 1.26 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 20:04:25.561330: step 44020, loss = 1.04 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 20:04:32.697835: step 44030, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 20:04:39.859674: step 44040, loss = 0.99 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 20:04:46.913539: step 44050, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:04:54.053074: step 44060, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 20:05:01.248580: step 44070, loss = 1.12 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:05:08.332562: step 44080, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 20:05:15.471025: step 44090, loss = 1.08 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 20:05:22.538204: step 44100, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 20:05:32.714679: step 44110, loss = 1.00 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 20:05:39.918462: step 44120, loss = 1.10 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 20:05:47.051506: step 44130, loss = 1.11 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:05:54.193483: step 44140, loss = 1.02 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 20:06:01.351249: step 44150, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 20:06:08.479859: step 44160, loss = 1.06 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 20:06:15.565845: step 44170, loss = 1.01 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 20:06:22.728838: step 44180, loss = 1.09 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:06:29.848861: step 44190, loss = 1.20 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:06:36.938317: step 44200, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:06:46.698276: step 44210, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 20:06:53.811495: step 44220, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:07:00.908973: step 44230, loss = 1.04 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 20:07:08.116985: step 44240, loss = 1.07 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 20:07:15.247508: step 44250, loss = 0.99 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 20:07:22.367752: step 44260, loss = 1.12 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 20:07:29.497088: step 44270, loss = 1.06 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:07:36.645847: step 44280, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:07:43.826660: step 44290, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 20:07:51.011858: step 44300, loss = 1.06 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 20:08:00.715487: step 44310, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 20:08:07.744887: step 44320, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 20:08:14.837575: step 44330, loss = 1.21 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 20:08:21.988546: step 44340, loss = 1.06 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 20:08:29.065875: step 44350, loss = 1.02 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 20:08:36.222800: step 44360, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 20:08:43.309568: step 44370, loss = 1.11 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:08:50.444402: step 44380, loss = 1.13 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 20:08:57.521777: step 44390, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 20:09:04.680183: step 44400, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:09:14.372489: step 44410, loss = 1.04 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 20:09:21.513946: step 44420, loss = 1.11 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 20:09:28.626371: step 44430, loss = 1.11 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 20:09:35.759714: step 44440, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:09:42.863605: step 44450, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:09:50.004323: step 44460, loss = 1.02 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 20:09:57.131207: step 44470, loss = 1.08 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:10:04.222628: step 44480, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 20:10:11.454569: step 44490, loss = 1.13 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 20:10:18.637326: step 44500, loss = 1.01 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 20:10:28.403291: step 44510, loss = 1.12 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 20:10:35.440897: step 44520, loss = 1.15 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:10:42.659832: step 44530, loss = 1.14 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 20:10:49.840834: step 44540, loss = 1.09 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 20:10:57.011592: step 44550, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 20:11:04.166888: step 44560, loss = 1.02 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 20:11:11.253241: step 44570, loss = 1.09 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 20:11:18.386530: step 44580, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:11:25.497532: step 44590, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 20:11:32.621927: step 44600, loss = 1.06 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:11:42.280812: step 44610, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 20:11:49.381758: step 44620, loss = 1.01 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 20:11:56.494700: step 44630, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 20:12:03.719490: step 44640, loss = 1.02 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 20:12:10.858542: step 44650, loss = 1.21 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:12:18.012866: step 44660, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 20:12:25.195730: step 44670, loss = 1.02 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 20:12:32.343517: step 44680, loss = 1.03 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 20:12:39.457673: step 44690, loss = 0.99 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 20:12:46.596614: step 44700, loss = 1.02 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 20:12:56.704757: step 44710, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 20:13:03.788793: step 44720, loss = 1.05 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 20:13:10.933129: step 44730, loss = 1.04 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 20:13:18.151144: step 44740, loss = 1.10 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:13:25.269559: step 44750, loss = 1.16 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 20:13:32.462725: step 44760, loss = 1.07 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 20:13:39.653870: step 44770, loss = 1.03 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 20:13:46.717776: step 44780, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 20:13:53.834831: step 44790, loss = 1.04 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 20:14:00.992279: step 44800, loss = 1.02 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 20:14:10.550739: step 44810, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:14:17.613851: step 44820, loss = 1.10 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:14:24.720008: step 44830, loss = 1.11 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 20:14:31.874575: step 44840, loss = 1.15 (45.7 examples/sec; 0.699 sec/batch)
2018-10-16 20:14:38.963161: step 44850, loss = 1.08 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 20:14:46.098353: step 44860, loss = 1.01 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 20:14:53.205750: step 44870, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:15:00.401830: step 44880, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:15:07.476940: step 44890, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:15:14.704068: step 44900, loss = 1.06 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 20:15:24.575318: step 44910, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 20:15:31.637301: step 44920, loss = 1.15 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:15:38.791559: step 44930, loss = 1.04 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 20:15:45.915939: step 44940, loss = 1.00 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 20:15:53.055860: step 44950, loss = 1.09 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:16:00.182550: step 44960, loss = 1.07 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 20:16:07.276851: step 44970, loss = 1.25 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 20:16:14.415775: step 44980, loss = 1.08 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:16:21.542865: step 44990, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 20:16:28.688299: step 45000, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 20:16:41.813013: step 45010, loss = 1.17 (47.2 examples/sec; 0.678 sec/batch)
2018-10-16 20:16:48.611246: step 45020, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:16:55.807214: step 45030, loss = 1.10 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 20:17:02.965244: step 45040, loss = 1.00 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 20:17:10.220016: step 45050, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:17:17.389390: step 45060, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 20:17:24.459788: step 45070, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 20:17:31.685022: step 45080, loss = 1.08 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 20:17:38.787501: step 45090, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 20:17:45.924495: step 45100, loss = 1.05 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 20:17:55.635114: step 45110, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:18:02.749461: step 45120, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:18:09.826742: step 45130, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:18:16.958648: step 45140, loss = 1.01 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 20:18:24.040792: step 45150, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:18:31.107392: step 45160, loss = 1.12 (47.3 examples/sec; 0.677 sec/batch)
2018-10-16 20:18:38.288357: step 45170, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 20:18:45.479014: step 45180, loss = 1.05 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 20:18:52.675120: step 45190, loss = 1.00 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 20:18:59.892260: step 45200, loss = 1.02 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 20:19:09.590842: step 45210, loss = 1.05 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 20:19:16.676047: step 45220, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:19:23.878505: step 45230, loss = 1.02 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 20:19:31.030555: step 45240, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 20:19:38.211062: step 45250, loss = 0.99 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 20:19:45.363173: step 45260, loss = 1.08 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:19:52.554441: step 45270, loss = 1.14 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 20:19:59.686138: step 45280, loss = 1.00 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 20:20:06.804415: step 45290, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 20:20:13.906194: step 45300, loss = 1.20 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 20:20:23.976750: step 45310, loss = 1.12 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 20:20:31.223134: step 45320, loss = 1.04 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 20:20:38.345745: step 45330, loss = 1.03 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 20:20:45.486061: step 45340, loss = 1.07 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 20:20:52.590286: step 45350, loss = 1.24 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 20:20:59.757085: step 45360, loss = 1.15 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 20:21:06.908809: step 45370, loss = 1.01 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 20:21:14.079860: step 45380, loss = 1.07 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 20:21:21.244555: step 45390, loss = 1.10 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 20:21:28.401233: step 45400, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 20:21:38.400065: step 45410, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 20:21:45.478990: step 45420, loss = 1.00 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 20:21:52.662278: step 45430, loss = 1.12 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:21:59.849221: step 45440, loss = 1.04 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 20:22:06.902309: step 45450, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:22:14.074003: step 45460, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 20:22:21.201641: step 45470, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:22:28.299476: step 45480, loss = 1.11 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 20:22:35.461667: step 45490, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:22:42.560870: step 45500, loss = 1.04 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 20:22:52.675826: step 45510, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 20:22:59.771367: step 45520, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 20:23:06.914414: step 45530, loss = 1.05 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:23:14.058674: step 45540, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 20:23:21.297998: step 45550, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:23:28.377028: step 45560, loss = 1.12 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 20:23:35.522051: step 45570, loss = 1.08 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 20:23:42.598878: step 45580, loss = 1.09 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 20:23:49.734738: step 45590, loss = 1.04 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 20:23:56.968415: step 45600, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 20:24:06.642077: step 45610, loss = 1.08 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:24:13.772231: step 45620, loss = 1.01 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 20:24:20.864262: step 45630, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:24:28.057104: step 45640, loss = 1.11 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 20:24:35.215493: step 45650, loss = 1.02 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 20:24:42.364270: step 45660, loss = 1.05 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 20:24:49.567010: step 45670, loss = 1.05 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 20:24:56.706006: step 45680, loss = 1.06 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 20:25:03.879880: step 45690, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 20:25:10.971445: step 45700, loss = 0.99 (47.3 examples/sec; 0.677 sec/batch)
2018-10-16 20:25:21.369733: step 45710, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:25:28.466117: step 45720, loss = 1.05 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 20:25:35.587524: step 45730, loss = 1.19 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:25:42.754000: step 45740, loss = 1.10 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 20:25:49.905261: step 45750, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 20:25:57.124136: step 45760, loss = 1.02 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 20:26:04.257188: step 45770, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 20:26:11.451888: step 45780, loss = 1.01 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 20:26:18.606766: step 45790, loss = 1.05 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 20:26:25.780014: step 45800, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:26:35.594334: step 45810, loss = 1.17 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:26:42.707486: step 45820, loss = 1.08 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 20:26:49.837019: step 45830, loss = 1.15 (45.6 examples/sec; 0.703 sec/batch)
2018-10-16 20:26:57.001104: step 45840, loss = 1.15 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 20:27:04.130701: step 45850, loss = 1.24 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 20:27:11.310091: step 45860, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 20:27:18.538564: step 45870, loss = 1.11 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 20:27:25.730268: step 45880, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:27:32.886538: step 45890, loss = 1.12 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 20:27:40.001824: step 45900, loss = 1.12 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 20:27:49.743819: step 45910, loss = 1.19 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 20:27:56.793170: step 45920, loss = 1.09 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 20:28:03.898564: step 45930, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 20:28:11.107576: step 45940, loss = 1.19 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 20:28:18.269171: step 45950, loss = 1.04 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 20:28:25.422415: step 45960, loss = 1.04 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 20:28:32.549341: step 45970, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 20:28:39.649715: step 45980, loss = 1.29 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 20:28:46.787333: step 45990, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 20:28:53.956322: step 46000, loss = 1.06 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 20:29:03.779324: step 46010, loss = 1.11 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 20:29:10.836076: step 46020, loss = 1.10 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 20:29:17.972044: step 46030, loss = 0.99 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 20:29:25.103242: step 46040, loss = 1.03 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 20:29:32.240273: step 46050, loss = 1.05 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 20:29:39.400082: step 46060, loss = 1.05 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 20:29:46.612298: step 46070, loss = 1.18 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 20:29:53.780522: step 46080, loss = 1.10 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:30:00.869685: step 46090, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 20:30:07.923169: step 46100, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 20:30:18.300036: step 46110, loss = 1.05 (46.4 examples/sec; 0.689 sec/batch)
2018-10-16 20:30:25.376800: step 46120, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:30:32.478762: step 46130, loss = 1.08 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 20:30:39.682546: step 46140, loss = 0.99 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 20:30:46.724998: step 46150, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 20:30:53.831233: step 46160, loss = 1.08 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 20:31:00.967694: step 46170, loss = 1.06 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:31:08.050710: step 46180, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 20:31:15.200204: step 46190, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 20:31:22.304453: step 46200, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:31:31.949780: step 46210, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 20:31:39.121378: step 46220, loss = 1.03 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 20:31:46.303412: step 46230, loss = 1.06 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 20:31:53.379972: step 46240, loss = 1.09 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 20:32:00.540475: step 46250, loss = 1.08 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 20:32:07.713346: step 46260, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 20:32:14.891874: step 46270, loss = 1.03 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 20:32:22.001737: step 46280, loss = 1.00 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 20:32:29.128498: step 46290, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:32:36.297410: step 46300, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 20:32:45.980594: step 46310, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:32:53.015898: step 46320, loss = 1.08 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 20:33:00.178659: step 46330, loss = 1.15 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 20:33:07.311059: step 46340, loss = 1.11 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:33:14.488628: step 46350, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 20:33:21.641730: step 46360, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 20:33:28.777567: step 46370, loss = 1.03 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 20:33:36.008604: step 46380, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 20:33:43.161315: step 46390, loss = 1.02 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 20:33:50.273989: step 46400, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 20:33:59.915023: step 46410, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 20:34:06.954977: step 46420, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 20:34:14.066183: step 46430, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 20:34:21.249435: step 46440, loss = 1.14 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 20:34:28.397080: step 46450, loss = 1.19 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 20:34:35.576376: step 46460, loss = 1.00 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 20:34:42.764353: step 46470, loss = 1.07 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 20:34:49.910912: step 46480, loss = 1.02 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 20:34:56.991670: step 46490, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:35:04.139240: step 46500, loss = 1.25 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 20:35:13.871352: step 46510, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 20:35:21.018716: step 46520, loss = 1.02 (42.8 examples/sec; 0.747 sec/batch)
2018-10-16 20:35:28.173505: step 46530, loss = 1.04 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 20:35:35.224078: step 46540, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 20:35:42.339661: step 46550, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:35:49.552368: step 46560, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 20:35:56.686061: step 46570, loss = 0.99 (42.2 examples/sec; 0.758 sec/batch)
2018-10-16 20:36:03.745544: step 46580, loss = 1.20 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 20:36:10.840242: step 46590, loss = 1.19 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 20:36:17.959401: step 46600, loss = 1.03 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 20:36:27.668453: step 46610, loss = 1.04 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 20:36:34.774556: step 46620, loss = 1.16 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 20:36:41.917212: step 46630, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 20:36:49.048153: step 46640, loss = 1.13 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 20:36:56.196638: step 46650, loss = 1.05 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 20:37:03.262278: step 46660, loss = 1.04 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 20:37:10.484292: step 46670, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:37:17.666000: step 46680, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 20:37:24.735095: step 46690, loss = 1.20 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:37:31.886885: step 46700, loss = 1.11 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:37:41.558375: step 46710, loss = 1.05 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 20:37:48.692623: step 46720, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 20:37:55.908410: step 46730, loss = 1.05 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 20:38:03.138902: step 46740, loss = 1.07 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 20:38:10.301928: step 46750, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 20:38:17.475446: step 46760, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 20:38:24.673573: step 46770, loss = 1.07 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 20:38:31.847079: step 46780, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 20:38:38.971845: step 46790, loss = 1.02 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 20:38:46.084968: step 46800, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:38:55.757554: step 46810, loss = 0.99 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 20:39:02.954074: step 46820, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:39:10.037013: step 46830, loss = 0.99 (43.0 examples/sec; 0.743 sec/batch)
2018-10-16 20:39:17.244007: step 46840, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:39:24.425874: step 46850, loss = 1.02 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 20:39:31.597455: step 46860, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:39:38.667855: step 46870, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 20:39:45.778847: step 46880, loss = 1.03 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 20:39:52.915663: step 46890, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:40:00.102511: step 46900, loss = 1.21 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 20:40:09.711605: step 46910, loss = 1.06 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 20:40:16.846090: step 46920, loss = 1.05 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 20:40:24.060504: step 46930, loss = 1.03 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 20:40:31.154992: step 46940, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:40:38.250249: step 46950, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:40:45.375245: step 46960, loss = 1.03 (46.6 examples/sec; 0.687 sec/batch)
2018-10-16 20:40:52.476209: step 46970, loss = 1.06 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:40:59.646112: step 46980, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 20:41:06.790216: step 46990, loss = 1.05 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 20:41:13.916495: step 47000, loss = 1.06 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 20:41:23.708781: step 47010, loss = 1.14 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 20:41:30.866510: step 47020, loss = 1.22 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:41:38.004013: step 47030, loss = 1.01 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 20:41:45.181893: step 47040, loss = 1.00 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 20:41:52.295407: step 47050, loss = 1.15 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 20:41:59.411568: step 47060, loss = 1.27 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 20:42:06.513178: step 47070, loss = 1.03 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 20:42:13.658043: step 47080, loss = 1.03 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 20:42:20.783186: step 47090, loss = 1.21 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 20:42:27.901679: step 47100, loss = 1.12 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 20:42:37.910176: step 47110, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 20:42:45.015957: step 47120, loss = 1.24 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 20:42:52.212408: step 47130, loss = 1.07 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 20:42:59.345119: step 47140, loss = 1.02 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 20:43:06.517589: step 47150, loss = 0.99 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 20:43:13.685681: step 47160, loss = 1.01 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 20:43:20.827546: step 47170, loss = 1.08 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 20:43:27.922247: step 47180, loss = 1.00 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 20:43:35.075867: step 47190, loss = 1.07 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 20:43:42.219705: step 47200, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 20:43:51.935524: step 47210, loss = 1.20 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 20:43:59.065306: step 47220, loss = 1.01 (45.9 examples/sec; 0.696 sec/batch)
2018-10-16 20:44:06.169282: step 47230, loss = 1.08 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:44:13.246155: step 47240, loss = 1.00 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 20:44:20.439051: step 47250, loss = 1.10 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 20:44:27.590372: step 47260, loss = 1.20 (45.9 examples/sec; 0.696 sec/batch)
2018-10-16 20:44:34.678536: step 47270, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:44:41.823508: step 47280, loss = 1.01 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 20:44:48.935235: step 47290, loss = 1.00 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 20:44:56.010653: step 47300, loss = 1.10 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 20:45:05.824887: step 47310, loss = 1.10 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 20:45:13.025206: step 47320, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 20:45:20.117376: step 47330, loss = 1.09 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 20:45:27.277977: step 47340, loss = 0.99 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 20:45:34.411254: step 47350, loss = 1.07 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 20:45:41.505634: step 47360, loss = 1.07 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 20:45:48.640504: step 47370, loss = 1.12 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 20:45:55.756307: step 47380, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 20:46:02.884561: step 47390, loss = 1.00 (45.6 examples/sec; 0.703 sec/batch)
2018-10-16 20:46:10.064907: step 47400, loss = 1.09 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 20:46:19.907219: step 47410, loss = 1.23 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 20:46:27.068994: step 47420, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 20:46:34.213745: step 47430, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:46:41.312323: step 47440, loss = 1.04 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 20:46:48.507144: step 47450, loss = 1.05 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 20:46:55.669513: step 47460, loss = 1.02 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 20:47:02.761948: step 47470, loss = 1.08 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 20:47:09.921431: step 47480, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:47:16.994704: step 47490, loss = 1.16 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 20:47:24.093169: step 47500, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 20:47:33.772178: step 47510, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:47:40.866746: step 47520, loss = 1.10 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 20:47:47.985314: step 47530, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:47:55.083734: step 47540, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 20:48:02.159759: step 47550, loss = 0.99 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 20:48:09.379377: step 47560, loss = 1.14 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 20:48:16.488604: step 47570, loss = 1.18 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 20:48:23.545948: step 47580, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:48:30.661641: step 47590, loss = 1.08 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 20:48:37.810595: step 47600, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:48:47.539137: step 47610, loss = 1.01 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 20:48:54.665969: step 47620, loss = 1.26 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 20:49:01.732110: step 47630, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:49:08.849266: step 47640, loss = 1.03 (43.2 examples/sec; 0.742 sec/batch)
2018-10-16 20:49:15.988076: step 47650, loss = 1.04 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 20:49:23.154911: step 47660, loss = 1.02 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 20:49:30.333372: step 47670, loss = 1.00 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 20:49:37.428613: step 47680, loss = 1.04 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 20:49:44.577337: step 47690, loss = 1.07 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 20:49:51.699486: step 47700, loss = 1.01 (47.1 examples/sec; 0.679 sec/batch)
2018-10-16 20:50:01.441174: step 47710, loss = 0.99 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 20:50:08.529193: step 47720, loss = 1.16 (48.1 examples/sec; 0.665 sec/batch)
2018-10-16 20:50:15.660668: step 47730, loss = 1.07 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 20:50:22.797118: step 47740, loss = 1.00 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 20:50:29.910388: step 47750, loss = 1.06 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 20:50:37.099073: step 47760, loss = 1.07 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 20:50:44.184067: step 47770, loss = 1.00 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 20:50:51.361103: step 47780, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 20:50:58.586360: step 47790, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 20:51:05.753940: step 47800, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:51:15.464927: step 47810, loss = 1.15 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 20:51:22.552811: step 47820, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 20:51:29.794695: step 47830, loss = 1.08 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 20:51:36.923159: step 47840, loss = 0.99 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 20:51:44.006717: step 47850, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 20:51:51.130772: step 47860, loss = 1.05 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 20:51:58.221963: step 47870, loss = 1.12 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:52:05.447702: step 47880, loss = 1.05 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 20:52:12.641454: step 47890, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 20:52:19.818312: step 47900, loss = 1.01 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 20:52:29.440124: step 47910, loss = 0.99 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 20:52:36.595157: step 47920, loss = 1.08 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 20:52:43.669099: step 47930, loss = 1.02 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 20:52:50.787148: step 47940, loss = 1.07 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 20:52:57.840140: step 47950, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 20:53:04.909462: step 47960, loss = 1.14 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:53:12.078336: step 47970, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 20:53:19.199536: step 47980, loss = 1.04 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 20:53:26.376919: step 47990, loss = 1.11 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 20:53:33.519159: step 48000, loss = 1.06 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 20:53:43.215501: step 48010, loss = 1.13 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:53:50.308807: step 48020, loss = 0.99 (42.1 examples/sec; 0.760 sec/batch)
2018-10-16 20:53:57.457628: step 48030, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 20:54:04.665607: step 48040, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 20:54:11.867228: step 48050, loss = 1.08 (43.4 examples/sec; 0.736 sec/batch)
2018-10-16 20:54:18.976344: step 48060, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 20:54:26.179139: step 48070, loss = 1.03 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 20:54:33.318941: step 48080, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 20:54:40.470545: step 48090, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 20:54:47.582155: step 48100, loss = 1.09 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 20:54:57.301120: step 48110, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 20:55:04.340521: step 48120, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:55:11.486769: step 48130, loss = 1.11 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 20:55:18.564845: step 48140, loss = 1.09 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 20:55:25.697413: step 48150, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:55:32.772057: step 48160, loss = 1.06 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 20:55:39.822658: step 48170, loss = 1.09 (46.1 examples/sec; 0.693 sec/batch)
2018-10-16 20:55:46.967556: step 48180, loss = 1.22 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 20:55:54.038574: step 48190, loss = 1.10 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 20:56:01.115284: step 48200, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:56:11.269168: step 48210, loss = 1.02 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 20:56:18.442764: step 48220, loss = 1.02 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 20:56:25.547389: step 48230, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 20:56:32.673430: step 48240, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 20:56:39.759898: step 48250, loss = 1.05 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 20:56:46.846881: step 48260, loss = 1.02 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 20:56:53.963758: step 48270, loss = 1.02 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 20:57:01.126200: step 48280, loss = 0.99 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 20:57:08.282201: step 48290, loss = 1.02 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 20:57:15.351880: step 48300, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 20:57:25.161917: step 48310, loss = 1.06 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 20:57:32.229893: step 48320, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 20:57:39.440940: step 48330, loss = 1.01 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 20:57:46.480216: step 48340, loss = 1.00 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 20:57:53.722613: step 48350, loss = 1.02 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 20:58:00.931976: step 48360, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 20:58:08.003429: step 48370, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 20:58:15.095073: step 48380, loss = 1.08 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 20:58:22.230415: step 48390, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 20:58:29.348761: step 48400, loss = 1.12 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 20:58:38.941553: step 48410, loss = 1.12 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 20:58:46.015441: step 48420, loss = 1.05 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 20:58:53.150923: step 48430, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 20:59:00.314959: step 48440, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 20:59:07.482781: step 48450, loss = 1.05 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 20:59:14.666039: step 48460, loss = 1.02 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 20:59:21.834774: step 48470, loss = 1.05 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 20:59:28.962435: step 48480, loss = 1.11 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 20:59:36.147040: step 48490, loss = 1.03 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 20:59:43.288091: step 48500, loss = 1.08 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 20:59:52.982841: step 48510, loss = 1.00 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 21:00:00.003145: step 48520, loss = 1.03 (46.7 examples/sec; 0.686 sec/batch)
2018-10-16 21:00:07.073822: step 48530, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:00:14.241539: step 48540, loss = 1.18 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 21:00:21.437082: step 48550, loss = 1.03 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 21:00:28.535964: step 48560, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:00:35.699317: step 48570, loss = 1.26 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:00:42.780092: step 48580, loss = 1.14 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 21:00:49.924448: step 48590, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:00:57.097618: step 48600, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 21:01:06.700645: step 48610, loss = 1.11 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 21:01:13.783148: step 48620, loss = 1.06 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 21:01:20.899569: step 48630, loss = 1.16 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 21:01:27.945825: step 48640, loss = 1.00 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 21:01:35.083492: step 48650, loss = 1.08 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 21:01:42.241060: step 48660, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:01:49.457172: step 48670, loss = 1.00 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 21:01:56.575384: step 48680, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:02:03.683515: step 48690, loss = 1.18 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:02:10.754071: step 48700, loss = 1.21 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 21:02:20.475467: step 48710, loss = 1.04 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 21:02:27.594760: step 48720, loss = 1.03 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 21:02:34.665736: step 48730, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 21:02:41.774490: step 48740, loss = 1.01 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 21:02:48.869059: step 48750, loss = 1.06 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 21:02:56.014900: step 48760, loss = 1.01 (47.1 examples/sec; 0.679 sec/batch)
2018-10-16 21:03:03.136042: step 48770, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 21:03:10.293038: step 48780, loss = 1.03 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 21:03:17.405498: step 48790, loss = 1.06 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 21:03:24.512086: step 48800, loss = 0.99 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 21:03:34.154210: step 48810, loss = 1.06 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 21:03:41.140528: step 48820, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 21:03:48.261211: step 48830, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:03:55.418820: step 48840, loss = 1.01 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 21:04:02.517132: step 48850, loss = 0.99 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 21:04:09.609030: step 48860, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:04:16.743031: step 48870, loss = 1.09 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 21:04:23.887831: step 48880, loss = 1.16 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 21:04:31.066604: step 48890, loss = 1.02 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 21:04:38.203158: step 48900, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 21:04:48.888531: step 48910, loss = 1.00 (47.7 examples/sec; 0.672 sec/batch)
2018-10-16 21:04:55.923093: step 48920, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:05:03.091836: step 48930, loss = 1.16 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 21:05:10.162992: step 48940, loss = 1.16 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 21:05:17.331045: step 48950, loss = 1.05 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 21:05:24.501638: step 48960, loss = 0.99 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 21:05:31.686927: step 48970, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 21:05:38.831192: step 48980, loss = 1.00 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 21:05:45.939715: step 48990, loss = 1.15 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 21:05:53.098584: step 49000, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:06:02.700926: step 49010, loss = 1.00 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 21:06:09.775467: step 49020, loss = 1.20 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 21:06:16.829466: step 49030, loss = 0.99 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 21:06:24.051994: step 49040, loss = 1.05 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 21:06:31.148129: step 49050, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 21:06:38.289699: step 49060, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:06:45.410595: step 49070, loss = 1.00 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 21:06:52.498593: step 49080, loss = 1.06 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 21:06:59.683872: step 49090, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 21:07:06.967335: step 49100, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 21:07:17.607984: step 49110, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 21:07:24.716534: step 49120, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 21:07:31.841068: step 49130, loss = 1.03 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 21:07:38.994731: step 49140, loss = 1.06 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 21:07:46.093082: step 49150, loss = 1.29 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 21:07:53.273044: step 49160, loss = 1.09 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 21:08:00.398720: step 49170, loss = 1.08 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 21:08:07.524079: step 49180, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:08:14.643655: step 49190, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:08:21.741664: step 49200, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 21:08:31.803846: step 49210, loss = 1.00 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 21:08:38.950208: step 49220, loss = 1.06 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 21:08:46.077126: step 49230, loss = 1.20 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:08:53.191105: step 49240, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 21:09:00.347172: step 49250, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 21:09:07.477087: step 49260, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:09:14.645639: step 49270, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:09:21.750651: step 49280, loss = 1.08 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 21:09:28.886511: step 49290, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 21:09:36.026756: step 49300, loss = 1.08 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:09:46.247675: step 49310, loss = 1.04 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 21:09:53.316932: step 49320, loss = 1.13 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 21:10:00.427723: step 49330, loss = 1.10 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 21:10:07.548011: step 49340, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:10:14.637875: step 49350, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:10:21.810312: step 49360, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 21:10:28.970222: step 49370, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:10:36.031553: step 49380, loss = 1.14 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 21:10:43.225738: step 49390, loss = 1.13 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 21:10:50.304595: step 49400, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:11:00.456279: step 49410, loss = 1.05 (48.0 examples/sec; 0.667 sec/batch)
2018-10-16 21:11:07.636364: step 49420, loss = 1.04 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 21:11:14.754097: step 49430, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:11:21.902536: step 49440, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 21:11:29.055382: step 49450, loss = 1.05 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 21:11:36.180163: step 49460, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 21:11:43.276732: step 49470, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 21:11:50.503812: step 49480, loss = 1.11 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 21:11:57.665855: step 49490, loss = 1.10 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 21:12:04.748594: step 49500, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:12:14.520728: step 49510, loss = 1.02 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 21:12:21.634247: step 49520, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 21:12:28.680862: step 49530, loss = 1.10 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 21:12:35.797890: step 49540, loss = 1.04 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 21:12:42.943286: step 49550, loss = 1.04 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 21:12:50.030927: step 49560, loss = 1.10 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:12:57.171661: step 49570, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:13:04.364744: step 49580, loss = 1.12 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 21:13:11.527637: step 49590, loss = 1.07 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 21:13:18.748558: step 49600, loss = 1.12 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 21:13:28.612547: step 49610, loss = 0.99 (46.7 examples/sec; 0.686 sec/batch)
2018-10-16 21:13:35.830503: step 49620, loss = 1.09 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:13:42.946312: step 49630, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 21:13:50.160089: step 49640, loss = 1.03 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 21:13:57.280875: step 49650, loss = 1.00 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 21:14:04.417692: step 49660, loss = 1.19 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:14:11.506343: step 49670, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 21:14:18.685542: step 49680, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 21:14:25.684344: step 49690, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 21:14:32.812352: step 49700, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 21:14:42.457873: step 49710, loss = 1.28 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 21:14:49.519663: step 49720, loss = 1.04 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 21:14:56.668453: step 49730, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:15:03.772877: step 49740, loss = 1.00 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 21:15:10.852124: step 49750, loss = 1.15 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 21:15:18.008194: step 49760, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:15:25.117458: step 49770, loss = 1.30 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 21:15:32.180813: step 49780, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 21:15:39.304020: step 49790, loss = 1.04 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 21:15:46.492267: step 49800, loss = 1.02 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 21:15:56.206835: step 49810, loss = 1.15 (47.8 examples/sec; 0.669 sec/batch)
2018-10-16 21:16:03.284676: step 49820, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:16:10.390024: step 49830, loss = 1.04 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 21:16:17.491165: step 49840, loss = 1.01 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 21:16:24.617787: step 49850, loss = 1.02 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 21:16:31.705220: step 49860, loss = 1.08 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 21:16:38.899747: step 49870, loss = 1.10 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:16:46.037087: step 49880, loss = 1.16 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:16:53.186578: step 49890, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:17:00.368481: step 49900, loss = 1.09 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 21:17:09.993894: step 49910, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 21:17:17.110564: step 49920, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 21:17:24.258346: step 49930, loss = 1.04 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 21:17:31.377883: step 49940, loss = 1.07 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 21:17:38.599245: step 49950, loss = 1.04 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 21:17:45.748453: step 49960, loss = 1.02 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 21:17:52.948561: step 49970, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 21:18:00.065169: step 49980, loss = 1.11 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 21:18:07.219569: step 49990, loss = 1.06 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 21:18:14.389045: step 50000, loss = 1.20 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 21:18:26.974497: step 50010, loss = 1.02 (47.3 examples/sec; 0.677 sec/batch)
2018-10-16 21:18:33.680860: step 50020, loss = 1.00 (47.5 examples/sec; 0.674 sec/batch)
2018-10-16 21:18:40.863063: step 50030, loss = 1.02 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 21:18:48.007232: step 50040, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 21:18:55.192512: step 50050, loss = 1.25 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 21:19:02.433470: step 50060, loss = 1.08 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 21:19:09.499866: step 50070, loss = 1.13 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:19:16.610210: step 50080, loss = 1.15 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 21:19:23.687256: step 50090, loss = 1.06 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 21:19:30.808107: step 50100, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 21:19:40.419904: step 50110, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 21:19:47.566790: step 50120, loss = 1.05 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 21:19:54.785368: step 50130, loss = 1.23 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:20:01.900856: step 50140, loss = 1.19 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 21:20:09.103949: step 50150, loss = 1.04 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 21:20:16.269121: step 50160, loss = 1.27 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 21:20:23.384973: step 50170, loss = 1.05 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 21:20:30.515680: step 50180, loss = 1.04 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 21:20:37.704658: step 50190, loss = 1.23 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 21:20:44.980516: step 50200, loss = 1.00 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 21:20:54.901039: step 50210, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 21:21:02.061618: step 50220, loss = 1.11 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 21:21:09.285480: step 50230, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 21:21:16.406540: step 50240, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 21:21:23.536618: step 50250, loss = 1.09 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 21:21:30.735567: step 50260, loss = 1.09 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 21:21:37.904216: step 50270, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:21:45.084039: step 50280, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 21:21:52.277417: step 50290, loss = 1.00 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 21:21:59.342443: step 50300, loss = 1.06 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 21:22:09.155885: step 50310, loss = 1.10 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 21:22:16.254940: step 50320, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 21:22:23.393357: step 50330, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 21:22:30.494810: step 50340, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 21:22:37.600532: step 50350, loss = 1.02 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 21:22:44.768137: step 50360, loss = 1.01 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 21:22:51.952168: step 50370, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 21:22:59.083001: step 50380, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 21:23:06.125910: step 50390, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:23:13.243621: step 50400, loss = 1.05 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:23:22.854437: step 50410, loss = 1.07 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 21:23:29.982866: step 50420, loss = 1.17 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:23:37.098617: step 50430, loss = 1.07 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 21:23:44.196673: step 50440, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:23:51.317673: step 50450, loss = 0.99 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 21:23:58.552582: step 50460, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 21:24:05.618264: step 50470, loss = 1.13 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:24:12.812310: step 50480, loss = 1.09 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 21:24:20.015284: step 50490, loss = 1.05 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 21:24:27.131646: step 50500, loss = 1.16 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 21:24:36.815361: step 50510, loss = 0.99 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 21:24:43.916439: step 50520, loss = 1.02 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 21:24:51.019178: step 50530, loss = 1.20 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 21:24:58.142245: step 50540, loss = 1.16 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 21:25:05.226050: step 50550, loss = 1.02 (47.4 examples/sec; 0.675 sec/batch)
2018-10-16 21:25:12.402062: step 50560, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:25:19.520931: step 50570, loss = 1.04 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 21:25:26.660924: step 50580, loss = 1.13 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 21:25:33.795812: step 50590, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:25:40.918952: step 50600, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 21:25:50.814078: step 50610, loss = 1.03 (46.6 examples/sec; 0.687 sec/batch)
2018-10-16 21:25:57.925836: step 50620, loss = 0.99 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 21:26:05.019517: step 50630, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:26:12.204110: step 50640, loss = 1.07 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 21:26:19.419267: step 50650, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 21:26:26.496594: step 50660, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:26:33.658317: step 50670, loss = 1.14 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 21:26:40.820422: step 50680, loss = 1.12 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:26:47.938745: step 50690, loss = 1.10 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 21:26:55.071652: step 50700, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 21:27:04.815999: step 50710, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 21:27:11.940608: step 50720, loss = 1.25 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 21:27:19.018600: step 50730, loss = 1.01 (47.1 examples/sec; 0.679 sec/batch)
2018-10-16 21:27:26.172294: step 50740, loss = 1.18 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 21:27:33.340904: step 50750, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 21:27:40.460848: step 50760, loss = 1.12 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 21:27:47.617097: step 50770, loss = 1.21 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 21:27:54.735884: step 50780, loss = 1.11 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:28:01.866623: step 50790, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:28:09.036869: step 50800, loss = 1.12 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 21:28:18.686188: step 50810, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 21:28:25.749978: step 50820, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:28:32.819339: step 50830, loss = 1.16 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 21:28:39.983571: step 50840, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 21:28:47.170437: step 50850, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 21:28:54.330610: step 50860, loss = 1.02 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 21:29:01.454173: step 50870, loss = 1.00 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 21:29:08.538966: step 50880, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 21:29:15.755260: step 50890, loss = 1.03 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 21:29:22.849603: step 50900, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 21:29:32.612838: step 50910, loss = 0.99 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 21:29:39.650207: step 50920, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:29:46.731671: step 50930, loss = 1.14 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 21:29:53.819975: step 50940, loss = 1.01 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 21:30:01.012449: step 50950, loss = 1.11 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 21:30:08.186703: step 50960, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 21:30:15.353746: step 50970, loss = 1.02 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 21:30:22.552269: step 50980, loss = 1.01 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 21:30:29.766421: step 50990, loss = 1.08 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 21:30:36.819911: step 51000, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 21:30:46.753536: step 51010, loss = 1.03 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 21:30:53.809699: step 51020, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 21:31:00.888667: step 51030, loss = 1.07 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:31:08.064114: step 51040, loss = 1.03 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 21:31:15.190075: step 51050, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:31:22.242401: step 51060, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 21:31:29.368457: step 51070, loss = 0.99 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 21:31:36.544278: step 51080, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 21:31:43.618190: step 51090, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:31:50.782501: step 51100, loss = 0.99 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 21:32:00.362397: step 51110, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 21:32:07.497382: step 51120, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 21:32:14.617780: step 51130, loss = 1.22 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 21:32:21.787302: step 51140, loss = 1.02 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 21:32:28.845148: step 51150, loss = 1.22 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 21:32:35.985083: step 51160, loss = 1.14 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 21:32:43.148053: step 51170, loss = 1.05 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 21:32:50.359635: step 51180, loss = 1.00 (45.9 examples/sec; 0.696 sec/batch)
2018-10-16 21:32:57.473743: step 51190, loss = 1.09 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 21:33:04.669764: step 51200, loss = 1.10 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 21:33:14.378080: step 51210, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 21:33:21.443429: step 51220, loss = 1.11 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 21:33:28.582321: step 51230, loss = 0.99 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 21:33:35.666915: step 51240, loss = 1.10 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:33:42.794758: step 51250, loss = 1.01 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 21:33:49.915030: step 51260, loss = 1.03 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 21:33:57.024414: step 51270, loss = 1.15 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 21:34:04.126029: step 51280, loss = 1.08 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:34:11.248976: step 51290, loss = 1.09 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 21:34:18.366964: step 51300, loss = 1.03 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 21:34:28.476633: step 51310, loss = 1.07 (46.9 examples/sec; 0.682 sec/batch)
2018-10-16 21:34:35.610086: step 51320, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:34:42.667051: step 51330, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 21:34:49.809062: step 51340, loss = 1.15 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:34:56.901662: step 51350, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 21:35:03.956908: step 51360, loss = 1.04 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 21:35:11.165604: step 51370, loss = 1.10 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 21:35:18.260857: step 51380, loss = 1.04 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 21:35:25.343083: step 51390, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:35:32.567673: step 51400, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 21:35:42.168817: step 51410, loss = 1.09 (48.1 examples/sec; 0.666 sec/batch)
2018-10-16 21:35:49.245058: step 51420, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 21:35:56.338504: step 51430, loss = 1.07 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 21:36:03.561595: step 51440, loss = 0.99 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 21:36:10.667514: step 51450, loss = 1.10 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 21:36:17.800053: step 51460, loss = 1.07 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 21:36:24.947013: step 51470, loss = 1.01 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 21:36:32.036796: step 51480, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:36:39.108339: step 51490, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 21:36:46.273035: step 51500, loss = 1.24 (42.5 examples/sec; 0.752 sec/batch)
2018-10-16 21:36:56.301728: step 51510, loss = 1.01 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 21:37:03.380576: step 51520, loss = 1.00 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 21:37:10.517044: step 51530, loss = 1.07 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 21:37:17.702085: step 51540, loss = 1.11 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 21:37:24.816737: step 51550, loss = 1.10 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:37:31.882578: step 51560, loss = 1.14 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 21:37:39.032320: step 51570, loss = 1.02 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 21:37:46.197688: step 51580, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:37:53.297872: step 51590, loss = 1.11 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 21:38:00.481761: step 51600, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 21:38:10.510031: step 51610, loss = 1.04 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 21:38:17.600970: step 51620, loss = 1.00 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 21:38:24.655433: step 51630, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:38:31.819698: step 51640, loss = 1.22 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 21:38:38.917172: step 51650, loss = 0.99 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 21:38:46.007075: step 51660, loss = 1.03 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 21:38:53.163043: step 51670, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 21:39:00.326406: step 51680, loss = 1.05 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 21:39:07.483261: step 51690, loss = 1.05 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 21:39:14.574376: step 51700, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 21:39:24.220693: step 51710, loss = 1.03 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 21:39:31.387518: step 51720, loss = 1.17 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 21:39:38.441456: step 51730, loss = 1.21 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:39:45.526632: step 51740, loss = 1.17 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:39:52.639203: step 51750, loss = 1.16 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 21:39:59.732505: step 51760, loss = 1.16 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 21:40:06.798849: step 51770, loss = 1.10 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 21:40:13.921401: step 51780, loss = 1.03 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 21:40:21.085943: step 51790, loss = 1.10 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 21:40:28.206229: step 51800, loss = 1.06 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 21:40:38.168594: step 51810, loss = 1.09 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 21:40:45.318798: step 51820, loss = 1.08 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 21:40:52.477557: step 51830, loss = 1.14 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 21:40:59.680814: step 51840, loss = 1.08 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 21:41:06.800457: step 51850, loss = 1.07 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 21:41:13.919538: step 51860, loss = 0.99 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 21:41:21.067455: step 51870, loss = 1.01 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 21:41:28.133092: step 51880, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 21:41:35.189768: step 51890, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:41:42.335946: step 51900, loss = 1.07 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 21:41:52.279646: step 51910, loss = 1.08 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 21:41:59.359682: step 51920, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:42:06.484563: step 51930, loss = 1.17 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:42:13.649768: step 51940, loss = 1.15 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 21:42:20.791584: step 51950, loss = 1.00 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 21:42:27.937817: step 51960, loss = 1.09 (42.3 examples/sec; 0.756 sec/batch)
2018-10-16 21:42:35.041198: step 51970, loss = 1.01 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 21:42:42.127353: step 51980, loss = 1.01 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 21:42:49.252498: step 51990, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 21:42:56.433146: step 52000, loss = 0.99 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 21:43:06.316025: step 52010, loss = 1.02 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 21:43:13.446065: step 52020, loss = 1.05 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 21:43:20.467265: step 52030, loss = 1.14 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 21:43:27.572371: step 52040, loss = 1.08 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 21:43:34.724232: step 52050, loss = 1.03 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 21:43:41.796580: step 52060, loss = 1.20 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 21:43:48.972676: step 52070, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 21:43:56.066728: step 52080, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 21:44:03.188562: step 52090, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 21:44:10.305397: step 52100, loss = 1.09 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 21:44:19.992850: step 52110, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:44:27.083991: step 52120, loss = 1.14 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 21:44:34.262467: step 52130, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:44:41.430700: step 52140, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 21:44:48.556978: step 52150, loss = 1.17 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 21:44:55.687571: step 52160, loss = 1.03 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 21:45:02.820354: step 52170, loss = 1.00 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 21:45:09.920694: step 52180, loss = 1.13 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 21:45:17.117410: step 52190, loss = 1.06 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 21:45:24.133742: step 52200, loss = 1.07 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 21:45:33.875744: step 52210, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 21:45:40.888631: step 52220, loss = 1.07 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 21:45:48.036967: step 52230, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 21:45:55.107328: step 52240, loss = 1.05 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 21:46:02.203086: step 52250, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:46:09.245657: step 52260, loss = 1.06 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 21:46:16.379350: step 52270, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:46:23.485148: step 52280, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:46:30.605831: step 52290, loss = 1.11 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 21:46:37.754176: step 52300, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 21:46:47.432801: step 52310, loss = 1.05 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 21:46:54.504972: step 52320, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:47:01.709545: step 52330, loss = 1.00 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 21:47:08.817514: step 52340, loss = 1.15 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 21:47:15.857030: step 52350, loss = 1.16 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 21:47:22.984020: step 52360, loss = 1.10 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:47:30.144424: step 52370, loss = 1.03 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 21:47:37.306567: step 52380, loss = 0.99 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 21:47:44.455921: step 52390, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 21:47:51.618085: step 52400, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 21:48:01.397515: step 52410, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 21:48:08.520967: step 52420, loss = 1.24 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 21:48:15.582593: step 52430, loss = 1.11 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 21:48:22.721974: step 52440, loss = 1.26 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:48:29.890592: step 52450, loss = 1.29 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 21:48:37.096604: step 52460, loss = 1.08 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 21:48:44.250300: step 52470, loss = 1.18 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 21:48:51.406332: step 52480, loss = 1.12 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 21:48:58.500807: step 52490, loss = 1.06 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 21:49:05.585294: step 52500, loss = 1.05 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 21:49:15.220772: step 52510, loss = 1.04 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 21:49:22.352907: step 52520, loss = 1.02 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 21:49:29.518922: step 52530, loss = 1.00 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 21:49:36.675681: step 52540, loss = 1.18 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 21:49:43.697160: step 52550, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 21:49:50.807252: step 52560, loss = 1.02 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 21:49:57.942856: step 52570, loss = 1.08 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 21:50:05.038693: step 52580, loss = 1.08 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:50:12.134217: step 52590, loss = 1.01 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 21:50:19.320930: step 52600, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 21:50:28.949708: step 52610, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 21:50:35.983352: step 52620, loss = 1.09 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 21:50:43.091298: step 52630, loss = 1.03 (47.1 examples/sec; 0.680 sec/batch)
2018-10-16 21:50:50.155689: step 52640, loss = 1.00 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 21:50:57.283664: step 52650, loss = 1.19 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 21:51:04.369297: step 52660, loss = 1.04 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 21:51:11.522734: step 52670, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 21:51:18.565083: step 52680, loss = 1.01 (46.6 examples/sec; 0.687 sec/batch)
2018-10-16 21:51:25.751587: step 52690, loss = 1.16 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 21:51:32.854079: step 52700, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:51:42.384145: step 52710, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 21:51:49.503861: step 52720, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 21:51:56.645222: step 52730, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:52:03.781966: step 52740, loss = 1.08 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 21:52:10.919990: step 52750, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:52:18.001126: step 52760, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 21:52:25.145930: step 52770, loss = 1.05 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 21:52:32.300827: step 52780, loss = 1.18 (46.9 examples/sec; 0.682 sec/batch)
2018-10-16 21:52:39.456362: step 52790, loss = 1.15 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 21:52:46.576456: step 52800, loss = 1.15 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 21:52:56.279573: step 52810, loss = 1.00 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 21:53:03.393650: step 52820, loss = 1.07 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 21:53:10.471702: step 52830, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:53:17.602865: step 52840, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 21:53:24.667379: step 52850, loss = 0.99 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 21:53:31.783718: step 52860, loss = 1.01 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 21:53:38.925392: step 52870, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 21:53:46.012001: step 52880, loss = 1.00 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 21:53:53.137023: step 52890, loss = 1.23 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 21:54:00.199065: step 52900, loss = 1.06 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 21:54:10.212630: step 52910, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:54:17.280882: step 52920, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:54:24.401101: step 52930, loss = 1.01 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 21:54:31.507908: step 52940, loss = 1.06 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 21:54:38.669427: step 52950, loss = 1.07 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 21:54:45.789054: step 52960, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:54:52.909040: step 52970, loss = 1.11 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 21:55:00.088636: step 52980, loss = 1.16 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 21:55:07.226926: step 52990, loss = 1.00 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 21:55:14.369117: step 53000, loss = 1.17 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:55:24.174618: step 53010, loss = 1.02 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 21:55:31.231762: step 53020, loss = 1.04 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 21:55:38.343018: step 53030, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 21:55:45.502895: step 53040, loss = 1.13 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 21:55:52.704130: step 53050, loss = 1.11 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 21:55:59.806072: step 53060, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 21:56:06.864378: step 53070, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:56:14.040369: step 53080, loss = 1.03 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 21:56:21.214300: step 53090, loss = 1.12 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 21:56:28.365959: step 53100, loss = 1.11 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:56:38.065620: step 53110, loss = 1.05 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 21:56:45.142663: step 53120, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 21:56:52.329325: step 53130, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 21:56:59.412048: step 53140, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 21:57:06.513226: step 53150, loss = 1.20 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 21:57:13.665701: step 53160, loss = 1.10 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 21:57:20.791689: step 53170, loss = 1.02 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 21:57:27.798270: step 53180, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 21:57:34.900926: step 53190, loss = 1.07 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:57:42.013582: step 53200, loss = 1.00 (42.8 examples/sec; 0.748 sec/batch)
2018-10-16 21:57:51.822549: step 53210, loss = 1.12 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 21:57:58.875565: step 53220, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:58:06.070012: step 53230, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 21:58:13.193622: step 53240, loss = 1.15 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 21:58:20.294515: step 53250, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 21:58:27.513523: step 53260, loss = 1.13 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 21:58:34.642701: step 53270, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 21:58:41.773268: step 53280, loss = 1.05 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 21:58:48.955444: step 53290, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 21:58:56.060107: step 53300, loss = 1.02 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 21:59:05.625027: step 53310, loss = 1.03 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 21:59:12.721052: step 53320, loss = 1.15 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 21:59:19.883982: step 53330, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 21:59:27.028545: step 53340, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 21:59:34.213247: step 53350, loss = 1.12 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 21:59:41.326629: step 53360, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 21:59:48.410576: step 53370, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 21:59:55.534221: step 53380, loss = 1.15 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 22:00:02.624356: step 53390, loss = 1.04 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 22:00:09.694953: step 53400, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:00:19.271980: step 53410, loss = 1.25 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 22:00:26.396716: step 53420, loss = 1.00 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 22:00:33.592288: step 53430, loss = 1.06 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 22:00:40.732516: step 53440, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 22:00:47.869361: step 53450, loss = 1.03 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 22:00:55.009271: step 53460, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:01:02.086407: step 53470, loss = 0.99 (46.5 examples/sec; 0.687 sec/batch)
2018-10-16 22:01:09.224492: step 53480, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 22:01:16.306737: step 53490, loss = 1.20 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 22:01:23.414492: step 53500, loss = 1.06 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 22:01:32.996775: step 53510, loss = 1.17 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:01:40.015796: step 53520, loss = 0.99 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 22:01:47.159408: step 53530, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 22:01:54.355278: step 53540, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:02:01.492610: step 53550, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:02:08.680571: step 53560, loss = 1.24 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 22:02:15.717337: step 53570, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:02:22.876180: step 53580, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 22:02:30.012016: step 53590, loss = 1.11 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 22:02:37.101824: step 53600, loss = 1.03 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 22:02:46.791542: step 53610, loss = 1.05 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 22:02:53.906921: step 53620, loss = 1.05 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 22:03:01.094167: step 53630, loss = 1.07 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:03:08.236154: step 53640, loss = 1.01 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 22:03:15.288295: step 53650, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:03:22.456394: step 53660, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 22:03:29.531148: step 53670, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 22:03:36.622092: step 53680, loss = 1.05 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 22:03:43.796205: step 53690, loss = 1.07 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 22:03:50.882548: step 53700, loss = 1.11 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:04:00.704182: step 53710, loss = 1.01 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 22:04:07.853062: step 53720, loss = 1.02 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 22:04:15.025626: step 53730, loss = 1.03 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 22:04:22.143744: step 53740, loss = 1.11 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 22:04:29.255808: step 53750, loss = 1.01 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 22:04:36.379299: step 53760, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 22:04:43.494529: step 53770, loss = 1.00 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 22:04:50.584293: step 53780, loss = 1.09 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:04:57.669359: step 53790, loss = 1.09 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:05:04.864608: step 53800, loss = 0.99 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 22:05:15.399369: step 53810, loss = 1.01 (47.7 examples/sec; 0.671 sec/batch)
2018-10-16 22:05:22.383237: step 53820, loss = 1.05 (46.4 examples/sec; 0.689 sec/batch)
2018-10-16 22:05:29.528396: step 53830, loss = 1.06 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:05:36.574108: step 53840, loss = 1.05 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 22:05:43.658023: step 53850, loss = 1.07 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 22:05:50.802383: step 53860, loss = 1.02 (46.7 examples/sec; 0.686 sec/batch)
2018-10-16 22:05:57.974444: step 53870, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:06:05.021814: step 53880, loss = 1.06 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 22:06:12.197188: step 53890, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:06:19.300416: step 53900, loss = 1.04 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 22:06:28.991229: step 53910, loss = 1.04 (46.1 examples/sec; 0.693 sec/batch)
2018-10-16 22:06:36.071771: step 53920, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 22:06:43.196354: step 53930, loss = 1.09 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 22:06:50.253057: step 53940, loss = 1.13 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:06:57.349791: step 53950, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:07:04.444719: step 53960, loss = 1.05 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 22:07:11.603823: step 53970, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 22:07:18.659569: step 53980, loss = 1.16 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 22:07:25.737590: step 53990, loss = 1.04 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 22:07:32.823871: step 54000, loss = 1.09 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 22:07:42.451238: step 54010, loss = 1.09 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:07:49.516082: step 54020, loss = 1.13 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 22:07:56.671163: step 54030, loss = 1.05 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 22:08:03.774747: step 54040, loss = 1.02 (48.1 examples/sec; 0.666 sec/batch)
2018-10-16 22:08:10.891770: step 54050, loss = 1.07 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 22:08:17.983232: step 54060, loss = 1.01 (46.6 examples/sec; 0.686 sec/batch)
2018-10-16 22:08:25.151837: step 54070, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:08:32.286345: step 54080, loss = 1.17 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 22:08:39.429169: step 54090, loss = 1.05 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 22:08:46.577559: step 54100, loss = 1.10 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 22:08:56.202464: step 54110, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:09:03.294639: step 54120, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 22:09:10.476316: step 54130, loss = 1.10 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:09:17.630298: step 54140, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 22:09:24.754317: step 54150, loss = 1.15 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 22:09:31.874735: step 54160, loss = 1.18 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 22:09:38.969469: step 54170, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 22:09:46.085749: step 54180, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:09:53.147313: step 54190, loss = 1.04 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 22:10:00.210340: step 54200, loss = 1.23 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:10:09.967308: step 54210, loss = 1.03 (43.4 examples/sec; 0.736 sec/batch)
2018-10-16 22:10:17.150710: step 54220, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:10:24.307418: step 54230, loss = 1.00 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 22:10:31.407129: step 54240, loss = 1.12 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:10:38.508354: step 54250, loss = 1.10 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 22:10:45.610343: step 54260, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 22:10:52.695873: step 54270, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:10:59.768001: step 54280, loss = 1.00 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 22:11:06.978252: step 54290, loss = 1.08 (42.6 examples/sec; 0.751 sec/batch)
2018-10-16 22:11:14.077698: step 54300, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:11:24.103206: step 54310, loss = 1.16 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:11:31.205373: step 54320, loss = 0.99 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 22:11:38.378678: step 54330, loss = 1.12 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 22:11:45.480317: step 54340, loss = 1.06 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 22:11:52.613996: step 54350, loss = 1.07 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 22:11:59.722668: step 54360, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:12:06.783781: step 54370, loss = 1.05 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 22:12:13.895523: step 54380, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 22:12:20.955850: step 54390, loss = 1.05 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 22:12:28.064796: step 54400, loss = 1.02 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 22:12:37.758661: step 54410, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:12:44.855454: step 54420, loss = 1.10 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 22:12:51.995569: step 54430, loss = 0.99 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 22:12:59.122736: step 54440, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 22:13:06.255654: step 54450, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 22:13:13.333212: step 54460, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 22:13:20.516467: step 54470, loss = 1.13 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 22:13:27.627312: step 54480, loss = 1.03 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 22:13:34.825897: step 54490, loss = 1.13 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 22:13:41.999369: step 54500, loss = 0.99 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 22:13:51.681023: step 54510, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 22:13:58.765012: step 54520, loss = 1.17 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 22:14:05.912730: step 54530, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 22:14:13.040519: step 54540, loss = 1.13 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 22:14:20.119047: step 54550, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 22:14:27.248756: step 54560, loss = 0.99 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 22:14:34.291110: step 54570, loss = 1.12 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 22:14:41.401075: step 54580, loss = 1.04 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 22:14:48.521107: step 54590, loss = 1.12 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:14:55.696543: step 54600, loss = 1.24 (46.7 examples/sec; 0.686 sec/batch)
2018-10-16 22:15:05.723161: step 54610, loss = 1.07 (47.1 examples/sec; 0.680 sec/batch)
2018-10-16 22:15:12.815936: step 54620, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:15:19.851476: step 54630, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:15:26.931409: step 54640, loss = 1.08 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 22:15:34.070209: step 54650, loss = 1.09 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 22:15:41.342596: step 54660, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 22:15:48.436874: step 54670, loss = 1.14 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 22:15:55.593896: step 54680, loss = 1.10 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 22:16:02.744694: step 54690, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 22:16:09.864984: step 54700, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:16:19.568212: step 54710, loss = 1.11 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 22:16:26.652909: step 54720, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 22:16:33.816984: step 54730, loss = 1.19 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:16:40.973527: step 54740, loss = 1.02 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 22:16:48.131625: step 54750, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:16:55.197554: step 54760, loss = 1.02 (46.6 examples/sec; 0.687 sec/batch)
2018-10-16 22:17:02.342058: step 54770, loss = 0.99 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 22:17:09.436450: step 54780, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:17:16.470215: step 54790, loss = 1.01 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 22:17:23.617701: step 54800, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 22:17:33.613273: step 54810, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 22:17:40.736136: step 54820, loss = 1.06 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 22:17:47.812685: step 54830, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 22:17:54.889999: step 54840, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 22:18:02.038243: step 54850, loss = 1.07 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 22:18:09.169675: step 54860, loss = 1.05 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 22:18:16.374041: step 54870, loss = 1.05 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 22:18:23.568105: step 54880, loss = 1.12 (42.9 examples/sec; 0.745 sec/batch)
2018-10-16 22:18:30.665983: step 54890, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 22:18:37.807582: step 54900, loss = 1.03 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 22:18:47.920217: step 54910, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 22:18:54.934647: step 54920, loss = 1.00 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 22:19:02.072946: step 54930, loss = 1.00 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 22:19:09.182393: step 54940, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 22:19:16.355059: step 54950, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 22:19:23.465378: step 54960, loss = 1.10 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 22:19:30.533426: step 54970, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 22:19:37.682667: step 54980, loss = 1.05 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 22:19:44.746681: step 54990, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:19:51.851984: step 55000, loss = 1.02 (46.4 examples/sec; 0.689 sec/batch)
2018-10-16 22:20:05.103428: step 55010, loss = 1.00 (48.5 examples/sec; 0.659 sec/batch)
2018-10-16 22:20:11.886092: step 55020, loss = 1.19 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:20:18.933147: step 55030, loss = 1.07 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 22:20:26.054463: step 55040, loss = 1.08 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 22:20:33.189652: step 55050, loss = 1.04 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 22:20:40.320959: step 55060, loss = 1.00 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 22:20:47.450540: step 55070, loss = 0.99 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 22:20:54.523137: step 55080, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:21:01.696934: step 55090, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:21:08.820483: step 55100, loss = 1.02 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 22:21:18.551161: step 55110, loss = 1.05 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 22:21:25.668819: step 55120, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 22:21:32.775941: step 55130, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 22:21:39.905975: step 55140, loss = 1.04 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 22:21:47.034808: step 55150, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 22:21:54.166275: step 55160, loss = 1.06 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 22:22:01.264753: step 55170, loss = 1.07 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 22:22:08.403492: step 55180, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 22:22:15.525020: step 55190, loss = 1.07 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:22:22.665728: step 55200, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:22:32.345314: step 55210, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:22:39.378450: step 55220, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 22:22:46.512533: step 55230, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:22:53.582030: step 55240, loss = 1.19 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 22:23:00.679567: step 55250, loss = 1.02 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 22:23:07.814101: step 55260, loss = 0.99 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 22:23:14.912414: step 55270, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 22:23:22.136200: step 55280, loss = 1.06 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 22:23:29.230063: step 55290, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 22:23:36.411723: step 55300, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 22:23:46.238674: step 55310, loss = 1.04 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 22:23:53.434160: step 55320, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 22:24:00.510906: step 55330, loss = 1.06 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 22:24:07.605467: step 55340, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 22:24:14.665166: step 55350, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 22:24:21.805698: step 55360, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:24:28.933270: step 55370, loss = 1.11 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 22:24:36.004292: step 55380, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:24:43.064970: step 55390, loss = 0.99 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 22:24:50.189167: step 55400, loss = 1.18 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 22:24:59.984739: step 55410, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 22:25:07.101482: step 55420, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 22:25:14.183254: step 55430, loss = 1.02 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 22:25:21.287968: step 55440, loss = 1.09 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 22:25:28.425004: step 55450, loss = 1.10 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 22:25:35.669613: step 55460, loss = 1.01 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 22:25:42.668061: step 55470, loss = 1.22 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 22:25:49.784684: step 55480, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:25:56.925930: step 55490, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 22:26:03.960937: step 55500, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 22:26:13.733090: step 55510, loss = 1.05 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 22:26:20.800135: step 55520, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:26:27.861042: step 55530, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:26:34.998367: step 55540, loss = 1.00 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 22:26:42.142429: step 55550, loss = 1.07 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 22:26:49.222793: step 55560, loss = 0.99 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 22:26:56.308456: step 55570, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:27:03.378401: step 55580, loss = 1.12 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:27:10.514909: step 55590, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 22:27:17.660395: step 55600, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 22:27:27.519045: step 55610, loss = 1.14 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 22:27:34.547474: step 55620, loss = 1.00 (47.5 examples/sec; 0.674 sec/batch)
2018-10-16 22:27:41.596545: step 55630, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 22:27:48.756611: step 55640, loss = 0.99 (43.0 examples/sec; 0.745 sec/batch)
2018-10-16 22:27:55.847062: step 55650, loss = 1.23 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 22:28:02.996908: step 55660, loss = 1.02 (43.3 examples/sec; 0.738 sec/batch)
2018-10-16 22:28:10.193772: step 55670, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 22:28:17.358818: step 55680, loss = 1.06 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 22:28:24.458590: step 55690, loss = 1.15 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 22:28:31.580715: step 55700, loss = 1.02 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 22:28:41.275441: step 55710, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:28:48.349336: step 55720, loss = 1.11 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:28:55.438078: step 55730, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:29:02.547435: step 55740, loss = 1.07 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 22:29:09.724978: step 55750, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:29:16.875813: step 55760, loss = 1.06 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 22:29:24.014486: step 55770, loss = 1.02 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 22:29:31.172651: step 55780, loss = 1.13 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 22:29:38.333087: step 55790, loss = 1.05 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 22:29:45.432149: step 55800, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 22:29:55.056675: step 55810, loss = 1.17 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 22:30:02.098080: step 55820, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 22:30:09.117970: step 55830, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 22:30:16.225840: step 55840, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 22:30:23.396249: step 55850, loss = 1.19 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 22:30:30.516053: step 55860, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 22:30:37.630260: step 55870, loss = 0.99 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 22:30:44.723962: step 55880, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:30:51.828940: step 55890, loss = 1.20 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 22:30:58.946396: step 55900, loss = 1.10 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 22:31:09.557053: step 55910, loss = 1.02 (47.1 examples/sec; 0.680 sec/batch)
2018-10-16 22:31:16.570571: step 55920, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:31:23.735228: step 55930, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 22:31:30.857889: step 55940, loss = 1.00 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 22:31:38.041696: step 55950, loss = 0.99 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 22:31:45.199359: step 55960, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 22:31:52.324270: step 55970, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 22:31:59.468098: step 55980, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 22:32:06.554241: step 55990, loss = 1.07 (46.7 examples/sec; 0.686 sec/batch)
2018-10-16 22:32:13.691230: step 56000, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 22:32:23.399718: step 56010, loss = 1.07 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 22:32:30.504639: step 56020, loss = 1.08 (43.7 examples/sec; 0.733 sec/batch)
2018-10-16 22:32:37.576847: step 56030, loss = 1.10 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 22:32:44.762244: step 56040, loss = 1.03 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 22:32:51.886844: step 56050, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 22:32:58.937106: step 56060, loss = 1.05 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 22:33:06.021407: step 56070, loss = 0.99 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 22:33:13.103075: step 56080, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 22:33:20.216257: step 56090, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 22:33:27.319328: step 56100, loss = 1.18 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:33:36.963853: step 56110, loss = 1.03 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 22:33:44.022886: step 56120, loss = 1.11 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:33:51.136586: step 56130, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:33:58.207671: step 56140, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:34:05.382195: step 56150, loss = 1.15 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:34:12.462003: step 56160, loss = 1.02 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 22:34:19.549040: step 56170, loss = 1.07 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 22:34:26.572620: step 56180, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 22:34:33.759687: step 56190, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 22:34:40.897668: step 56200, loss = 1.20 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 22:34:50.557856: step 56210, loss = 1.10 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:34:57.691430: step 56220, loss = 1.09 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 22:35:04.772063: step 56230, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:35:11.820005: step 56240, loss = 1.05 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 22:35:18.982747: step 56250, loss = 1.09 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:35:26.135310: step 56260, loss = 1.05 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 22:35:33.278621: step 56270, loss = 1.02 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 22:35:40.422844: step 56280, loss = 1.15 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 22:35:47.535679: step 56290, loss = 1.02 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 22:35:54.615532: step 56300, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 22:36:04.859213: step 56310, loss = 1.00 (47.5 examples/sec; 0.673 sec/batch)
2018-10-16 22:36:11.965662: step 56320, loss = 1.02 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 22:36:19.118759: step 56330, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 22:36:26.291780: step 56340, loss = 1.07 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 22:36:33.367483: step 56350, loss = 1.04 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:36:40.546245: step 56360, loss = 0.99 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 22:36:47.574688: step 56370, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:36:54.627656: step 56380, loss = 1.30 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:37:01.682858: step 56390, loss = 1.17 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:37:08.801920: step 56400, loss = 0.99 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 22:37:18.456828: step 56410, loss = 1.03 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 22:37:25.498338: step 56420, loss = 1.16 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 22:37:32.619796: step 56430, loss = 1.10 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:37:39.757606: step 56440, loss = 1.04 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 22:37:47.001371: step 56450, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 22:37:54.152816: step 56460, loss = 1.12 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 22:38:01.308949: step 56470, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:38:08.464788: step 56480, loss = 1.14 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 22:38:15.564001: step 56490, loss = 1.07 (44.2 examples/sec; 0.723 sec/batch)
2018-10-16 22:38:22.716546: step 56500, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:38:32.652929: step 56510, loss = 1.09 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 22:38:39.672365: step 56520, loss = 1.09 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 22:38:46.815036: step 56530, loss = 1.09 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 22:38:53.864451: step 56540, loss = 1.11 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 22:39:01.007161: step 56550, loss = 1.19 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 22:39:08.149578: step 56560, loss = 1.00 (45.9 examples/sec; 0.696 sec/batch)
2018-10-16 22:39:15.289232: step 56570, loss = 1.00 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 22:39:22.371909: step 56580, loss = 1.11 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 22:39:29.527821: step 56590, loss = 1.09 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:39:36.611270: step 56600, loss = 1.07 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:39:46.669093: step 56610, loss = 1.02 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 22:39:53.728660: step 56620, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 22:40:00.826091: step 56630, loss = 1.05 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:40:07.945061: step 56640, loss = 1.00 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 22:40:15.068627: step 56650, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:40:22.096293: step 56660, loss = 1.06 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 22:40:29.295304: step 56670, loss = 1.03 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 22:40:36.316102: step 56680, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 22:40:43.394622: step 56690, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:40:50.594244: step 56700, loss = 0.99 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 22:41:00.295694: step 56710, loss = 1.07 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 22:41:07.294995: step 56720, loss = 1.06 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:41:14.502978: step 56730, loss = 1.12 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 22:41:21.549526: step 56740, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 22:41:28.619955: step 56750, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 22:41:35.781247: step 56760, loss = 1.08 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 22:41:42.903921: step 56770, loss = 1.09 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 22:41:50.078325: step 56780, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:41:57.266770: step 56790, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 22:42:04.343536: step 56800, loss = 1.12 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:42:13.973563: step 56810, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 22:42:21.100974: step 56820, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:42:28.227675: step 56830, loss = 1.02 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 22:42:35.337772: step 56840, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 22:42:42.432826: step 56850, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:42:49.564339: step 56860, loss = 1.03 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 22:42:56.705040: step 56870, loss = 1.01 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 22:43:03.854234: step 56880, loss = 1.10 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 22:43:10.979629: step 56890, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:43:18.094646: step 56900, loss = 1.01 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 22:43:27.718160: step 56910, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 22:43:34.741339: step 56920, loss = 1.03 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 22:43:41.808166: step 56930, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 22:43:48.940918: step 56940, loss = 1.13 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 22:43:56.048194: step 56950, loss = 0.99 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 22:44:03.114967: step 56960, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:44:10.331795: step 56970, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 22:44:17.447141: step 56980, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:44:24.540672: step 56990, loss = 1.10 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 22:44:31.653379: step 57000, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:44:41.469573: step 57010, loss = 1.04 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 22:44:48.612175: step 57020, loss = 1.02 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 22:44:55.749831: step 57030, loss = 1.07 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 22:45:02.816080: step 57040, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:45:09.959058: step 57050, loss = 1.15 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 22:45:17.082072: step 57060, loss = 1.00 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 22:45:24.228154: step 57070, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 22:45:31.403769: step 57080, loss = 1.03 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 22:45:38.528758: step 57090, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 22:45:45.721404: step 57100, loss = 1.21 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 22:45:55.684266: step 57110, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:46:02.729969: step 57120, loss = 1.23 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 22:46:09.810464: step 57130, loss = 1.04 (46.9 examples/sec; 0.682 sec/batch)
2018-10-16 22:46:16.990877: step 57140, loss = 1.04 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 22:46:24.223573: step 57150, loss = 1.04 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 22:46:31.345024: step 57160, loss = 1.06 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:46:38.468295: step 57170, loss = 1.06 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 22:46:45.557695: step 57180, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:46:52.720610: step 57190, loss = 1.13 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 22:46:59.787300: step 57200, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 22:47:09.480365: step 57210, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:47:16.604083: step 57220, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:47:23.717197: step 57230, loss = 1.02 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 22:47:30.794647: step 57240, loss = 1.00 (45.0 examples/sec; 0.710 sec/batch)
2018-10-16 22:47:37.939901: step 57250, loss = 1.16 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:47:45.072407: step 57260, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 22:47:52.155608: step 57270, loss = 1.04 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 22:47:59.262485: step 57280, loss = 1.03 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 22:48:06.401306: step 57290, loss = 1.19 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 22:48:13.576442: step 57300, loss = 1.01 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 22:48:23.258429: step 57310, loss = 1.07 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 22:48:30.415943: step 57320, loss = 1.03 (43.4 examples/sec; 0.738 sec/batch)
2018-10-16 22:48:37.458327: step 57330, loss = 1.16 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 22:48:44.500620: step 57340, loss = 1.05 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 22:48:51.627584: step 57350, loss = 0.99 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 22:48:58.733544: step 57360, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:49:05.754111: step 57370, loss = 1.02 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 22:49:12.868465: step 57380, loss = 1.11 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:49:19.979885: step 57390, loss = 1.06 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 22:49:27.151174: step 57400, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 22:49:36.798944: step 57410, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:49:43.866508: step 57420, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 22:49:50.954507: step 57430, loss = 1.03 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 22:49:58.132576: step 57440, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 22:50:05.239173: step 57450, loss = 1.19 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 22:50:12.318400: step 57460, loss = 1.02 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 22:50:19.402711: step 57470, loss = 1.12 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 22:50:26.478870: step 57480, loss = 0.99 (43.8 examples/sec; 0.731 sec/batch)
2018-10-16 22:50:33.570239: step 57490, loss = 1.01 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 22:50:40.680234: step 57500, loss = 1.12 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 22:50:50.409262: step 57510, loss = 1.14 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 22:50:57.513466: step 57520, loss = 1.17 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 22:51:04.575636: step 57530, loss = 1.03 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 22:51:11.679877: step 57540, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:51:18.789365: step 57550, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 22:51:25.874532: step 57560, loss = 1.04 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 22:51:32.961291: step 57570, loss = 1.08 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 22:51:40.112853: step 57580, loss = 1.10 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:51:47.173923: step 57590, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:51:54.289543: step 57600, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:52:03.914838: step 57610, loss = 1.15 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 22:52:10.975660: step 57620, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 22:52:18.026158: step 57630, loss = 1.12 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:52:25.144587: step 57640, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 22:52:32.221503: step 57650, loss = 1.20 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:52:39.344455: step 57660, loss = 1.10 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 22:52:46.422708: step 57670, loss = 1.18 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 22:52:53.532853: step 57680, loss = 1.16 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 22:53:00.665502: step 57690, loss = 1.02 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 22:53:07.714837: step 57700, loss = 1.09 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:53:17.904486: step 57710, loss = 1.02 (47.6 examples/sec; 0.672 sec/batch)
2018-10-16 22:53:24.984107: step 57720, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 22:53:32.098993: step 57730, loss = 1.04 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 22:53:39.191055: step 57740, loss = 1.01 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 22:53:46.372888: step 57750, loss = 1.06 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 22:53:53.519281: step 57760, loss = 1.07 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 22:54:00.648310: step 57770, loss = 1.03 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 22:54:07.760208: step 57780, loss = 1.02 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 22:54:14.882179: step 57790, loss = 1.07 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 22:54:21.992608: step 57800, loss = 1.17 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 22:54:32.459661: step 57810, loss = 1.02 (47.7 examples/sec; 0.670 sec/batch)
2018-10-16 22:54:39.487250: step 57820, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 22:54:46.690936: step 57830, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 22:54:53.820625: step 57840, loss = 1.05 (47.6 examples/sec; 0.673 sec/batch)
2018-10-16 22:55:00.954874: step 57850, loss = 1.08 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 22:55:08.002481: step 57860, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 22:55:15.183844: step 57870, loss = 1.00 (46.4 examples/sec; 0.689 sec/batch)
2018-10-16 22:55:22.260478: step 57880, loss = 1.05 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 22:55:29.359015: step 57890, loss = 1.02 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 22:55:36.534412: step 57900, loss = 1.21 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 22:55:46.259501: step 57910, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 22:55:53.373566: step 57920, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:56:00.520350: step 57930, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 22:56:07.685972: step 57940, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 22:56:14.769474: step 57950, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 22:56:21.813686: step 57960, loss = 1.12 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 22:56:28.964463: step 57970, loss = 1.16 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:56:36.127998: step 57980, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 22:56:43.238423: step 57990, loss = 0.99 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 22:56:50.324559: step 58000, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:57:00.130241: step 58010, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 22:57:07.274031: step 58020, loss = 1.13 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 22:57:14.361674: step 58030, loss = 1.05 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:57:21.440914: step 58040, loss = 1.13 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 22:57:28.477002: step 58050, loss = 0.99 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 22:57:35.530005: step 58060, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 22:57:42.645412: step 58070, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 22:57:49.785020: step 58080, loss = 0.99 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 22:57:56.943176: step 58090, loss = 1.08 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 22:58:04.123445: step 58100, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 22:58:14.360041: step 58110, loss = 1.05 (47.2 examples/sec; 0.677 sec/batch)
2018-10-16 22:58:21.390470: step 58120, loss = 0.99 (47.2 examples/sec; 0.678 sec/batch)
2018-10-16 22:58:28.457337: step 58130, loss = 1.03 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 22:58:35.616433: step 58140, loss = 1.03 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 22:58:42.686050: step 58150, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 22:58:49.797608: step 58160, loss = 1.06 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 22:58:56.862201: step 58170, loss = 1.08 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 22:59:03.986645: step 58180, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 22:59:11.059582: step 58190, loss = 1.16 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 22:59:18.224109: step 58200, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 22:59:28.209656: step 58210, loss = 0.99 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 22:59:35.304942: step 58220, loss = 1.08 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 22:59:42.405107: step 58230, loss = 1.16 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 22:59:49.619027: step 58240, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 22:59:56.744922: step 58250, loss = 1.01 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 23:00:03.957420: step 58260, loss = 0.99 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 23:00:11.035448: step 58270, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 23:00:18.151473: step 58280, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 23:00:25.178101: step 58290, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 23:00:32.171764: step 58300, loss = 1.00 (47.5 examples/sec; 0.674 sec/batch)
2018-10-16 23:00:42.197981: step 58310, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 23:00:49.323104: step 58320, loss = 0.99 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 23:00:56.442325: step 58330, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 23:01:03.475588: step 58340, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:01:10.591312: step 58350, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:01:17.742773: step 58360, loss = 1.03 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 23:01:24.868478: step 58370, loss = 1.01 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 23:01:31.975065: step 58380, loss = 1.01 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 23:01:39.024952: step 58390, loss = 1.02 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 23:01:46.120557: step 58400, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 23:01:55.807348: step 58410, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 23:02:02.884950: step 58420, loss = 1.02 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 23:02:10.005842: step 58430, loss = 1.10 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 23:02:17.092576: step 58440, loss = 1.11 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 23:02:24.237566: step 58450, loss = 1.01 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 23:02:31.316268: step 58460, loss = 1.03 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 23:02:38.388881: step 58470, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 23:02:45.447495: step 58480, loss = 0.99 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 23:02:52.501221: step 58490, loss = 1.11 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 23:02:59.663911: step 58500, loss = 1.16 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 23:03:09.394723: step 58510, loss = 1.00 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 23:03:16.447812: step 58520, loss = 1.09 (43.6 examples/sec; 0.735 sec/batch)
2018-10-16 23:03:23.541742: step 58530, loss = 1.15 (43.2 examples/sec; 0.742 sec/batch)
2018-10-16 23:03:30.680067: step 58540, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 23:03:37.876742: step 58550, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 23:03:44.895395: step 58560, loss = 1.08 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 23:03:52.020776: step 58570, loss = 1.04 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 23:03:59.093556: step 58580, loss = 1.09 (46.4 examples/sec; 0.689 sec/batch)
2018-10-16 23:04:06.165555: step 58590, loss = 1.04 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 23:04:13.225395: step 58600, loss = 1.12 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 23:04:23.041745: step 58610, loss = 1.04 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 23:04:30.089145: step 58620, loss = 1.07 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:04:37.130627: step 58630, loss = 1.02 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 23:04:44.303934: step 58640, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 23:04:51.417882: step 58650, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 23:04:58.557048: step 58660, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 23:05:05.676838: step 58670, loss = 1.07 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 23:05:12.815199: step 58680, loss = 1.18 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 23:05:19.985935: step 58690, loss = 1.05 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 23:05:27.126957: step 58700, loss = 1.21 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 23:05:36.849103: step 58710, loss = 1.13 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 23:05:43.915396: step 58720, loss = 1.16 (46.3 examples/sec; 0.690 sec/batch)
2018-10-16 23:05:51.025724: step 58730, loss = 1.09 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 23:05:58.055734: step 58740, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 23:06:05.195175: step 58750, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 23:06:12.302389: step 58760, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 23:06:19.393631: step 58770, loss = 0.99 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 23:06:26.571854: step 58780, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 23:06:33.752962: step 58790, loss = 1.02 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 23:06:40.821839: step 58800, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 23:06:50.570994: step 58810, loss = 1.26 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 23:06:57.557289: step 58820, loss = 1.20 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 23:07:04.678539: step 58830, loss = 1.08 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 23:07:11.787227: step 58840, loss = 1.12 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 23:07:18.917366: step 58850, loss = 1.05 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 23:07:26.036396: step 58860, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:07:33.043564: step 58870, loss = 1.06 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 23:07:40.135755: step 58880, loss = 1.16 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 23:07:47.241058: step 58890, loss = 1.15 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 23:07:54.390190: step 58900, loss = 1.13 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 23:08:04.000613: step 58910, loss = 1.04 (47.3 examples/sec; 0.677 sec/batch)
2018-10-16 23:08:11.092181: step 58920, loss = 1.01 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 23:08:18.145866: step 58930, loss = 1.08 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 23:08:25.291355: step 58940, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 23:08:32.410000: step 58950, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 23:08:39.506383: step 58960, loss = 1.12 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 23:08:46.639754: step 58970, loss = 0.99 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 23:08:53.726833: step 58980, loss = 1.09 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 23:09:00.791532: step 58990, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:09:07.884933: step 59000, loss = 1.06 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:09:17.590805: step 59010, loss = 1.12 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 23:09:24.668001: step 59020, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 23:09:31.774055: step 59030, loss = 1.00 (43.3 examples/sec; 0.740 sec/batch)
2018-10-16 23:09:38.960516: step 59040, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 23:09:46.062185: step 59050, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 23:09:53.170902: step 59060, loss = 1.24 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 23:10:00.179197: step 59070, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 23:10:07.240354: step 59080, loss = 1.13 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:10:14.363781: step 59090, loss = 1.01 (43.9 examples/sec; 0.730 sec/batch)
2018-10-16 23:10:21.491297: step 59100, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:10:31.414523: step 59110, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 23:10:38.541440: step 59120, loss = 0.99 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 23:10:45.638241: step 59130, loss = 1.00 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 23:10:52.744576: step 59140, loss = 1.09 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 23:10:59.861139: step 59150, loss = 1.07 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 23:11:07.039773: step 59160, loss = 1.10 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 23:11:14.148409: step 59170, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 23:11:21.258613: step 59180, loss = 1.13 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 23:11:28.363658: step 59190, loss = 1.11 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 23:11:35.505153: step 59200, loss = 1.15 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 23:11:45.095793: step 59210, loss = 1.02 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 23:11:52.147192: step 59220, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:11:59.309856: step 59230, loss = 1.05 (43.1 examples/sec; 0.742 sec/batch)
2018-10-16 23:12:06.416259: step 59240, loss = 1.17 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 23:12:13.431996: step 59250, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 23:12:20.560993: step 59260, loss = 1.07 (46.1 examples/sec; 0.693 sec/batch)
2018-10-16 23:12:27.697631: step 59270, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 23:12:34.796590: step 59280, loss = 1.03 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 23:12:41.901317: step 59290, loss = 1.15 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 23:12:49.038362: step 59300, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:12:58.870678: step 59310, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:13:05.949106: step 59320, loss = 1.03 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 23:13:13.041952: step 59330, loss = 1.06 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 23:13:20.202237: step 59340, loss = 1.18 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 23:13:27.264302: step 59350, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 23:13:34.429873: step 59360, loss = 1.07 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 23:13:41.511543: step 59370, loss = 1.06 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 23:13:48.646468: step 59380, loss = 1.00 (46.9 examples/sec; 0.683 sec/batch)
2018-10-16 23:13:55.768463: step 59390, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:14:02.880128: step 59400, loss = 1.01 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 23:14:12.465852: step 59410, loss = 1.10 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:14:19.521273: step 59420, loss = 1.00 (47.7 examples/sec; 0.671 sec/batch)
2018-10-16 23:14:26.718212: step 59430, loss = 1.02 (43.7 examples/sec; 0.731 sec/batch)
2018-10-16 23:14:33.838184: step 59440, loss = 1.18 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 23:14:40.941262: step 59450, loss = 1.01 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 23:14:48.063417: step 59460, loss = 1.16 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 23:14:55.247080: step 59470, loss = 1.11 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 23:15:02.327474: step 59480, loss = 1.00 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 23:15:09.445299: step 59490, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 23:15:16.556893: step 59500, loss = 1.03 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 23:15:26.756529: step 59510, loss = 1.00 (47.6 examples/sec; 0.672 sec/batch)
2018-10-16 23:15:33.825418: step 59520, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 23:15:41.009328: step 59530, loss = 1.01 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 23:15:48.044030: step 59540, loss = 1.01 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 23:15:55.110547: step 59550, loss = 1.20 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:16:02.207433: step 59560, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 23:16:09.302146: step 59570, loss = 1.03 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 23:16:16.326458: step 59580, loss = 1.11 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 23:16:23.471259: step 59590, loss = 1.15 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 23:16:30.687305: step 59600, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 23:16:40.398273: step 59610, loss = 1.05 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 23:16:47.504113: step 59620, loss = 1.01 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 23:16:54.637662: step 59630, loss = 1.16 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:17:01.724466: step 59640, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 23:17:08.792079: step 59650, loss = 1.20 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:17:15.880245: step 59660, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 23:17:22.944837: step 59670, loss = 0.99 (47.3 examples/sec; 0.676 sec/batch)
2018-10-16 23:17:30.008907: step 59680, loss = 1.07 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 23:17:37.079284: step 59690, loss = 1.16 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 23:17:44.182478: step 59700, loss = 1.13 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 23:17:53.840729: step 59710, loss = 1.03 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 23:18:00.962136: step 59720, loss = 1.02 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 23:18:07.929039: step 59730, loss = 1.11 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 23:18:14.978165: step 59740, loss = 1.01 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 23:18:22.046641: step 59750, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:18:29.086189: step 59760, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 23:18:36.202052: step 59770, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 23:18:43.269622: step 59780, loss = 1.04 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 23:18:50.395320: step 59790, loss = 1.06 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 23:18:57.514095: step 59800, loss = 1.22 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:19:07.154504: step 59810, loss = 1.14 (46.9 examples/sec; 0.683 sec/batch)
2018-10-16 23:19:14.183973: step 59820, loss = 1.03 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 23:19:21.319716: step 59830, loss = 1.01 (45.7 examples/sec; 0.699 sec/batch)
2018-10-16 23:19:28.457688: step 59840, loss = 1.20 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 23:19:35.519176: step 59850, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 23:19:42.602913: step 59860, loss = 1.13 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 23:19:49.727610: step 59870, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 23:19:56.848410: step 59880, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 23:20:04.008986: step 59890, loss = 1.06 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 23:20:11.147490: step 59900, loss = 1.13 (42.7 examples/sec; 0.750 sec/batch)
2018-10-16 23:20:20.941043: step 59910, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 23:20:28.039101: step 59920, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 23:20:35.159403: step 59930, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:20:42.257756: step 59940, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:20:49.328526: step 59950, loss = 0.99 (47.6 examples/sec; 0.673 sec/batch)
2018-10-16 23:20:56.401889: step 59960, loss = 1.07 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 23:21:03.597960: step 59970, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 23:21:10.630740: step 59980, loss = 1.16 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 23:21:17.730925: step 59990, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 23:21:24.816364: step 60000, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:21:37.892357: step 60010, loss = 0.99 (47.8 examples/sec; 0.670 sec/batch)
2018-10-16 23:21:44.656673: step 60020, loss = 1.05 (47.1 examples/sec; 0.680 sec/batch)
2018-10-16 23:21:51.652470: step 60030, loss = 1.10 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 23:21:58.815935: step 60040, loss = 1.01 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 23:22:05.962547: step 60050, loss = 1.00 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 23:22:13.037555: step 60060, loss = 1.01 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 23:22:20.161588: step 60070, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 23:22:27.218903: step 60080, loss = 1.11 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 23:22:34.295597: step 60090, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 23:22:41.394230: step 60100, loss = 1.00 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 23:22:51.147149: step 60110, loss = 1.11 (44.7 examples/sec; 0.715 sec/batch)
2018-10-16 23:22:58.286565: step 60120, loss = 1.04 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 23:23:05.303672: step 60130, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 23:23:12.381374: step 60140, loss = 1.12 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:23:19.556084: step 60150, loss = 1.09 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 23:23:26.649640: step 60160, loss = 1.09 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 23:23:33.817251: step 60170, loss = 1.18 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:23:40.901449: step 60180, loss = 1.04 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 23:23:47.981589: step 60190, loss = 1.14 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 23:23:55.090009: step 60200, loss = 0.99 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 23:24:04.783754: step 60210, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 23:24:11.759197: step 60220, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 23:24:18.803644: step 60230, loss = 1.01 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 23:24:26.009108: step 60240, loss = 1.04 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 23:24:33.114836: step 60250, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 23:24:40.212911: step 60260, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:24:47.312405: step 60270, loss = 1.11 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 23:24:54.345678: step 60280, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 23:25:01.447793: step 60290, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 23:25:08.523345: step 60300, loss = 1.10 (47.1 examples/sec; 0.680 sec/batch)
2018-10-16 23:25:18.174212: step 60310, loss = 1.11 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 23:25:25.402827: step 60320, loss = 1.17 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 23:25:32.504988: step 60330, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 23:25:39.585725: step 60340, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 23:25:46.708566: step 60350, loss = 1.09 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 23:25:53.847783: step 60360, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:26:00.944636: step 60370, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:26:08.004378: step 60380, loss = 1.22 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:26:15.089503: step 60390, loss = 1.02 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 23:26:22.212116: step 60400, loss = 1.13 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 23:26:31.987591: step 60410, loss = 1.00 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 23:26:39.052902: step 60420, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 23:26:46.085758: step 60430, loss = 1.07 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 23:26:53.187891: step 60440, loss = 1.06 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 23:27:00.305388: step 60450, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:27:07.312633: step 60460, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 23:27:14.490397: step 60470, loss = 1.14 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 23:27:21.580142: step 60480, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 23:27:28.795445: step 60490, loss = 1.01 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 23:27:35.879313: step 60500, loss = 1.06 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 23:27:45.473563: step 60510, loss = 1.01 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 23:27:52.643799: step 60520, loss = 1.13 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 23:27:59.703444: step 60530, loss = 1.01 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 23:28:06.886166: step 60540, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 23:28:14.021657: step 60550, loss = 1.02 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 23:28:21.141744: step 60560, loss = 1.03 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 23:28:28.385974: step 60570, loss = 1.02 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 23:28:35.410106: step 60580, loss = 1.15 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 23:28:42.483811: step 60590, loss = 1.03 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 23:28:49.506924: step 60600, loss = 1.23 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 23:28:59.174954: step 60610, loss = 0.99 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 23:29:06.328874: step 60620, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 23:29:13.327936: step 60630, loss = 1.13 (47.3 examples/sec; 0.676 sec/batch)
2018-10-16 23:29:20.509365: step 60640, loss = 1.03 (44.0 examples/sec; 0.728 sec/batch)
2018-10-16 23:29:27.692501: step 60650, loss = 1.12 (46.4 examples/sec; 0.689 sec/batch)
2018-10-16 23:29:34.814253: step 60660, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:29:41.927608: step 60670, loss = 1.11 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 23:29:49.030803: step 60680, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 23:29:56.148340: step 60690, loss = 1.11 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 23:30:03.270788: step 60700, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 23:30:12.992993: step 60710, loss = 1.08 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:30:20.102266: step 60720, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 23:30:27.281105: step 60730, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 23:30:34.327249: step 60740, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 23:30:41.478462: step 60750, loss = 1.20 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 23:30:48.557403: step 60760, loss = 1.03 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 23:30:55.664953: step 60770, loss = 1.05 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 23:31:02.756285: step 60780, loss = 1.05 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 23:31:09.806857: step 60790, loss = 1.01 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 23:31:16.942186: step 60800, loss = 1.02 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 23:31:26.697868: step 60810, loss = 1.19 (43.3 examples/sec; 0.739 sec/batch)
2018-10-16 23:31:33.760593: step 60820, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:31:40.891615: step 60830, loss = 1.06 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 23:31:48.086895: step 60840, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 23:31:55.207351: step 60850, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 23:32:02.334058: step 60860, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 23:32:09.423006: step 60870, loss = 1.04 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 23:32:16.557901: step 60880, loss = 1.04 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 23:32:23.598508: step 60890, loss = 1.13 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 23:32:30.687497: step 60900, loss = 1.06 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:32:40.473891: step 60910, loss = 1.03 (46.7 examples/sec; 0.686 sec/batch)
2018-10-16 23:32:47.503612: step 60920, loss = 1.09 (46.4 examples/sec; 0.689 sec/batch)
2018-10-16 23:32:54.658499: step 60930, loss = 1.12 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 23:33:01.714708: step 60940, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 23:33:08.866991: step 60950, loss = 1.06 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 23:33:15.969571: step 60960, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 23:33:23.114263: step 60970, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 23:33:30.272132: step 60980, loss = 1.10 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:33:37.374064: step 60990, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 23:33:44.574303: step 61000, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 23:33:54.323009: step 61010, loss = 1.08 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 23:34:01.372219: step 61020, loss = 1.04 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 23:34:08.407695: step 61030, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 23:34:15.470798: step 61040, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 23:34:22.517922: step 61050, loss = 0.99 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 23:34:29.626944: step 61060, loss = 1.08 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 23:34:36.721793: step 61070, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 23:34:43.791512: step 61080, loss = 1.11 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:34:50.925104: step 61090, loss = 1.13 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 23:34:57.957899: step 61100, loss = 0.99 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 23:35:07.519460: step 61110, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 23:35:14.567819: step 61120, loss = 1.01 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 23:35:21.623024: step 61130, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 23:35:28.760307: step 61140, loss = 1.12 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 23:35:35.840972: step 61150, loss = 1.10 (45.9 examples/sec; 0.698 sec/batch)
2018-10-16 23:35:42.960915: step 61160, loss = 1.14 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:35:50.019610: step 61170, loss = 1.14 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 23:35:57.118273: step 61180, loss = 1.03 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 23:36:04.314801: step 61190, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:36:11.460952: step 61200, loss = 1.00 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 23:36:21.079773: step 61210, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 23:36:28.157739: step 61220, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 23:36:35.258240: step 61230, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:36:42.353910: step 61240, loss = 1.09 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 23:36:49.461239: step 61250, loss = 1.10 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 23:36:56.547639: step 61260, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:37:03.568034: step 61270, loss = 1.04 (46.3 examples/sec; 0.692 sec/batch)
2018-10-16 23:37:10.599951: step 61280, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 23:37:17.658841: step 61290, loss = 1.05 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 23:37:24.781533: step 61300, loss = 1.14 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:37:34.408777: step 61310, loss = 1.02 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 23:37:41.476203: step 61320, loss = 1.14 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 23:37:48.521541: step 61330, loss = 1.12 (46.8 examples/sec; 0.684 sec/batch)
2018-10-16 23:37:55.659624: step 61340, loss = 1.10 (44.4 examples/sec; 0.720 sec/batch)
2018-10-16 23:38:02.745232: step 61350, loss = 1.15 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 23:38:09.855969: step 61360, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:38:16.999869: step 61370, loss = 1.03 (43.9 examples/sec; 0.728 sec/batch)
2018-10-16 23:38:24.092657: step 61380, loss = 1.23 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 23:38:31.162106: step 61390, loss = 1.10 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:38:38.290621: step 61400, loss = 1.21 (45.2 examples/sec; 0.707 sec/batch)
2018-10-16 23:38:47.889323: step 61410, loss = 1.06 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 23:38:54.989691: step 61420, loss = 1.13 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 23:39:02.094148: step 61430, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 23:39:09.258275: step 61440, loss = 1.03 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 23:39:16.383564: step 61450, loss = 0.99 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 23:39:23.444274: step 61460, loss = 1.11 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 23:39:30.580214: step 61470, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-16 23:39:37.628148: step 61480, loss = 1.06 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 23:39:44.676639: step 61490, loss = 1.11 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 23:39:51.799078: step 61500, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:40:01.399886: step 61510, loss = 1.10 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 23:40:08.471765: step 61520, loss = 1.06 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 23:40:15.564055: step 61530, loss = 1.23 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 23:40:22.629079: step 61540, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 23:40:29.807370: step 61550, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-16 23:40:36.947466: step 61560, loss = 1.13 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 23:40:43.943753: step 61570, loss = 0.99 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 23:40:51.054279: step 61580, loss = 1.12 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 23:40:58.121411: step 61590, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 23:41:05.212586: step 61600, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:41:14.982234: step 61610, loss = 0.99 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 23:41:22.072232: step 61620, loss = 0.99 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 23:41:29.203342: step 61630, loss = 1.03 (44.5 examples/sec; 0.718 sec/batch)
2018-10-16 23:41:36.248205: step 61640, loss = 1.08 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 23:41:43.333591: step 61650, loss = 1.13 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 23:41:50.491123: step 61660, loss = 1.05 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:41:57.600868: step 61670, loss = 1.06 (46.0 examples/sec; 0.696 sec/batch)
2018-10-16 23:42:04.686961: step 61680, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 23:42:11.851320: step 61690, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 23:42:18.959933: step 61700, loss = 1.21 (43.5 examples/sec; 0.735 sec/batch)
2018-10-16 23:42:28.839896: step 61710, loss = 1.05 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 23:42:35.940067: step 61720, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 23:42:42.999522: step 61730, loss = 1.03 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 23:42:50.063347: step 61740, loss = 1.07 (46.2 examples/sec; 0.693 sec/batch)
2018-10-16 23:42:57.249484: step 61750, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 23:43:04.350413: step 61760, loss = 1.07 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 23:43:11.502539: step 61770, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 23:43:18.578419: step 61780, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 23:43:25.708641: step 61790, loss = 1.04 (46.2 examples/sec; 0.692 sec/batch)
2018-10-16 23:43:32.804928: step 61800, loss = 1.14 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 23:43:42.433618: step 61810, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 23:43:49.543304: step 61820, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 23:43:56.583073: step 61830, loss = 1.17 (45.2 examples/sec; 0.709 sec/batch)
2018-10-16 23:44:03.682816: step 61840, loss = 1.03 (45.4 examples/sec; 0.706 sec/batch)
2018-10-16 23:44:10.831992: step 61850, loss = 1.04 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 23:44:17.941734: step 61860, loss = 1.02 (43.0 examples/sec; 0.744 sec/batch)
2018-10-16 23:44:25.094423: step 61870, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 23:44:32.178698: step 61880, loss = 1.01 (44.1 examples/sec; 0.726 sec/batch)
2018-10-16 23:44:39.288123: step 61890, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:44:46.411878: step 61900, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 23:44:56.173584: step 61910, loss = 1.09 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 23:45:03.188727: step 61920, loss = 1.08 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 23:45:10.356067: step 61930, loss = 1.02 (43.9 examples/sec; 0.729 sec/batch)
2018-10-16 23:45:17.409081: step 61940, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:45:24.477437: step 61950, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:45:31.653951: step 61960, loss = 0.99 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 23:45:38.751600: step 61970, loss = 1.06 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 23:45:45.832118: step 61980, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 23:45:53.021528: step 61990, loss = 1.02 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 23:46:00.105745: step 62000, loss = 1.01 (44.2 examples/sec; 0.725 sec/batch)
2018-10-16 23:46:10.737670: step 62010, loss = 1.02 (47.7 examples/sec; 0.671 sec/batch)
2018-10-16 23:46:17.701623: step 62020, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:46:24.759433: step 62030, loss = 1.10 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 23:46:31.890730: step 62040, loss = 1.00 (43.7 examples/sec; 0.731 sec/batch)
2018-10-16 23:46:38.974295: step 62050, loss = 1.11 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:46:46.128484: step 62060, loss = 1.08 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 23:46:53.188720: step 62070, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:47:00.340492: step 62080, loss = 1.24 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 23:47:07.485923: step 62090, loss = 1.10 (42.7 examples/sec; 0.749 sec/batch)
2018-10-16 23:47:14.554969: step 62100, loss = 1.10 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:47:24.222740: step 62110, loss = 1.01 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 23:47:31.310725: step 62120, loss = 1.02 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 23:47:38.351709: step 62130, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:47:45.489772: step 62140, loss = 1.00 (46.9 examples/sec; 0.683 sec/batch)
2018-10-16 23:47:52.603456: step 62150, loss = 1.16 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 23:47:59.675955: step 62160, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 23:48:06.847745: step 62170, loss = 1.11 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 23:48:13.949577: step 62180, loss = 1.12 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:48:21.112542: step 62190, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 23:48:28.121111: step 62200, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:48:37.837206: step 62210, loss = 0.99 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 23:48:44.987412: step 62220, loss = 1.16 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 23:48:52.103277: step 62230, loss = 1.08 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 23:48:59.181216: step 62240, loss = 1.06 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 23:49:06.349781: step 62250, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 23:49:13.510142: step 62260, loss = 1.22 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 23:49:20.650603: step 62270, loss = 0.99 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 23:49:27.733077: step 62280, loss = 1.08 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:49:34.811513: step 62290, loss = 1.00 (47.0 examples/sec; 0.681 sec/batch)
2018-10-16 23:49:41.946850: step 62300, loss = 1.02 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 23:49:51.516063: step 62310, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:49:58.664691: step 62320, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-16 23:50:05.809589: step 62330, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 23:50:12.909319: step 62340, loss = 1.00 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 23:50:20.016087: step 62350, loss = 1.02 (45.1 examples/sec; 0.710 sec/batch)
2018-10-16 23:50:27.184851: step 62360, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 23:50:34.340817: step 62370, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 23:50:41.482955: step 62380, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:50:48.543296: step 62390, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 23:50:55.576185: step 62400, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-16 23:51:05.878043: step 62410, loss = 1.05 (48.4 examples/sec; 0.661 sec/batch)
2018-10-16 23:51:12.999073: step 62420, loss = 1.06 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:51:20.057426: step 62430, loss = 1.20 (45.6 examples/sec; 0.702 sec/batch)
2018-10-16 23:51:27.218317: step 62440, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:51:34.327916: step 62450, loss = 1.12 (45.3 examples/sec; 0.707 sec/batch)
2018-10-16 23:51:41.406985: step 62460, loss = 1.15 (43.5 examples/sec; 0.736 sec/batch)
2018-10-16 23:51:48.503917: step 62470, loss = 1.09 (43.1 examples/sec; 0.743 sec/batch)
2018-10-16 23:51:55.654606: step 62480, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:52:02.687608: step 62490, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 23:52:09.790564: step 62500, loss = 1.04 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 23:52:19.550530: step 62510, loss = 1.02 (46.4 examples/sec; 0.690 sec/batch)
2018-10-16 23:52:26.635118: step 62520, loss = 1.15 (44.9 examples/sec; 0.712 sec/batch)
2018-10-16 23:52:33.726790: step 62530, loss = 1.06 (44.8 examples/sec; 0.714 sec/batch)
2018-10-16 23:52:40.888295: step 62540, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 23:52:47.969692: step 62550, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 23:52:55.171676: step 62560, loss = 1.04 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 23:53:02.209539: step 62570, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-16 23:53:09.343359: step 62580, loss = 1.05 (43.8 examples/sec; 0.730 sec/batch)
2018-10-16 23:53:16.428064: step 62590, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-16 23:53:23.538328: step 62600, loss = 1.04 (46.3 examples/sec; 0.691 sec/batch)
2018-10-16 23:53:33.124086: step 62610, loss = 1.02 (47.3 examples/sec; 0.677 sec/batch)
2018-10-16 23:53:40.225592: step 62620, loss = 1.00 (44.7 examples/sec; 0.717 sec/batch)
2018-10-16 23:53:47.238905: step 62630, loss = 1.09 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 23:53:54.291028: step 62640, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-16 23:54:01.376987: step 62650, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 23:54:08.541256: step 62660, loss = 1.02 (44.5 examples/sec; 0.720 sec/batch)
2018-10-16 23:54:15.673114: step 62670, loss = 1.02 (43.2 examples/sec; 0.741 sec/batch)
2018-10-16 23:54:22.844162: step 62680, loss = 1.02 (42.9 examples/sec; 0.746 sec/batch)
2018-10-16 23:54:29.921166: step 62690, loss = 1.03 (46.1 examples/sec; 0.695 sec/batch)
2018-10-16 23:54:37.050203: step 62700, loss = 1.10 (44.6 examples/sec; 0.717 sec/batch)
2018-10-16 23:54:46.708611: step 62710, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-16 23:54:53.841963: step 62720, loss = 1.02 (44.4 examples/sec; 0.721 sec/batch)
2018-10-16 23:55:00.978934: step 62730, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:55:08.186681: step 62740, loss = 1.18 (46.5 examples/sec; 0.688 sec/batch)
2018-10-16 23:55:15.325232: step 62750, loss = 1.01 (46.7 examples/sec; 0.685 sec/batch)
2018-10-16 23:55:22.480823: step 62760, loss = 1.00 (43.2 examples/sec; 0.740 sec/batch)
2018-10-16 23:55:29.660889: step 62770, loss = 0.99 (42.5 examples/sec; 0.754 sec/batch)
2018-10-16 23:55:36.737334: step 62780, loss = 1.07 (44.1 examples/sec; 0.725 sec/batch)
2018-10-16 23:55:43.890372: step 62790, loss = 1.08 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 23:55:50.988072: step 62800, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 23:56:00.687728: step 62810, loss = 1.09 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 23:56:07.774581: step 62820, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-16 23:56:14.895258: step 62830, loss = 1.03 (44.3 examples/sec; 0.723 sec/batch)
2018-10-16 23:56:22.035927: step 62840, loss = 1.06 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 23:56:29.193434: step 62850, loss = 1.23 (43.6 examples/sec; 0.733 sec/batch)
2018-10-16 23:56:36.342034: step 62860, loss = 1.09 (43.6 examples/sec; 0.734 sec/batch)
2018-10-16 23:56:43.393491: step 62870, loss = 1.08 (45.7 examples/sec; 0.701 sec/batch)
2018-10-16 23:56:50.456029: step 62880, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-16 23:56:57.547152: step 62890, loss = 1.10 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 23:57:04.647830: step 62900, loss = 1.18 (46.1 examples/sec; 0.694 sec/batch)
2018-10-16 23:57:14.291070: step 62910, loss = 1.12 (46.5 examples/sec; 0.689 sec/batch)
2018-10-16 23:57:21.357892: step 62920, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 23:57:28.471588: step 62930, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 23:57:35.619101: step 62940, loss = 1.11 (45.6 examples/sec; 0.701 sec/batch)
2018-10-16 23:57:42.824617: step 62950, loss = 1.07 (44.9 examples/sec; 0.713 sec/batch)
2018-10-16 23:57:49.925744: step 62960, loss = 1.12 (44.3 examples/sec; 0.722 sec/batch)
2018-10-16 23:57:56.999601: step 62970, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-16 23:58:04.138175: step 62980, loss = 0.99 (46.8 examples/sec; 0.683 sec/batch)
2018-10-16 23:58:11.265721: step 62990, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-16 23:58:18.409644: step 63000, loss = 1.10 (44.7 examples/sec; 0.716 sec/batch)
2018-10-16 23:58:28.129452: step 63010, loss = 1.05 (45.4 examples/sec; 0.704 sec/batch)
2018-10-16 23:58:35.283258: step 63020, loss = 1.02 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 23:58:42.390001: step 63030, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 23:58:49.504875: step 63040, loss = 1.11 (45.0 examples/sec; 0.712 sec/batch)
2018-10-16 23:58:56.597947: step 63050, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 23:59:03.694534: step 63060, loss = 1.25 (44.8 examples/sec; 0.715 sec/batch)
2018-10-16 23:59:10.830400: step 63070, loss = 1.11 (45.8 examples/sec; 0.698 sec/batch)
2018-10-16 23:59:17.824832: step 63080, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-16 23:59:24.937775: step 63090, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-16 23:59:32.023951: step 63100, loss = 1.14 (46.0 examples/sec; 0.695 sec/batch)
2018-10-16 23:59:41.823209: step 63110, loss = 1.04 (45.2 examples/sec; 0.708 sec/batch)
2018-10-16 23:59:48.911206: step 63120, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-16 23:59:56.040933: step 63130, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 00:00:03.144333: step 63140, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 00:00:10.219620: step 63150, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 00:00:17.355130: step 63160, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 00:00:24.494962: step 63170, loss = 1.11 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 00:00:31.616972: step 63180, loss = 1.10 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:00:38.689507: step 63190, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 00:00:45.725060: step 63200, loss = 1.00 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 00:00:55.392502: step 63210, loss = 1.08 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 00:01:02.498932: step 63220, loss = 1.07 (43.6 examples/sec; 0.735 sec/batch)
2018-10-17 00:01:09.547526: step 63230, loss = 1.21 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:01:16.656961: step 63240, loss = 1.27 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 00:01:23.834886: step 63250, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:01:30.854912: step 63260, loss = 1.17 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:01:37.881832: step 63270, loss = 1.05 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 00:01:44.984719: step 63280, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 00:01:52.178155: step 63290, loss = 1.09 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 00:01:59.301115: step 63300, loss = 1.10 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 00:02:09.230761: step 63310, loss = 1.12 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 00:02:16.307928: step 63320, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 00:02:23.428149: step 63330, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 00:02:30.526131: step 63340, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 00:02:37.664867: step 63350, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 00:02:44.807880: step 63360, loss = 1.06 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 00:02:51.878281: step 63370, loss = 1.04 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:02:58.980074: step 63380, loss = 1.12 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 00:03:06.134922: step 63390, loss = 1.03 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 00:03:13.244295: step 63400, loss = 1.14 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 00:03:22.798888: step 63410, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:03:29.943246: step 63420, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 00:03:36.970277: step 63430, loss = 1.21 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:03:44.100555: step 63440, loss = 0.99 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 00:03:51.271639: step 63450, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 00:03:58.341518: step 63460, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 00:04:05.520693: step 63470, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 00:04:12.629926: step 63480, loss = 1.05 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 00:04:19.782914: step 63490, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 00:04:26.819847: step 63500, loss = 1.11 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 00:04:36.439945: step 63510, loss = 1.18 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 00:04:43.573892: step 63520, loss = 1.26 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 00:04:50.624415: step 63530, loss = 1.09 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 00:04:57.773202: step 63540, loss = 1.04 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 00:05:04.795602: step 63550, loss = 1.05 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 00:05:11.895824: step 63560, loss = 0.99 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 00:05:18.959163: step 63570, loss = 1.00 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 00:05:26.117900: step 63580, loss = 1.11 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 00:05:33.165610: step 63590, loss = 1.01 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 00:05:40.227379: step 63600, loss = 1.10 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 00:05:50.098586: step 63610, loss = 0.99 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 00:05:57.180012: step 63620, loss = 1.01 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 00:06:04.252255: step 63630, loss = 1.12 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 00:06:11.324384: step 63640, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 00:06:18.490027: step 63650, loss = 0.99 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 00:06:25.639868: step 63660, loss = 1.21 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 00:06:32.768691: step 63670, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 00:06:39.872827: step 63680, loss = 1.06 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:06:46.952243: step 63690, loss = 1.03 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 00:06:54.050395: step 63700, loss = 1.05 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 00:07:03.671848: step 63710, loss = 1.13 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 00:07:10.848402: step 63720, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 00:07:17.795410: step 63730, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 00:07:24.993680: step 63740, loss = 1.12 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 00:07:32.112135: step 63750, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 00:07:39.283763: step 63760, loss = 1.01 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 00:07:46.350997: step 63770, loss = 1.05 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 00:07:53.484103: step 63780, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 00:08:00.588614: step 63790, loss = 1.04 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 00:08:07.713338: step 63800, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 00:08:17.485166: step 63810, loss = 1.12 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 00:08:24.569337: step 63820, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 00:08:31.718349: step 63830, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 00:08:38.835697: step 63840, loss = 1.06 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 00:08:45.967025: step 63850, loss = 1.15 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:08:53.097072: step 63860, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 00:09:00.226861: step 63870, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 00:09:07.326404: step 63880, loss = 1.00 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 00:09:14.469895: step 63890, loss = 1.10 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 00:09:21.555641: step 63900, loss = 1.07 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 00:09:31.488973: step 63910, loss = 1.13 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 00:09:38.575836: step 63920, loss = 1.03 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 00:09:45.728802: step 63930, loss = 1.05 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 00:09:52.856764: step 63940, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 00:10:00.017982: step 63950, loss = 1.03 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 00:10:07.079161: step 63960, loss = 1.06 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 00:10:14.216437: step 63970, loss = 1.05 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 00:10:21.263583: step 63980, loss = 1.14 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 00:10:28.377133: step 63990, loss = 1.10 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 00:10:35.429212: step 64000, loss = 1.26 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 00:10:45.154140: step 64010, loss = 1.10 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:10:52.251195: step 64020, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 00:10:59.381622: step 64030, loss = 1.02 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 00:11:06.407416: step 64040, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 00:11:13.491008: step 64050, loss = 1.26 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 00:11:20.570973: step 64060, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 00:11:27.684483: step 64070, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 00:11:34.722914: step 64080, loss = 1.03 (47.5 examples/sec; 0.673 sec/batch)
2018-10-17 00:11:41.858256: step 64090, loss = 1.02 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 00:11:48.984037: step 64100, loss = 1.00 (46.3 examples/sec; 0.690 sec/batch)
2018-10-17 00:11:58.704750: step 64110, loss = 1.08 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 00:12:05.828761: step 64120, loss = 1.18 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 00:12:12.883253: step 64130, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 00:12:19.998947: step 64140, loss = 1.02 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 00:12:27.115386: step 64150, loss = 1.14 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 00:12:34.277780: step 64160, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 00:12:41.308695: step 64170, loss = 1.09 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 00:12:48.429231: step 64180, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 00:12:55.516226: step 64190, loss = 1.01 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 00:13:02.521360: step 64200, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 00:13:12.132760: step 64210, loss = 1.04 (47.4 examples/sec; 0.676 sec/batch)
2018-10-17 00:13:19.241999: step 64220, loss = 1.11 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:13:26.268467: step 64230, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 00:13:33.404231: step 64240, loss = 1.03 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 00:13:40.445518: step 64250, loss = 1.13 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 00:13:47.575831: step 64260, loss = 1.09 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 00:13:54.619889: step 64270, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 00:14:01.678271: step 64280, loss = 1.04 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 00:14:08.873057: step 64290, loss = 1.09 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 00:14:15.947183: step 64300, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:14:25.570889: step 64310, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 00:14:32.663502: step 64320, loss = 1.10 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 00:14:39.771885: step 64330, loss = 1.20 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 00:14:46.857842: step 64340, loss = 1.04 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:14:53.914161: step 64350, loss = 1.12 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:15:00.941287: step 64360, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 00:15:08.018360: step 64370, loss = 1.04 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 00:15:15.075374: step 64380, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:15:22.184729: step 64390, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:15:29.323583: step 64400, loss = 1.06 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 00:15:38.994175: step 64410, loss = 1.08 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 00:15:46.087803: step 64420, loss = 1.00 (45.9 examples/sec; 0.696 sec/batch)
2018-10-17 00:15:53.130931: step 64430, loss = 1.07 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 00:16:00.213966: step 64440, loss = 1.05 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:16:07.326578: step 64450, loss = 1.09 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 00:16:14.429972: step 64460, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:16:21.479113: step 64470, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 00:16:28.572906: step 64480, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 00:16:35.652069: step 64490, loss = 0.99 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 00:16:42.704630: step 64500, loss = 1.04 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 00:16:52.355418: step 64510, loss = 1.09 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 00:16:59.379019: step 64520, loss = 1.04 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 00:17:06.440321: step 64530, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:17:13.592176: step 64540, loss = 1.04 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 00:17:20.668202: step 64550, loss = 1.06 (47.3 examples/sec; 0.676 sec/batch)
2018-10-17 00:17:27.783031: step 64560, loss = 1.08 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 00:17:34.927236: step 64570, loss = 1.14 (42.8 examples/sec; 0.749 sec/batch)
2018-10-17 00:17:41.983419: step 64580, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 00:17:49.051309: step 64590, loss = 1.02 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 00:17:56.123514: step 64600, loss = 1.02 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 00:18:05.734993: step 64610, loss = 1.03 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 00:18:12.845881: step 64620, loss = 1.00 (43.6 examples/sec; 0.735 sec/batch)
2018-10-17 00:18:19.929933: step 64630, loss = 1.02 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 00:18:27.014900: step 64640, loss = 1.18 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 00:18:34.177026: step 64650, loss = 1.11 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 00:18:41.238684: step 64660, loss = 1.10 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 00:18:48.324874: step 64670, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:18:55.441681: step 64680, loss = 1.08 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:19:02.591968: step 64690, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 00:19:09.695165: step 64700, loss = 0.99 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 00:19:19.489326: step 64710, loss = 1.08 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 00:19:26.654506: step 64720, loss = 1.08 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 00:19:33.717173: step 64730, loss = 1.05 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 00:19:40.878768: step 64740, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:19:48.021164: step 64750, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 00:19:55.122265: step 64760, loss = 1.00 (43.3 examples/sec; 0.740 sec/batch)
2018-10-17 00:20:02.220910: step 64770, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 00:20:09.296541: step 64780, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 00:20:16.395753: step 64790, loss = 1.08 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 00:20:23.554608: step 64800, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:20:33.213358: step 64810, loss = 1.04 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 00:20:40.274927: step 64820, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 00:20:47.384829: step 64830, loss = 1.13 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 00:20:54.559871: step 64840, loss = 1.01 (42.5 examples/sec; 0.753 sec/batch)
2018-10-17 00:21:01.666958: step 64850, loss = 1.07 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 00:21:08.675268: step 64860, loss = 1.06 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 00:21:15.757960: step 64870, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 00:21:22.880291: step 64880, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 00:21:30.075147: step 64890, loss = 1.08 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 00:21:37.101774: step 64900, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 00:21:46.721737: step 64910, loss = 1.00 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 00:21:53.741875: step 64920, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 00:22:00.930138: step 64930, loss = 1.22 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 00:22:07.988753: step 64940, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 00:22:15.059543: step 64950, loss = 1.09 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 00:22:22.251892: step 64960, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 00:22:29.287085: step 64970, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 00:22:36.362819: step 64980, loss = 1.07 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 00:22:43.467455: step 64990, loss = 1.15 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 00:22:50.571228: step 65000, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 00:23:03.148855: step 65010, loss = 1.10 (47.5 examples/sec; 0.674 sec/batch)
2018-10-17 00:23:09.951442: step 65020, loss = 1.15 (48.7 examples/sec; 0.658 sec/batch)
2018-10-17 00:23:17.018294: step 65030, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 00:23:24.160937: step 65040, loss = 1.00 (46.1 examples/sec; 0.693 sec/batch)
2018-10-17 00:23:31.206677: step 65050, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 00:23:38.344874: step 65060, loss = 1.04 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 00:23:45.410516: step 65070, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 00:23:52.527047: step 65080, loss = 1.10 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 00:23:59.681441: step 65090, loss = 1.12 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 00:24:06.743152: step 65100, loss = 1.07 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 00:24:16.456944: step 65110, loss = 1.07 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 00:24:23.461754: step 65120, loss = 1.02 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 00:24:30.536910: step 65130, loss = 1.05 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 00:24:37.584923: step 65140, loss = 1.05 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:24:44.775520: step 65150, loss = 1.03 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 00:24:51.881858: step 65160, loss = 1.02 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 00:24:58.940182: step 65170, loss = 1.14 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 00:25:06.066115: step 65180, loss = 1.05 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 00:25:13.141499: step 65190, loss = 1.18 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 00:25:20.246433: step 65200, loss = 1.16 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 00:25:29.886269: step 65210, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 00:25:36.956390: step 65220, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 00:25:43.999461: step 65230, loss = 1.26 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 00:25:51.074601: step 65240, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:25:58.100793: step 65250, loss = 1.07 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:26:05.249528: step 65260, loss = 1.14 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 00:26:12.337357: step 65270, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 00:26:19.510185: step 65280, loss = 1.05 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:26:26.627397: step 65290, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 00:26:33.702108: step 65300, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 00:26:43.292464: step 65310, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 00:26:50.384674: step 65320, loss = 1.19 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 00:26:57.478998: step 65330, loss = 0.99 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 00:27:04.579245: step 65340, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 00:27:11.625705: step 65350, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:27:18.704821: step 65360, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 00:27:25.854466: step 65370, loss = 1.04 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 00:27:32.988291: step 65380, loss = 1.10 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 00:27:40.053232: step 65390, loss = 1.11 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 00:27:47.223665: step 65400, loss = 1.06 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 00:27:56.901187: step 65410, loss = 1.15 (46.3 examples/sec; 0.690 sec/batch)
2018-10-17 00:28:03.961026: step 65420, loss = 1.00 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 00:28:11.125951: step 65430, loss = 1.05 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 00:28:18.256211: step 65440, loss = 1.13 (45.9 examples/sec; 0.696 sec/batch)
2018-10-17 00:28:25.407941: step 65450, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 00:28:32.546955: step 65460, loss = 1.02 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 00:28:39.642353: step 65470, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 00:28:46.699673: step 65480, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 00:28:53.773556: step 65490, loss = 1.04 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 00:29:00.884826: step 65500, loss = 1.11 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 00:29:10.448558: step 65510, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 00:29:17.535388: step 65520, loss = 1.07 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 00:29:24.527623: step 65530, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 00:29:31.609550: step 65540, loss = 1.05 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 00:29:38.685943: step 65550, loss = 1.02 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 00:29:45.799875: step 65560, loss = 1.05 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 00:29:52.929434: step 65570, loss = 1.01 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 00:30:00.046426: step 65580, loss = 1.10 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 00:30:07.092846: step 65590, loss = 1.05 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:30:14.212737: step 65600, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 00:30:23.907098: step 65610, loss = 1.04 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 00:30:30.937938: step 65620, loss = 1.08 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:30:38.028577: step 65630, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:30:45.137163: step 65640, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:30:52.285995: step 65650, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 00:30:59.329078: step 65660, loss = 1.06 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 00:31:06.380160: step 65670, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:31:13.529467: step 65680, loss = 1.02 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 00:31:20.647504: step 65690, loss = 1.07 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 00:31:27.717349: step 65700, loss = 1.15 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 00:31:37.325161: step 65710, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 00:31:44.335309: step 65720, loss = 1.06 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 00:31:51.363129: step 65730, loss = 1.00 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 00:31:58.520744: step 65740, loss = 1.12 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 00:32:05.574583: step 65750, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 00:32:12.727770: step 65760, loss = 1.02 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 00:32:19.821012: step 65770, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 00:32:26.911118: step 65780, loss = 1.18 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 00:32:34.024942: step 65790, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 00:32:41.202132: step 65800, loss = 1.05 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 00:32:51.272638: step 65810, loss = 1.12 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:32:58.383345: step 65820, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:33:05.486045: step 65830, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 00:33:12.607413: step 65840, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:33:19.748412: step 65850, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 00:33:26.840486: step 65860, loss = 1.11 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 00:33:33.928505: step 65870, loss = 1.16 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 00:33:41.010039: step 65880, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 00:33:48.123833: step 65890, loss = 1.10 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 00:33:55.189499: step 65900, loss = 1.00 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 00:34:04.692002: step 65910, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 00:34:11.825838: step 65920, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 00:34:18.897659: step 65930, loss = 0.99 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 00:34:26.033798: step 65940, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 00:34:33.126969: step 65950, loss = 1.03 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:34:40.218343: step 65960, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 00:34:47.280879: step 65970, loss = 1.15 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 00:34:54.351486: step 65980, loss = 1.02 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 00:35:01.569932: step 65990, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 00:35:08.679898: step 66000, loss = 1.00 (43.3 examples/sec; 0.738 sec/batch)
2018-10-17 00:35:18.346771: step 66010, loss = 1.10 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 00:35:25.437732: step 66020, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 00:35:32.509114: step 66030, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:35:39.642344: step 66040, loss = 1.19 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 00:35:46.697256: step 66050, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 00:35:53.802476: step 66060, loss = 1.02 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 00:36:00.906992: step 66070, loss = 1.11 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 00:36:07.944947: step 66080, loss = 1.09 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 00:36:15.051919: step 66090, loss = 1.02 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 00:36:22.052769: step 66100, loss = 1.00 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 00:36:31.704312: step 66110, loss = 1.10 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 00:36:38.763901: step 66120, loss = 1.00 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 00:36:45.859300: step 66130, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:36:52.944624: step 66140, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:37:00.033317: step 66150, loss = 1.09 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:37:07.162450: step 66160, loss = 1.02 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 00:37:14.313441: step 66170, loss = 1.11 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 00:37:21.500364: step 66180, loss = 1.01 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 00:37:28.673500: step 66190, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 00:37:35.715929: step 66200, loss = 1.04 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 00:37:45.349103: step 66210, loss = 1.07 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 00:37:52.449245: step 66220, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 00:37:59.455575: step 66230, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:38:06.502009: step 66240, loss = 1.10 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:38:13.476698: step 66250, loss = 1.12 (47.2 examples/sec; 0.679 sec/batch)
2018-10-17 00:38:20.579332: step 66260, loss = 1.10 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:38:27.630439: step 66270, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:38:34.724353: step 66280, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 00:38:41.823043: step 66290, loss = 1.00 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 00:38:48.979821: step 66300, loss = 1.01 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 00:38:58.546767: step 66310, loss = 1.14 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 00:39:05.641387: step 66320, loss = 1.02 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 00:39:12.787653: step 66330, loss = 1.00 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 00:39:19.837768: step 66340, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:39:26.882848: step 66350, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 00:39:34.017454: step 66360, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 00:39:41.121455: step 66370, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 00:39:48.213492: step 66380, loss = 1.02 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 00:39:55.265326: step 66390, loss = 1.19 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 00:40:02.369064: step 66400, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 00:40:12.283015: step 66410, loss = 1.04 (47.9 examples/sec; 0.669 sec/batch)
2018-10-17 00:40:19.342839: step 66420, loss = 1.11 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 00:40:26.387525: step 66430, loss = 1.05 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 00:40:33.565889: step 66440, loss = 1.14 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 00:40:40.614419: step 66450, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 00:40:47.767825: step 66460, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 00:40:54.902040: step 66470, loss = 1.07 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 00:41:01.969301: step 66480, loss = 1.01 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 00:41:09.079163: step 66490, loss = 1.17 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 00:41:16.200475: step 66500, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 00:41:25.944720: step 66510, loss = 1.04 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 00:41:33.073886: step 66520, loss = 1.17 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 00:41:40.198203: step 66530, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 00:41:47.252803: step 66540, loss = 1.09 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 00:41:54.417863: step 66550, loss = 1.00 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 00:42:01.483093: step 66560, loss = 1.08 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 00:42:08.530721: step 66570, loss = 1.13 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 00:42:15.707310: step 66580, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:42:22.860392: step 66590, loss = 1.11 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:42:29.952180: step 66600, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 00:42:39.620565: step 66610, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:42:46.698927: step 66620, loss = 1.12 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 00:42:53.756826: step 66630, loss = 1.06 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:43:00.768615: step 66640, loss = 1.09 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 00:43:07.781630: step 66650, loss = 1.05 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:43:14.904137: step 66660, loss = 1.14 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 00:43:21.993713: step 66670, loss = 1.15 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 00:43:29.044797: step 66680, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 00:43:36.136552: step 66690, loss = 1.17 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 00:43:43.253367: step 66700, loss = 0.99 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 00:43:53.038264: step 66710, loss = 1.05 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 00:44:00.133767: step 66720, loss = 1.08 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 00:44:07.241577: step 66730, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 00:44:14.330204: step 66740, loss = 1.04 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 00:44:21.376924: step 66750, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 00:44:28.536908: step 66760, loss = 0.99 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 00:44:35.576788: step 66770, loss = 0.99 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 00:44:42.631334: step 66780, loss = 1.08 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 00:44:49.727534: step 66790, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 00:44:56.862890: step 66800, loss = 1.08 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 00:45:06.810364: step 66810, loss = 1.03 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 00:45:13.867041: step 66820, loss = 1.05 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 00:45:20.973795: step 66830, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 00:45:28.086448: step 66840, loss = 1.14 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 00:45:35.189907: step 66850, loss = 1.15 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 00:45:42.274832: step 66860, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 00:45:49.433347: step 66870, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 00:45:56.510499: step 66880, loss = 1.17 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 00:46:03.663491: step 66890, loss = 1.13 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 00:46:10.768311: step 66900, loss = 1.03 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 00:46:20.375255: step 66910, loss = 1.04 (47.7 examples/sec; 0.670 sec/batch)
2018-10-17 00:46:27.369415: step 66920, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 00:46:34.490667: step 66930, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 00:46:41.603942: step 66940, loss = 1.02 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 00:46:48.710832: step 66950, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:46:55.811048: step 66960, loss = 1.08 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 00:47:02.889683: step 66970, loss = 1.21 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:47:10.016630: step 66980, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 00:47:17.058619: step 66990, loss = 1.17 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 00:47:24.205368: step 67000, loss = 1.06 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:47:33.808305: step 67010, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 00:47:40.845786: step 67020, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 00:47:47.951693: step 67030, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 00:47:55.079622: step 67040, loss = 1.14 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 00:48:02.162265: step 67050, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:48:09.207453: step 67060, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 00:48:16.264007: step 67070, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:48:23.411761: step 67080, loss = 1.06 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 00:48:30.522744: step 67090, loss = 0.99 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 00:48:37.620987: step 67100, loss = 1.18 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 00:48:47.251589: step 67110, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 00:48:54.257589: step 67120, loss = 1.13 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 00:49:01.297872: step 67130, loss = 0.99 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 00:49:08.366790: step 67140, loss = 1.03 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 00:49:15.472262: step 67150, loss = 1.07 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 00:49:22.559052: step 67160, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 00:49:29.671876: step 67170, loss = 1.10 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 00:49:36.745750: step 67180, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 00:49:43.989686: step 67190, loss = 1.17 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 00:49:51.082538: step 67200, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 00:50:01.060218: step 67210, loss = 0.99 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 00:50:08.113044: step 67220, loss = 1.08 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 00:50:15.213129: step 67230, loss = 1.05 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 00:50:22.371137: step 67240, loss = 1.03 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 00:50:29.441951: step 67250, loss = 1.15 (44.7 examples/sec; 0.717 sec/batch)
2018-10-17 00:50:36.458893: step 67260, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 00:50:43.649913: step 67270, loss = 1.04 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 00:50:50.763410: step 67280, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:50:57.856931: step 67290, loss = 1.04 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:51:04.883023: step 67300, loss = 1.02 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 00:51:14.474215: step 67310, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:51:21.620246: step 67320, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 00:51:28.625323: step 67330, loss = 1.13 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:51:35.723243: step 67340, loss = 0.99 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 00:51:42.814932: step 67350, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 00:51:49.895846: step 67360, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 00:51:57.033933: step 67370, loss = 1.18 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 00:52:04.128986: step 67380, loss = 1.01 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 00:52:11.212321: step 67390, loss = 1.38 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 00:52:18.342534: step 67400, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 00:52:27.915165: step 67410, loss = 1.04 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 00:52:34.917019: step 67420, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:52:41.957731: step 67430, loss = 1.03 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 00:52:49.010023: step 67440, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 00:52:56.072759: step 67450, loss = 1.13 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:53:03.227731: step 67460, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 00:53:10.379367: step 67470, loss = 1.01 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 00:53:17.492136: step 67480, loss = 0.99 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 00:53:24.577857: step 67490, loss = 1.22 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 00:53:31.687272: step 67500, loss = 1.12 (42.9 examples/sec; 0.745 sec/batch)
2018-10-17 00:53:41.286175: step 67510, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 00:53:48.337744: step 67520, loss = 1.01 (44.7 examples/sec; 0.717 sec/batch)
2018-10-17 00:53:55.440485: step 67530, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:54:02.505508: step 67540, loss = 1.08 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:54:09.627262: step 67550, loss = 1.06 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:54:16.720950: step 67560, loss = 1.02 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 00:54:23.779756: step 67570, loss = 1.05 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 00:54:30.876442: step 67580, loss = 1.13 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:54:37.911618: step 67590, loss = 1.08 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 00:54:45.048083: step 67600, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:54:54.718648: step 67610, loss = 1.00 (46.5 examples/sec; 0.687 sec/batch)
2018-10-17 00:55:01.767759: step 67620, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 00:55:08.881034: step 67630, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 00:55:16.031641: step 67640, loss = 1.01 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 00:55:23.062729: step 67650, loss = 1.04 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:55:30.186450: step 67660, loss = 1.04 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 00:55:37.327785: step 67670, loss = 1.04 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 00:55:44.392541: step 67680, loss = 1.19 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:55:51.475462: step 67690, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 00:55:58.487627: step 67700, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 00:56:08.268587: step 67710, loss = 1.00 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 00:56:15.327961: step 67720, loss = 1.01 (45.9 examples/sec; 0.696 sec/batch)
2018-10-17 00:56:22.464917: step 67730, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 00:56:29.555944: step 67740, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 00:56:36.604169: step 67750, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 00:56:43.700539: step 67760, loss = 1.06 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:56:50.774976: step 67770, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 00:56:57.829549: step 67780, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 00:57:04.940866: step 67790, loss = 1.10 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 00:57:12.051220: step 67800, loss = 1.00 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 00:57:21.695988: step 67810, loss = 1.00 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 00:57:28.717374: step 67820, loss = 1.25 (47.5 examples/sec; 0.674 sec/batch)
2018-10-17 00:57:35.822957: step 67830, loss = 1.18 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 00:57:42.954118: step 67840, loss = 0.99 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 00:57:50.017332: step 67850, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 00:57:57.052990: step 67860, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 00:58:04.222653: step 67870, loss = 1.09 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 00:58:11.324463: step 67880, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 00:58:18.390581: step 67890, loss = 1.16 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:58:25.468345: step 67900, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:58:35.387625: step 67910, loss = 1.09 (47.6 examples/sec; 0.672 sec/batch)
2018-10-17 00:58:42.570432: step 67920, loss = 1.33 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 00:58:49.718778: step 67930, loss = 1.01 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 00:58:56.820627: step 67940, loss = 1.08 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 00:59:03.868966: step 67950, loss = 1.02 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 00:59:10.904610: step 67960, loss = 1.00 (46.3 examples/sec; 0.690 sec/batch)
2018-10-17 00:59:18.040048: step 67970, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 00:59:25.170082: step 67980, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 00:59:32.303110: step 67990, loss = 1.00 (44.5 examples/sec; 0.718 sec/batch)
2018-10-17 00:59:39.396716: step 68000, loss = 1.31 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 00:59:49.015002: step 68010, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 00:59:56.109847: step 68020, loss = 1.00 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 01:00:03.204689: step 68030, loss = 1.11 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 01:00:10.334672: step 68040, loss = 1.20 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 01:00:17.438877: step 68050, loss = 0.99 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 01:00:24.527207: step 68060, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 01:00:31.614710: step 68070, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 01:00:38.712402: step 68080, loss = 1.06 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 01:00:45.835254: step 68090, loss = 1.07 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 01:00:52.949514: step 68100, loss = 0.99 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 01:01:02.816960: step 68110, loss = 1.26 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:01:09.874275: step 68120, loss = 1.11 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 01:01:17.007508: step 68130, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 01:01:24.004069: step 68140, loss = 1.06 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 01:01:31.134978: step 68150, loss = 1.04 (43.3 examples/sec; 0.740 sec/batch)
2018-10-17 01:01:38.152184: step 68160, loss = 1.16 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 01:01:45.286208: step 68170, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:01:52.380216: step 68180, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 01:01:59.547206: step 68190, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 01:02:06.675305: step 68200, loss = 1.04 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 01:02:16.265667: step 68210, loss = 1.20 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 01:02:23.334913: step 68220, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:02:30.376318: step 68230, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 01:02:37.451813: step 68240, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 01:02:44.540338: step 68250, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 01:02:51.650919: step 68260, loss = 1.02 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 01:02:58.770561: step 68270, loss = 1.07 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 01:03:05.885705: step 68280, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:03:12.934983: step 68290, loss = 1.16 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:03:20.051236: step 68300, loss = 1.06 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 01:03:30.355019: step 68310, loss = 1.00 (47.5 examples/sec; 0.673 sec/batch)
2018-10-17 01:03:37.411510: step 68320, loss = 1.03 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 01:03:44.514064: step 68330, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:03:51.591385: step 68340, loss = 1.05 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 01:03:58.757735: step 68350, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 01:04:05.846273: step 68360, loss = 1.00 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 01:04:12.897331: step 68370, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 01:04:19.982200: step 68380, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 01:04:27.050106: step 68390, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 01:04:34.112863: step 68400, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:04:44.064991: step 68410, loss = 1.01 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 01:04:51.124464: step 68420, loss = 1.06 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 01:04:58.246819: step 68430, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 01:05:05.339072: step 68440, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 01:05:12.389525: step 68450, loss = 1.04 (47.6 examples/sec; 0.672 sec/batch)
2018-10-17 01:05:19.496027: step 68460, loss = 1.04 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 01:05:26.593791: step 68470, loss = 1.01 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 01:05:33.642886: step 68480, loss = 1.03 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 01:05:40.762773: step 68490, loss = 1.05 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 01:05:47.773007: step 68500, loss = 1.04 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 01:05:57.434495: step 68510, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 01:06:04.486370: step 68520, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:06:11.563338: step 68530, loss = 1.03 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 01:06:18.649367: step 68540, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:06:25.693248: step 68550, loss = 1.09 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 01:06:32.866811: step 68560, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 01:06:39.962152: step 68570, loss = 1.11 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 01:06:47.083755: step 68580, loss = 1.01 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 01:06:54.187234: step 68590, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 01:07:01.294586: step 68600, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 01:07:10.907647: step 68610, loss = 1.06 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 01:07:17.994120: step 68620, loss = 1.10 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 01:07:25.022919: step 68630, loss = 1.33 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 01:07:32.124661: step 68640, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:07:39.119548: step 68650, loss = 1.03 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 01:07:46.280710: step 68660, loss = 1.10 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 01:07:53.384670: step 68670, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 01:08:00.543985: step 68680, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:08:07.637029: step 68690, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 01:08:14.693845: step 68700, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 01:08:24.676447: step 68710, loss = 1.05 (47.5 examples/sec; 0.673 sec/batch)
2018-10-17 01:08:31.712799: step 68720, loss = 1.03 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 01:08:38.798742: step 68730, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:08:45.891669: step 68740, loss = 0.99 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 01:08:52.899064: step 68750, loss = 1.05 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 01:08:59.927814: step 68760, loss = 1.10 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 01:09:07.034749: step 68770, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 01:09:14.068271: step 68780, loss = 1.15 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 01:09:21.141406: step 68790, loss = 1.02 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 01:09:28.168838: step 68800, loss = 1.07 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 01:09:38.070619: step 68810, loss = 1.00 (48.0 examples/sec; 0.667 sec/batch)
2018-10-17 01:09:45.179731: step 68820, loss = 1.03 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 01:09:52.287727: step 68830, loss = 0.99 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 01:09:59.316057: step 68840, loss = 1.02 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 01:10:06.331427: step 68850, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 01:10:13.402104: step 68860, loss = 1.15 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 01:10:20.485142: step 68870, loss = 1.04 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 01:10:27.639054: step 68880, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 01:10:34.678973: step 68890, loss = 0.99 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 01:10:41.751771: step 68900, loss = 1.08 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 01:10:51.408727: step 68910, loss = 1.02 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 01:10:58.490761: step 68920, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 01:11:05.603792: step 68930, loss = 1.03 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 01:11:12.704733: step 68940, loss = 0.99 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 01:11:19.823770: step 68950, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:11:26.992053: step 68960, loss = 0.99 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 01:11:34.015700: step 68970, loss = 1.04 (46.8 examples/sec; 0.683 sec/batch)
2018-10-17 01:11:41.192346: step 68980, loss = 0.99 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 01:11:48.286475: step 68990, loss = 1.05 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 01:11:55.415072: step 69000, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:12:05.096487: step 69010, loss = 1.02 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 01:12:12.171779: step 69020, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:12:19.350383: step 69030, loss = 1.15 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 01:12:26.556386: step 69040, loss = 1.03 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 01:12:33.571720: step 69050, loss = 0.99 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 01:12:40.688865: step 69060, loss = 1.10 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 01:12:47.814567: step 69070, loss = 1.13 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 01:12:54.883692: step 69080, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 01:13:01.941252: step 69090, loss = 1.03 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 01:13:09.013738: step 69100, loss = 1.10 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 01:13:19.655411: step 69110, loss = 0.99 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 01:13:26.626822: step 69120, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 01:13:33.836673: step 69130, loss = 1.06 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 01:13:40.891556: step 69140, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:13:47.976211: step 69150, loss = 1.06 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:13:55.097475: step 69160, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 01:14:02.204612: step 69170, loss = 0.99 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 01:14:09.331316: step 69180, loss = 1.29 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 01:14:16.399847: step 69190, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 01:14:23.465143: step 69200, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 01:14:33.298173: step 69210, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:14:40.393149: step 69220, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 01:14:47.453855: step 69230, loss = 1.17 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:14:54.580180: step 69240, loss = 0.99 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 01:15:01.622236: step 69250, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:15:08.745929: step 69260, loss = 1.19 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 01:15:15.815044: step 69270, loss = 1.19 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 01:15:22.956722: step 69280, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 01:15:30.069816: step 69290, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:15:37.111291: step 69300, loss = 1.05 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 01:15:47.106979: step 69310, loss = 1.05 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 01:15:54.220749: step 69320, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 01:16:01.301214: step 69330, loss = 1.03 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 01:16:08.384145: step 69340, loss = 1.29 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 01:16:15.549763: step 69350, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:16:22.759575: step 69360, loss = 1.29 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 01:16:29.881362: step 69370, loss = 1.09 (42.9 examples/sec; 0.745 sec/batch)
2018-10-17 01:16:36.897695: step 69380, loss = 0.99 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 01:16:43.903344: step 69390, loss = 1.04 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 01:16:51.061598: step 69400, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:17:00.669076: step 69410, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:17:07.734028: step 69420, loss = 1.08 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 01:17:14.815606: step 69430, loss = 1.06 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 01:17:21.923284: step 69440, loss = 1.11 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 01:17:29.031815: step 69450, loss = 0.99 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 01:17:36.129608: step 69460, loss = 1.06 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 01:17:43.185110: step 69470, loss = 1.10 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 01:17:50.275710: step 69480, loss = 1.12 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:17:57.377426: step 69490, loss = 1.03 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 01:18:04.436213: step 69500, loss = 1.02 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 01:18:14.144485: step 69510, loss = 1.19 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 01:18:21.265589: step 69520, loss = 1.02 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 01:18:28.344370: step 69530, loss = 1.07 (44.4 examples/sec; 0.722 sec/batch)
2018-10-17 01:18:35.393819: step 69540, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 01:18:42.472553: step 69550, loss = 1.05 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 01:18:49.611390: step 69560, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:18:56.685096: step 69570, loss = 1.10 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:19:03.826390: step 69580, loss = 1.06 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 01:19:10.898994: step 69590, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 01:19:18.016753: step 69600, loss = 1.09 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 01:19:27.666272: step 69610, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:19:34.750461: step 69620, loss = 1.13 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 01:19:41.834568: step 69630, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 01:19:48.945852: step 69640, loss = 1.12 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 01:19:56.068496: step 69650, loss = 1.03 (43.0 examples/sec; 0.744 sec/batch)
2018-10-17 01:20:03.071193: step 69660, loss = 1.13 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 01:20:10.108795: step 69670, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 01:20:17.207466: step 69680, loss = 1.11 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:20:24.289507: step 69690, loss = 0.99 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 01:20:31.333128: step 69700, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:20:40.973574: step 69710, loss = 1.08 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 01:20:48.088323: step 69720, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 01:20:55.101318: step 69730, loss = 1.11 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 01:21:02.187871: step 69740, loss = 1.14 (42.8 examples/sec; 0.749 sec/batch)
2018-10-17 01:21:09.262664: step 69750, loss = 1.13 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 01:21:16.346395: step 69760, loss = 1.12 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 01:21:23.405360: step 69770, loss = 1.01 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 01:21:30.460040: step 69780, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 01:21:37.576252: step 69790, loss = 1.13 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:21:44.672161: step 69800, loss = 1.03 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 01:21:54.646856: step 69810, loss = 0.99 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 01:22:01.733506: step 69820, loss = 1.04 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 01:22:08.810758: step 69830, loss = 1.08 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 01:22:15.920851: step 69840, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 01:22:23.040826: step 69850, loss = 1.16 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 01:22:30.143182: step 69860, loss = 1.12 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 01:22:37.226221: step 69870, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 01:22:44.375596: step 69880, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:22:51.374846: step 69890, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 01:22:58.450647: step 69900, loss = 1.08 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 01:23:08.033794: step 69910, loss = 1.05 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 01:23:15.111607: step 69920, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:23:22.294809: step 69930, loss = 1.07 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 01:23:29.313669: step 69940, loss = 1.02 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 01:23:36.437493: step 69950, loss = 1.04 (47.6 examples/sec; 0.672 sec/batch)
2018-10-17 01:23:43.553985: step 69960, loss = 0.99 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 01:23:50.663452: step 69970, loss = 1.14 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 01:23:57.833770: step 69980, loss = 1.28 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 01:24:04.888800: step 69990, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 01:24:11.916953: step 70000, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 01:24:25.186182: step 70010, loss = 0.99 (47.5 examples/sec; 0.673 sec/batch)
2018-10-17 01:24:32.082660: step 70020, loss = 1.24 (47.6 examples/sec; 0.672 sec/batch)
2018-10-17 01:24:39.119697: step 70030, loss = 1.10 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:24:46.297850: step 70040, loss = 1.01 (43.0 examples/sec; 0.743 sec/batch)
2018-10-17 01:24:53.453286: step 70050, loss = 1.03 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 01:25:00.556341: step 70060, loss = 1.04 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:25:07.582138: step 70070, loss = 1.00 (47.2 examples/sec; 0.677 sec/batch)
2018-10-17 01:25:14.713818: step 70080, loss = 1.00 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 01:25:21.831060: step 70090, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:25:28.952379: step 70100, loss = 1.12 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 01:25:38.636776: step 70110, loss = 1.06 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 01:25:45.813502: step 70120, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:25:52.888720: step 70130, loss = 1.15 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 01:25:59.979587: step 70140, loss = 1.18 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 01:26:07.158553: step 70150, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 01:26:14.226188: step 70160, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 01:26:21.412265: step 70170, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:26:28.506456: step 70180, loss = 1.12 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 01:26:35.521424: step 70190, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:26:42.684666: step 70200, loss = 1.05 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 01:26:52.320207: step 70210, loss = 1.09 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 01:26:59.452966: step 70220, loss = 1.09 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 01:27:06.525824: step 70230, loss = 1.04 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 01:27:13.664157: step 70240, loss = 1.10 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 01:27:20.789152: step 70250, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 01:27:27.899527: step 70260, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:27:34.990520: step 70270, loss = 1.07 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 01:27:42.073896: step 70280, loss = 1.02 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 01:27:49.215973: step 70290, loss = 1.07 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 01:27:56.308745: step 70300, loss = 1.29 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 01:28:06.624807: step 70310, loss = 1.06 (46.5 examples/sec; 0.687 sec/batch)
2018-10-17 01:28:13.667354: step 70320, loss = 1.22 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 01:28:20.733441: step 70330, loss = 1.03 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 01:28:27.887771: step 70340, loss = 1.06 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 01:28:35.003885: step 70350, loss = 1.03 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 01:28:42.076126: step 70360, loss = 1.10 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 01:28:49.127661: step 70370, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 01:28:56.230550: step 70380, loss = 1.05 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 01:29:03.295958: step 70390, loss = 1.15 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:29:10.408515: step 70400, loss = 1.06 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 01:29:20.187523: step 70410, loss = 1.16 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 01:29:27.343227: step 70420, loss = 1.02 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 01:29:34.493960: step 70430, loss = 1.12 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 01:29:41.541605: step 70440, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 01:29:48.601659: step 70450, loss = 1.14 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 01:29:55.740986: step 70460, loss = 1.01 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 01:30:02.914536: step 70470, loss = 1.03 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 01:30:09.967692: step 70480, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 01:30:17.049649: step 70490, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 01:30:24.169572: step 70500, loss = 1.10 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 01:30:33.815794: step 70510, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 01:30:40.862377: step 70520, loss = 1.14 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 01:30:47.944868: step 70530, loss = 1.00 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 01:30:54.997359: step 70540, loss = 1.03 (47.6 examples/sec; 0.673 sec/batch)
2018-10-17 01:31:02.228548: step 70550, loss = 1.15 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 01:31:09.334920: step 70560, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 01:31:16.466927: step 70570, loss = 0.99 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 01:31:23.543814: step 70580, loss = 1.22 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:31:30.697359: step 70590, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 01:31:37.841562: step 70600, loss = 1.09 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 01:31:47.562070: step 70610, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:31:54.619879: step 70620, loss = 1.02 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 01:32:01.678059: step 70630, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 01:32:08.833197: step 70640, loss = 1.10 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:32:15.937337: step 70650, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 01:32:23.102985: step 70660, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:32:30.198339: step 70670, loss = 1.16 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 01:32:37.212624: step 70680, loss = 1.07 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 01:32:44.279775: step 70690, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:32:51.358239: step 70700, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 01:33:01.074363: step 70710, loss = 1.06 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 01:33:08.140956: step 70720, loss = 1.14 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 01:33:15.246518: step 70730, loss = 1.01 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 01:33:22.333950: step 70740, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:33:29.467422: step 70750, loss = 1.03 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 01:33:36.603500: step 70760, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 01:33:43.635133: step 70770, loss = 1.08 (45.9 examples/sec; 0.696 sec/batch)
2018-10-17 01:33:50.754021: step 70780, loss = 1.31 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 01:33:57.837819: step 70790, loss = 1.15 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 01:34:04.925690: step 70800, loss = 1.02 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 01:34:14.560168: step 70810, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:34:21.595407: step 70820, loss = 1.01 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 01:34:28.690533: step 70830, loss = 1.01 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 01:34:35.767875: step 70840, loss = 0.99 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 01:34:42.852335: step 70850, loss = 1.08 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:34:49.888427: step 70860, loss = 1.03 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 01:34:56.970034: step 70870, loss = 1.03 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 01:35:04.167189: step 70880, loss = 1.00 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 01:35:11.272092: step 70890, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 01:35:18.336065: step 70900, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 01:35:28.547340: step 70910, loss = 1.01 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 01:35:35.568024: step 70920, loss = 1.06 (42.9 examples/sec; 0.746 sec/batch)
2018-10-17 01:35:42.749435: step 70930, loss = 0.99 (45.9 examples/sec; 0.696 sec/batch)
2018-10-17 01:35:49.843309: step 70940, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 01:35:56.978166: step 70950, loss = 1.08 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 01:36:03.990032: step 70960, loss = 1.08 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 01:36:11.039568: step 70970, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 01:36:18.094530: step 70980, loss = 1.10 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:36:25.258637: step 70990, loss = 1.02 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 01:36:32.335270: step 71000, loss = 1.28 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 01:36:41.989910: step 71010, loss = 1.14 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 01:36:49.114831: step 71020, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 01:36:56.238094: step 71030, loss = 1.08 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 01:37:03.290402: step 71040, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 01:37:10.405435: step 71050, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 01:37:17.467050: step 71060, loss = 1.03 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 01:37:24.582160: step 71070, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:37:31.610324: step 71080, loss = 1.02 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 01:37:38.678597: step 71090, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 01:37:45.727287: step 71100, loss = 0.99 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 01:37:55.295074: step 71110, loss = 1.18 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 01:38:02.287353: step 71120, loss = 1.02 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 01:38:09.338757: step 71130, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 01:38:16.409488: step 71140, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:38:23.566295: step 71150, loss = 1.03 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 01:38:30.641588: step 71160, loss = 1.16 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 01:38:37.610574: step 71170, loss = 1.12 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 01:38:44.793397: step 71180, loss = 1.05 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 01:38:51.852027: step 71190, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 01:38:58.971583: step 71200, loss = 1.02 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 01:39:08.695349: step 71210, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 01:39:15.750653: step 71220, loss = 1.01 (46.1 examples/sec; 0.693 sec/batch)
2018-10-17 01:39:22.859115: step 71230, loss = 1.06 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 01:39:29.875272: step 71240, loss = 1.13 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 01:39:36.959504: step 71250, loss = 1.14 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:39:44.035811: step 71260, loss = 1.10 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 01:39:51.126788: step 71270, loss = 1.11 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 01:39:58.318751: step 71280, loss = 0.99 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 01:40:05.476893: step 71290, loss = 1.04 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 01:40:12.586365: step 71300, loss = 1.09 (47.3 examples/sec; 0.677 sec/batch)
2018-10-17 01:40:22.266560: step 71310, loss = 1.05 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 01:40:29.372534: step 71320, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:40:36.401678: step 71330, loss = 1.00 (45.7 examples/sec; 0.699 sec/batch)
2018-10-17 01:40:43.465495: step 71340, loss = 1.11 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 01:40:50.519930: step 71350, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 01:40:57.518616: step 71360, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:41:04.664499: step 71370, loss = 1.03 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 01:41:11.723526: step 71380, loss = 1.22 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 01:41:18.847020: step 71390, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 01:41:25.934409: step 71400, loss = 1.00 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 01:41:35.595012: step 71410, loss = 1.09 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 01:41:42.642724: step 71420, loss = 1.14 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 01:41:49.704981: step 71430, loss = 1.04 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 01:41:56.740440: step 71440, loss = 1.10 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 01:42:03.828215: step 71450, loss = 0.99 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 01:42:11.008752: step 71460, loss = 1.07 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 01:42:18.080046: step 71470, loss = 1.01 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 01:42:25.203446: step 71480, loss = 1.05 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 01:42:32.348046: step 71490, loss = 1.14 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 01:42:39.464997: step 71500, loss = 1.16 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 01:42:49.853725: step 71510, loss = 1.03 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 01:42:56.919234: step 71520, loss = 1.14 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 01:43:04.037463: step 71530, loss = 1.04 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 01:43:11.080576: step 71540, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 01:43:18.110219: step 71550, loss = 1.12 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 01:43:25.111928: step 71560, loss = 1.05 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 01:43:32.279185: step 71570, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 01:43:39.377848: step 71580, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 01:43:46.431581: step 71590, loss = 1.05 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 01:43:53.565596: step 71600, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 01:44:03.663701: step 71610, loss = 1.28 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 01:44:10.703764: step 71620, loss = 1.08 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 01:44:17.841140: step 71630, loss = 1.16 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 01:44:24.962654: step 71640, loss = 1.02 (43.0 examples/sec; 0.744 sec/batch)
2018-10-17 01:44:32.087942: step 71650, loss = 1.08 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:44:39.252135: step 71660, loss = 1.10 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:44:46.271994: step 71670, loss = 0.99 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 01:44:53.357459: step 71680, loss = 1.12 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 01:45:00.445943: step 71690, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 01:45:07.623558: step 71700, loss = 1.09 (42.9 examples/sec; 0.746 sec/batch)
2018-10-17 01:45:17.232104: step 71710, loss = 1.01 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 01:45:24.294181: step 71720, loss = 1.02 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 01:45:31.393490: step 71730, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 01:45:38.438170: step 71740, loss = 0.99 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 01:45:45.620389: step 71750, loss = 1.02 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 01:45:52.668629: step 71760, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:45:59.721199: step 71770, loss = 1.00 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 01:46:06.802511: step 71780, loss = 1.23 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 01:46:13.891555: step 71790, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 01:46:21.041830: step 71800, loss = 1.07 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 01:46:30.811557: step 71810, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:46:38.018120: step 71820, loss = 1.04 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 01:46:45.119831: step 71830, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:46:52.188393: step 71840, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 01:46:59.326334: step 71850, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 01:47:06.383987: step 71860, loss = 1.14 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:47:13.441076: step 71870, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 01:47:20.537617: step 71880, loss = 1.02 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 01:47:27.570182: step 71890, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 01:47:34.630184: step 71900, loss = 1.22 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 01:47:44.246781: step 71910, loss = 1.07 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 01:47:51.366817: step 71920, loss = 1.23 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 01:47:58.484859: step 71930, loss = 1.10 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 01:48:05.523536: step 71940, loss = 1.27 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 01:48:12.699949: step 71950, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:48:19.821275: step 71960, loss = 1.08 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 01:48:26.923748: step 71970, loss = 1.12 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 01:48:34.001641: step 71980, loss = 1.15 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:48:41.082934: step 71990, loss = 1.05 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 01:48:48.204563: step 72000, loss = 1.14 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 01:48:57.812526: step 72010, loss = 1.01 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 01:49:04.860501: step 72020, loss = 1.01 (46.3 examples/sec; 0.690 sec/batch)
2018-10-17 01:49:11.918569: step 72030, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 01:49:18.988968: step 72040, loss = 1.16 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 01:49:26.105438: step 72050, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 01:49:33.155203: step 72060, loss = 1.17 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:49:40.238988: step 72070, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:49:47.368787: step 72080, loss = 1.03 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 01:49:54.422240: step 72090, loss = 1.05 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 01:50:01.629324: step 72100, loss = 1.04 (43.0 examples/sec; 0.744 sec/batch)
2018-10-17 01:50:11.565484: step 72110, loss = 1.04 (47.5 examples/sec; 0.674 sec/batch)
2018-10-17 01:50:18.647880: step 72120, loss = 1.00 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 01:50:25.672201: step 72130, loss = 1.06 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 01:50:32.819886: step 72140, loss = 1.12 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 01:50:39.910264: step 72150, loss = 1.12 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 01:50:46.957311: step 72160, loss = 1.02 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 01:50:54.135571: step 72170, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:51:01.161282: step 72180, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:51:08.253383: step 72190, loss = 1.25 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 01:51:15.266764: step 72200, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:51:24.957348: step 72210, loss = 1.04 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 01:51:32.021862: step 72220, loss = 1.10 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 01:51:39.002752: step 72230, loss = 1.13 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:51:46.071243: step 72240, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 01:51:53.129814: step 72250, loss = 1.07 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 01:52:00.162754: step 72260, loss = 1.11 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 01:52:07.257723: step 72270, loss = 1.03 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 01:52:14.343557: step 72280, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 01:52:21.365406: step 72290, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 01:52:28.449091: step 72300, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 01:52:38.110488: step 72310, loss = 1.06 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 01:52:45.065290: step 72320, loss = 1.01 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 01:52:52.126881: step 72330, loss = 1.25 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:52:59.190452: step 72340, loss = 1.00 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 01:53:06.248604: step 72350, loss = 1.04 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 01:53:13.344497: step 72360, loss = 1.09 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 01:53:20.381426: step 72370, loss = 0.99 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 01:53:27.395899: step 72380, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:53:34.461336: step 72390, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 01:53:41.503139: step 72400, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 01:53:51.171797: step 72410, loss = 1.19 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 01:53:58.252128: step 72420, loss = 1.05 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 01:54:05.341983: step 72430, loss = 1.14 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 01:54:12.447174: step 72440, loss = 1.00 (47.5 examples/sec; 0.673 sec/batch)
2018-10-17 01:54:19.558853: step 72450, loss = 1.12 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 01:54:26.626873: step 72460, loss = 1.05 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 01:54:33.721989: step 72470, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 01:54:40.754781: step 72480, loss = 1.02 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 01:54:47.867327: step 72490, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 01:54:54.952321: step 72500, loss = 1.07 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 01:55:05.148021: step 72510, loss = 1.00 (47.3 examples/sec; 0.677 sec/batch)
2018-10-17 01:55:12.080596: step 72520, loss = 1.00 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 01:55:19.207285: step 72530, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:55:26.254920: step 72540, loss = 1.07 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 01:55:33.337184: step 72550, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:55:40.436164: step 72560, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 01:55:47.505928: step 72570, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 01:55:54.577128: step 72580, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 01:56:01.760060: step 72590, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:56:08.867810: step 72600, loss = 1.23 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 01:56:18.707643: step 72610, loss = 1.06 (48.1 examples/sec; 0.665 sec/batch)
2018-10-17 01:56:25.825593: step 72620, loss = 1.02 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 01:56:32.934179: step 72630, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 01:56:40.117441: step 72640, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 01:56:47.223422: step 72650, loss = 1.23 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 01:56:54.238269: step 72660, loss = 0.99 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 01:57:01.404261: step 72670, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 01:57:08.492891: step 72680, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 01:57:15.624809: step 72690, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 01:57:22.763993: step 72700, loss = 1.08 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 01:57:32.377097: step 72710, loss = 1.04 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 01:57:39.381641: step 72720, loss = 1.04 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:57:46.418917: step 72730, loss = 1.03 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 01:57:53.505654: step 72740, loss = 1.13 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:58:00.585409: step 72750, loss = 1.01 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 01:58:07.709085: step 72760, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 01:58:14.831310: step 72770, loss = 1.09 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 01:58:21.883138: step 72780, loss = 1.20 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 01:58:28.959811: step 72790, loss = 1.08 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 01:58:36.060550: step 72800, loss = 1.08 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 01:58:45.667974: step 72810, loss = 1.05 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 01:58:52.775343: step 72820, loss = 1.09 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 01:58:59.811413: step 72830, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 01:59:06.831372: step 72840, loss = 1.02 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 01:59:13.922391: step 72850, loss = 1.03 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 01:59:20.921825: step 72860, loss = 1.06 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 01:59:27.939869: step 72870, loss = 1.06 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 01:59:35.046043: step 72880, loss = 1.02 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 01:59:42.151709: step 72890, loss = 1.11 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 01:59:49.229681: step 72900, loss = 1.10 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 01:59:59.052509: step 72910, loss = 0.99 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 02:00:06.100763: step 72920, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 02:00:13.245477: step 72930, loss = 1.03 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 02:00:20.417819: step 72940, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 02:00:27.490498: step 72950, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 02:00:34.616809: step 72960, loss = 1.11 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:00:41.615121: step 72970, loss = 1.06 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 02:00:48.709331: step 72980, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 02:00:55.761625: step 72990, loss = 1.06 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 02:01:02.844440: step 73000, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:01:12.517810: step 73010, loss = 1.01 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 02:01:19.495353: step 73020, loss = 1.07 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 02:01:26.545491: step 73030, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 02:01:33.592688: step 73040, loss = 0.99 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 02:01:40.706459: step 73050, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:01:47.827601: step 73060, loss = 1.06 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:01:54.892478: step 73070, loss = 1.05 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:02:02.054049: step 73080, loss = 1.01 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 02:02:09.140916: step 73090, loss = 1.06 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 02:02:16.165438: step 73100, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:02:25.814504: step 73110, loss = 1.03 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 02:02:32.843864: step 73120, loss = 1.07 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:02:39.923816: step 73130, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 02:02:46.989708: step 73140, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:02:54.039052: step 73150, loss = 1.07 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:03:01.083109: step 73160, loss = 1.09 (47.3 examples/sec; 0.677 sec/batch)
2018-10-17 02:03:08.245706: step 73170, loss = 1.04 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 02:03:15.339093: step 73180, loss = 1.13 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 02:03:22.434899: step 73190, loss = 1.05 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 02:03:29.496494: step 73200, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 02:03:39.199052: step 73210, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 02:03:46.206873: step 73220, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 02:03:53.326993: step 73230, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 02:04:00.378065: step 73240, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 02:04:07.409503: step 73250, loss = 1.07 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 02:04:14.523983: step 73260, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 02:04:21.584868: step 73270, loss = 1.08 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 02:04:28.625904: step 73280, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 02:04:35.732913: step 73290, loss = 1.01 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 02:04:42.782860: step 73300, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 02:04:52.373584: step 73310, loss = 1.09 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 02:04:59.493619: step 73320, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:05:06.558070: step 73330, loss = 1.07 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 02:05:13.623429: step 73340, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 02:05:20.669519: step 73350, loss = 1.06 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 02:05:27.756594: step 73360, loss = 1.02 (42.9 examples/sec; 0.746 sec/batch)
2018-10-17 02:05:34.940329: step 73370, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 02:05:41.996191: step 73380, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 02:05:49.115506: step 73390, loss = 1.08 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 02:05:56.147502: step 73400, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 02:06:06.238567: step 73410, loss = 1.17 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 02:06:13.208127: step 73420, loss = 1.10 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 02:06:20.296773: step 73430, loss = 1.08 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 02:06:27.354616: step 73440, loss = 1.02 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 02:06:34.457313: step 73450, loss = 1.18 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 02:06:41.549971: step 73460, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:06:48.646036: step 73470, loss = 1.07 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 02:06:55.814905: step 73480, loss = 1.08 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 02:07:02.863030: step 73490, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 02:07:09.937254: step 73500, loss = 1.05 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 02:07:19.581087: step 73510, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 02:07:26.603182: step 73520, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:07:33.764942: step 73530, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 02:07:40.862472: step 73540, loss = 1.19 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 02:07:47.885221: step 73550, loss = 1.03 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 02:07:54.944246: step 73560, loss = 1.02 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 02:08:02.124579: step 73570, loss = 1.32 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 02:08:09.154651: step 73580, loss = 1.12 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 02:08:16.304011: step 73590, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 02:08:23.359841: step 73600, loss = 1.07 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 02:08:32.932275: step 73610, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 02:08:40.013181: step 73620, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 02:08:47.055816: step 73630, loss = 1.12 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 02:08:54.128421: step 73640, loss = 1.04 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 02:09:01.166438: step 73650, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:09:08.336182: step 73660, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:09:15.385267: step 73670, loss = 0.99 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 02:09:22.435146: step 73680, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 02:09:29.525702: step 73690, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 02:09:36.661717: step 73700, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 02:09:46.283917: step 73710, loss = 0.99 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 02:09:53.320154: step 73720, loss = 1.10 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 02:10:00.424954: step 73730, loss = 1.00 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 02:10:07.464814: step 73740, loss = 1.07 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 02:10:14.523367: step 73750, loss = 1.15 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 02:10:21.602643: step 73760, loss = 1.15 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 02:10:28.704466: step 73770, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 02:10:35.874492: step 73780, loss = 1.14 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 02:10:42.915547: step 73790, loss = 1.07 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 02:10:49.937772: step 73800, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 02:10:59.765304: step 73810, loss = 1.04 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 02:11:06.893207: step 73820, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:11:13.979400: step 73830, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:11:21.074208: step 73840, loss = 1.01 (47.3 examples/sec; 0.677 sec/batch)
2018-10-17 02:11:28.167606: step 73850, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:11:35.245105: step 73860, loss = 1.05 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 02:11:42.421418: step 73870, loss = 1.00 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 02:11:49.431848: step 73880, loss = 1.09 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 02:11:56.516209: step 73890, loss = 1.11 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 02:12:03.514250: step 73900, loss = 1.05 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 02:12:13.196738: step 73910, loss = 1.03 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 02:12:20.223044: step 73920, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:12:27.347225: step 73930, loss = 1.05 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 02:12:34.473959: step 73940, loss = 1.12 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 02:12:41.556978: step 73950, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 02:12:48.599036: step 73960, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 02:12:55.619206: step 73970, loss = 1.23 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 02:13:02.645145: step 73980, loss = 1.20 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 02:13:09.694406: step 73990, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 02:13:16.771813: step 74000, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 02:13:26.521833: step 74010, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 02:13:33.599479: step 74020, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 02:13:40.723872: step 74030, loss = 1.03 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 02:13:47.775466: step 74040, loss = 1.00 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 02:13:54.884855: step 74050, loss = 1.08 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 02:14:02.017396: step 74060, loss = 1.09 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 02:14:09.072396: step 74070, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 02:14:16.203151: step 74080, loss = 1.03 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 02:14:23.249411: step 74090, loss = 1.03 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 02:14:30.361955: step 74100, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:14:40.425324: step 74110, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 02:14:47.410445: step 74120, loss = 1.08 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 02:14:54.576623: step 74130, loss = 1.23 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 02:15:01.565866: step 74140, loss = 0.99 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 02:15:08.627751: step 74150, loss = 1.08 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 02:15:15.676584: step 74160, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 02:15:22.752811: step 74170, loss = 1.14 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 02:15:29.889058: step 74180, loss = 1.05 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 02:15:37.092959: step 74190, loss = 1.04 (42.4 examples/sec; 0.754 sec/batch)
2018-10-17 02:15:44.211594: step 74200, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 02:15:53.954682: step 74210, loss = 1.13 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 02:16:01.059899: step 74220, loss = 1.01 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 02:16:08.131754: step 74230, loss = 1.09 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 02:16:15.122505: step 74240, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 02:16:22.211958: step 74250, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:16:29.292372: step 74260, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 02:16:36.373127: step 74270, loss = 1.08 (47.9 examples/sec; 0.668 sec/batch)
2018-10-17 02:16:43.519281: step 74280, loss = 1.17 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 02:16:50.658508: step 74290, loss = 1.02 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 02:16:57.751625: step 74300, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 02:17:07.404127: step 74310, loss = 1.10 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 02:17:14.505671: step 74320, loss = 1.10 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 02:17:21.526710: step 74330, loss = 1.04 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 02:17:28.628439: step 74340, loss = 1.02 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 02:17:35.684230: step 74350, loss = 1.20 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 02:17:42.749752: step 74360, loss = 1.06 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 02:17:49.791771: step 74370, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 02:17:56.894016: step 74380, loss = 0.99 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 02:18:04.030776: step 74390, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 02:18:11.105707: step 74400, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 02:18:21.165685: step 74410, loss = 1.30 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 02:18:28.169057: step 74420, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 02:18:35.286952: step 74430, loss = 1.01 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 02:18:42.354780: step 74440, loss = 1.07 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 02:18:49.486292: step 74450, loss = 1.03 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 02:18:56.646571: step 74460, loss = 1.00 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 02:19:03.696674: step 74470, loss = 1.01 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 02:19:10.765081: step 74480, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 02:19:17.888146: step 74490, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 02:19:24.955989: step 74500, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 02:19:34.681337: step 74510, loss = 1.02 (47.9 examples/sec; 0.668 sec/batch)
2018-10-17 02:19:41.673799: step 74520, loss = 1.10 (47.3 examples/sec; 0.676 sec/batch)
2018-10-17 02:19:48.784065: step 74530, loss = 1.09 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 02:19:55.950467: step 74540, loss = 1.14 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 02:20:03.010447: step 74550, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:20:10.135642: step 74560, loss = 1.13 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 02:20:17.191464: step 74570, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:20:24.258865: step 74580, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:20:31.342172: step 74590, loss = 1.03 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 02:20:38.464268: step 74600, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 02:20:48.244434: step 74610, loss = 1.13 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 02:20:55.302118: step 74620, loss = 1.01 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 02:21:02.398826: step 74630, loss = 1.01 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 02:21:09.474165: step 74640, loss = 1.07 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 02:21:16.527431: step 74650, loss = 1.01 (47.5 examples/sec; 0.674 sec/batch)
2018-10-17 02:21:23.609529: step 74660, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 02:21:30.765416: step 74670, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 02:21:37.849173: step 74680, loss = 0.99 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 02:21:44.998656: step 74690, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 02:21:52.087581: step 74700, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 02:22:01.624541: step 74710, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 02:22:08.671357: step 74720, loss = 1.28 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 02:22:15.755948: step 74730, loss = 1.22 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:22:22.859804: step 74740, loss = 0.99 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 02:22:29.928110: step 74750, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 02:22:37.052150: step 74760, loss = 1.03 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 02:22:44.196287: step 74770, loss = 1.00 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 02:22:51.267588: step 74780, loss = 1.01 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 02:22:58.296993: step 74790, loss = 1.13 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 02:23:05.398628: step 74800, loss = 1.03 (44.7 examples/sec; 0.717 sec/batch)
2018-10-17 02:23:14.953998: step 74810, loss = 1.04 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 02:23:22.042012: step 74820, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:23:29.055586: step 74830, loss = 1.14 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:23:36.083888: step 74840, loss = 1.05 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 02:23:43.249867: step 74850, loss = 0.99 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 02:23:50.272029: step 74860, loss = 1.05 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 02:23:57.356723: step 74870, loss = 1.00 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 02:24:04.404896: step 74880, loss = 1.04 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 02:24:11.563241: step 74890, loss = 1.01 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 02:24:18.680137: step 74900, loss = 1.01 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 02:24:28.280482: step 74910, loss = 1.04 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 02:24:35.289017: step 74920, loss = 1.12 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:24:42.416291: step 74930, loss = 1.00 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 02:24:49.491914: step 74940, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:24:56.643239: step 74950, loss = 1.15 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 02:25:03.802643: step 74960, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 02:25:10.988697: step 74970, loss = 1.05 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 02:25:18.098316: step 74980, loss = 1.11 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 02:25:25.173497: step 74990, loss = 1.08 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:25:32.282421: step 75000, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 02:25:45.863026: step 75010, loss = 1.02 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 02:25:52.707864: step 75020, loss = 1.09 (48.9 examples/sec; 0.654 sec/batch)
2018-10-17 02:25:59.657659: step 75030, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 02:26:06.763554: step 75040, loss = 1.06 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:26:13.904110: step 75050, loss = 1.15 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:26:21.042863: step 75060, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:26:28.164122: step 75070, loss = 1.01 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 02:26:35.251503: step 75080, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 02:26:42.391481: step 75090, loss = 1.16 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 02:26:49.613891: step 75100, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 02:26:59.323924: step 75110, loss = 1.12 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 02:27:06.378318: step 75120, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:27:13.463486: step 75130, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 02:27:20.573476: step 75140, loss = 1.00 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 02:27:27.759743: step 75150, loss = 1.10 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 02:27:35.054127: step 75160, loss = 0.99 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 02:27:42.141006: step 75170, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 02:27:49.268231: step 75180, loss = 1.03 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 02:27:56.354182: step 75190, loss = 1.03 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 02:28:03.392299: step 75200, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:28:12.944018: step 75210, loss = 1.01 (47.7 examples/sec; 0.670 sec/batch)
2018-10-17 02:28:20.075290: step 75220, loss = 1.05 (43.0 examples/sec; 0.744 sec/batch)
2018-10-17 02:28:27.157228: step 75230, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:28:34.347923: step 75240, loss = 1.12 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 02:28:41.495056: step 75250, loss = 1.04 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 02:28:48.594439: step 75260, loss = 1.00 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 02:28:55.664978: step 75270, loss = 1.02 (44.0 examples/sec; 0.726 sec/batch)
2018-10-17 02:29:02.811036: step 75280, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 02:29:09.941487: step 75290, loss = 1.21 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 02:29:17.061623: step 75300, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 02:29:26.717857: step 75310, loss = 1.05 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 02:29:33.729189: step 75320, loss = 1.24 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 02:29:40.779622: step 75330, loss = 1.16 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 02:29:47.980879: step 75340, loss = 0.99 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 02:29:55.058936: step 75350, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:30:02.204901: step 75360, loss = 1.14 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 02:30:09.353503: step 75370, loss = 1.04 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 02:30:16.481526: step 75380, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 02:30:23.564827: step 75390, loss = 1.02 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 02:30:30.619563: step 75400, loss = 1.05 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 02:30:40.217283: step 75410, loss = 1.07 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:30:47.405159: step 75420, loss = 1.02 (47.6 examples/sec; 0.672 sec/batch)
2018-10-17 02:30:54.391224: step 75430, loss = 1.16 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 02:31:01.421395: step 75440, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:31:08.578266: step 75450, loss = 1.11 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 02:31:15.672572: step 75460, loss = 1.02 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 02:31:22.792885: step 75470, loss = 1.15 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 02:31:29.863177: step 75480, loss = 1.04 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:31:37.054453: step 75490, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 02:31:44.092761: step 75500, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 02:31:54.422251: step 75510, loss = 1.00 (47.4 examples/sec; 0.676 sec/batch)
2018-10-17 02:32:01.367914: step 75520, loss = 1.14 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 02:32:08.443860: step 75530, loss = 1.10 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:32:15.591795: step 75540, loss = 1.03 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 02:32:22.606273: step 75550, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:32:29.791443: step 75560, loss = 1.12 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 02:32:36.812106: step 75570, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 02:32:43.876050: step 75580, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:32:51.025129: step 75590, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 02:32:58.132839: step 75600, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 02:33:07.651979: step 75610, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:33:14.647456: step 75620, loss = 0.99 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 02:33:21.801758: step 75630, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 02:33:28.793299: step 75640, loss = 1.10 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 02:33:35.906349: step 75650, loss = 1.12 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 02:33:42.946121: step 75660, loss = 1.09 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 02:33:50.023990: step 75670, loss = 1.05 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 02:33:57.119080: step 75680, loss = 1.18 (42.8 examples/sec; 0.747 sec/batch)
2018-10-17 02:34:04.299919: step 75690, loss = 1.09 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 02:34:11.331798: step 75700, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 02:34:20.974772: step 75710, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:34:27.987548: step 75720, loss = 1.07 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 02:34:35.037296: step 75730, loss = 1.03 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 02:34:42.160509: step 75740, loss = 1.06 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 02:34:49.227933: step 75750, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 02:34:56.393101: step 75760, loss = 1.12 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 02:35:03.580010: step 75770, loss = 1.05 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 02:35:10.644094: step 75780, loss = 1.04 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:35:17.753984: step 75790, loss = 1.09 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 02:35:24.850275: step 75800, loss = 1.04 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 02:35:34.577557: step 75810, loss = 1.00 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 02:35:41.702374: step 75820, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 02:35:48.791364: step 75830, loss = 1.11 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 02:35:55.885172: step 75840, loss = 0.99 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 02:36:02.964872: step 75850, loss = 1.11 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 02:36:10.027545: step 75860, loss = 1.03 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 02:36:17.063262: step 75870, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:36:24.150464: step 75880, loss = 1.11 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 02:36:31.226940: step 75890, loss = 1.17 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 02:36:38.200998: step 75900, loss = 1.05 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 02:36:48.169123: step 75910, loss = 1.19 (48.0 examples/sec; 0.667 sec/batch)
2018-10-17 02:36:55.303996: step 75920, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 02:37:02.489560: step 75930, loss = 0.99 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 02:37:09.470146: step 75940, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 02:37:16.516185: step 75950, loss = 1.06 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 02:37:23.524849: step 75960, loss = 1.10 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 02:37:30.570464: step 75970, loss = 1.07 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 02:37:37.605199: step 75980, loss = 1.12 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 02:37:44.744817: step 75990, loss = 1.10 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 02:37:51.806915: step 76000, loss = 1.01 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 02:38:01.675941: step 76010, loss = 0.99 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 02:38:08.784092: step 76020, loss = 1.03 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 02:38:15.848943: step 76030, loss = 0.99 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 02:38:22.947849: step 76040, loss = 1.01 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 02:38:29.937411: step 76050, loss = 1.05 (47.3 examples/sec; 0.676 sec/batch)
2018-10-17 02:38:37.036947: step 76060, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 02:38:44.109331: step 76070, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 02:38:51.261885: step 76080, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:38:58.243678: step 76090, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 02:39:05.380231: step 76100, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:39:14.989230: step 76110, loss = 1.02 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 02:39:22.052349: step 76120, loss = 1.08 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 02:39:29.135000: step 76130, loss = 1.03 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 02:39:36.267289: step 76140, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 02:39:43.301390: step 76150, loss = 1.00 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 02:39:50.361594: step 76160, loss = 1.14 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:39:57.438333: step 76170, loss = 1.04 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 02:40:04.527850: step 76180, loss = 1.08 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 02:40:11.667323: step 76190, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 02:40:18.727028: step 76200, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:40:28.348856: step 76210, loss = 1.13 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:40:35.427765: step 76220, loss = 1.13 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 02:40:42.498586: step 76230, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:40:49.651990: step 76240, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 02:40:56.740025: step 76250, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 02:41:03.857824: step 76260, loss = 1.20 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 02:41:10.911206: step 76270, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 02:41:18.024158: step 76280, loss = 1.02 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 02:41:25.102835: step 76290, loss = 1.18 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 02:41:32.215138: step 76300, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 02:41:41.959056: step 76310, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 02:41:49.096639: step 76320, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 02:41:56.153053: step 76330, loss = 1.06 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 02:42:03.232619: step 76340, loss = 1.06 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 02:42:10.325882: step 76350, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:42:17.380585: step 76360, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 02:42:24.433992: step 76370, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:42:31.565276: step 76380, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:42:38.632940: step 76390, loss = 1.03 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 02:42:45.748001: step 76400, loss = 1.01 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 02:42:55.507588: step 76410, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 02:43:02.667089: step 76420, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:43:09.757917: step 76430, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 02:43:16.814175: step 76440, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:43:23.971578: step 76450, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 02:43:31.033666: step 76460, loss = 1.03 (47.5 examples/sec; 0.673 sec/batch)
2018-10-17 02:43:38.187643: step 76470, loss = 1.11 (43.6 examples/sec; 0.735 sec/batch)
2018-10-17 02:43:45.307976: step 76480, loss = 1.02 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 02:43:52.352499: step 76490, loss = 1.04 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 02:43:59.446404: step 76500, loss = 1.16 (46.8 examples/sec; 0.683 sec/batch)
2018-10-17 02:44:09.176444: step 76510, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:44:16.293855: step 76520, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 02:44:23.396862: step 76530, loss = 1.05 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 02:44:30.532283: step 76540, loss = 1.09 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 02:44:37.648456: step 76550, loss = 1.09 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 02:44:44.695477: step 76560, loss = 1.06 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 02:44:51.810590: step 76570, loss = 1.06 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 02:44:58.882748: step 76580, loss = 1.00 (44.5 examples/sec; 0.718 sec/batch)
2018-10-17 02:45:05.965872: step 76590, loss = 1.03 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 02:45:13.076160: step 76600, loss = 1.12 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 02:45:22.903947: step 76610, loss = 1.03 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 02:45:30.015821: step 76620, loss = 1.00 (43.0 examples/sec; 0.744 sec/batch)
2018-10-17 02:45:37.097663: step 76630, loss = 1.05 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 02:45:44.184993: step 76640, loss = 1.04 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 02:45:51.264997: step 76650, loss = 1.10 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 02:45:58.326745: step 76660, loss = 1.12 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 02:46:05.389136: step 76670, loss = 1.23 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 02:46:12.427897: step 76680, loss = 1.04 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 02:46:19.506491: step 76690, loss = 1.00 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 02:46:26.592034: step 76700, loss = 1.11 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:46:36.579766: step 76710, loss = 0.99 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 02:46:43.625394: step 76720, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 02:46:50.711795: step 76730, loss = 1.12 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 02:46:57.767545: step 76740, loss = 1.00 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 02:47:04.846659: step 76750, loss = 1.09 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:47:11.941827: step 76760, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:47:19.070401: step 76770, loss = 1.01 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 02:47:26.227018: step 76780, loss = 1.06 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:47:33.341227: step 76790, loss = 1.09 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 02:47:40.419950: step 76800, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:47:50.110817: step 76810, loss = 1.02 (43.0 examples/sec; 0.744 sec/batch)
2018-10-17 02:47:57.150394: step 76820, loss = 1.02 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 02:48:04.233851: step 76830, loss = 1.00 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 02:48:11.281870: step 76840, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 02:48:18.412924: step 76850, loss = 1.12 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 02:48:25.553082: step 76860, loss = 1.08 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:48:32.590265: step 76870, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:48:39.717498: step 76880, loss = 1.05 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 02:48:46.816206: step 76890, loss = 1.14 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 02:48:53.865118: step 76900, loss = 1.02 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 02:49:03.653158: step 76910, loss = 1.02 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 02:49:10.677966: step 76920, loss = 1.03 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 02:49:17.789472: step 76930, loss = 0.99 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 02:49:24.849661: step 76940, loss = 1.03 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 02:49:31.908639: step 76950, loss = 1.06 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 02:49:38.968218: step 76960, loss = 0.99 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 02:49:46.072204: step 76970, loss = 1.08 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 02:49:53.184241: step 76980, loss = 1.06 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 02:50:00.264042: step 76990, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 02:50:07.311945: step 77000, loss = 1.04 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 02:50:17.420486: step 77010, loss = 1.00 (48.0 examples/sec; 0.667 sec/batch)
2018-10-17 02:50:24.464792: step 77020, loss = 1.05 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 02:50:31.595033: step 77030, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 02:50:38.683506: step 77040, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:50:45.769966: step 77050, loss = 1.17 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:50:52.900044: step 77060, loss = 1.11 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 02:50:59.936197: step 77070, loss = 1.05 (44.5 examples/sec; 0.718 sec/batch)
2018-10-17 02:51:07.053162: step 77080, loss = 1.08 (46.1 examples/sec; 0.693 sec/batch)
2018-10-17 02:51:14.132631: step 77090, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 02:51:21.226205: step 77100, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:51:31.269443: step 77110, loss = 1.19 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 02:51:38.367439: step 77120, loss = 1.17 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:51:45.491177: step 77130, loss = 1.17 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 02:51:52.580417: step 77140, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 02:51:59.691936: step 77150, loss = 1.03 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 02:52:06.766754: step 77160, loss = 1.11 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 02:52:13.868575: step 77170, loss = 1.03 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 02:52:20.947960: step 77180, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 02:52:28.130166: step 77190, loss = 1.04 (45.9 examples/sec; 0.696 sec/batch)
2018-10-17 02:52:35.165734: step 77200, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:52:45.150442: step 77210, loss = 1.05 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 02:52:52.181534: step 77220, loss = 1.01 (47.5 examples/sec; 0.674 sec/batch)
2018-10-17 02:52:59.281876: step 77230, loss = 1.07 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 02:53:06.387826: step 77240, loss = 1.02 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 02:53:13.531285: step 77250, loss = 1.13 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:53:20.631200: step 77260, loss = 1.09 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 02:53:27.693057: step 77270, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:53:34.806740: step 77280, loss = 1.02 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 02:53:41.841607: step 77290, loss = 1.08 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 02:53:48.912023: step 77300, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 02:53:58.462109: step 77310, loss = 1.16 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 02:54:05.565152: step 77320, loss = 1.09 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 02:54:12.590961: step 77330, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 02:54:19.725837: step 77340, loss = 1.05 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 02:54:26.843184: step 77350, loss = 1.03 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 02:54:33.937546: step 77360, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 02:54:41.118892: step 77370, loss = 1.05 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 02:54:48.255857: step 77380, loss = 1.00 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 02:54:55.346851: step 77390, loss = 1.20 (42.8 examples/sec; 0.747 sec/batch)
2018-10-17 02:55:02.472407: step 77400, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 02:55:12.094498: step 77410, loss = 1.08 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 02:55:19.229814: step 77420, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:55:26.285461: step 77430, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:55:33.404039: step 77440, loss = 1.13 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 02:55:40.484944: step 77450, loss = 1.02 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 02:55:47.595677: step 77460, loss = 1.07 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 02:55:54.650500: step 77470, loss = 0.99 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 02:56:01.741783: step 77480, loss = 1.14 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 02:56:08.816893: step 77490, loss = 1.02 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 02:56:15.933487: step 77500, loss = 1.01 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 02:56:25.515806: step 77510, loss = 1.12 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 02:56:32.542419: step 77520, loss = 1.08 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 02:56:39.654592: step 77530, loss = 1.07 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 02:56:46.779099: step 77540, loss = 1.14 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 02:56:53.827156: step 77550, loss = 0.99 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 02:57:00.909698: step 77560, loss = 1.04 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 02:57:07.997707: step 77570, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 02:57:15.161131: step 77580, loss = 1.14 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 02:57:22.198080: step 77590, loss = 1.03 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 02:57:29.230678: step 77600, loss = 1.17 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:57:38.872844: step 77610, loss = 0.99 (46.8 examples/sec; 0.683 sec/batch)
2018-10-17 02:57:45.955166: step 77620, loss = 0.99 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 02:57:53.047606: step 77630, loss = 1.01 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 02:58:00.166474: step 77640, loss = 1.02 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 02:58:07.286719: step 77650, loss = 0.99 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 02:58:14.321565: step 77660, loss = 1.12 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 02:58:21.434278: step 77670, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 02:58:28.490237: step 77680, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 02:58:35.668684: step 77690, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 02:58:42.719437: step 77700, loss = 1.09 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 02:58:52.378176: step 77710, loss = 1.24 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 02:58:59.467121: step 77720, loss = 1.03 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 02:59:06.543499: step 77730, loss = 1.05 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 02:59:13.637354: step 77740, loss = 1.10 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 02:59:20.811172: step 77750, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 02:59:27.902320: step 77760, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 02:59:35.034208: step 77770, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 02:59:42.088995: step 77780, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 02:59:49.259694: step 77790, loss = 1.15 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 02:59:56.381279: step 77800, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 03:00:06.006117: step 77810, loss = 1.01 (43.3 examples/sec; 0.740 sec/batch)
2018-10-17 03:00:13.152759: step 77820, loss = 1.05 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 03:00:20.323931: step 77830, loss = 1.13 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 03:00:27.411977: step 77840, loss = 1.08 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:00:34.441429: step 77850, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:00:41.473819: step 77860, loss = 1.16 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 03:00:48.513718: step 77870, loss = 1.00 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 03:00:55.631651: step 77880, loss = 1.05 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:01:02.707551: step 77890, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 03:01:09.740752: step 77900, loss = 1.16 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 03:01:19.473275: step 77910, loss = 1.04 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 03:01:26.446489: step 77920, loss = 1.06 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 03:01:33.528843: step 77930, loss = 1.15 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 03:01:40.590723: step 77940, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 03:01:47.693859: step 77950, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 03:01:54.824182: step 77960, loss = 1.04 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 03:02:01.881659: step 77970, loss = 1.10 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 03:02:08.956809: step 77980, loss = 1.09 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 03:02:16.016621: step 77990, loss = 1.07 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 03:02:23.109922: step 78000, loss = 1.07 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 03:02:32.746029: step 78010, loss = 1.10 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:02:39.849386: step 78020, loss = 1.10 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 03:02:46.892740: step 78030, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:02:54.005018: step 78040, loss = 1.02 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 03:03:01.159324: step 78050, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 03:03:08.196192: step 78060, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 03:03:15.294559: step 78070, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 03:03:22.419730: step 78080, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:03:29.562119: step 78090, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 03:03:36.636622: step 78100, loss = 1.01 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 03:03:46.303807: step 78110, loss = 1.12 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 03:03:53.384132: step 78120, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 03:04:00.485535: step 78130, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:04:07.618382: step 78140, loss = 1.10 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:04:14.663507: step 78150, loss = 1.08 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 03:04:21.726112: step 78160, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 03:04:28.778374: step 78170, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:04:35.939093: step 78180, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 03:04:42.953122: step 78190, loss = 1.04 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 03:04:50.063886: step 78200, loss = 1.20 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 03:05:00.109433: step 78210, loss = 1.00 (48.0 examples/sec; 0.666 sec/batch)
2018-10-17 03:05:07.176761: step 78220, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:05:14.220834: step 78230, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 03:05:21.387602: step 78240, loss = 1.05 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 03:05:28.440584: step 78250, loss = 1.06 (47.9 examples/sec; 0.669 sec/batch)
2018-10-17 03:05:35.506481: step 78260, loss = 1.03 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:05:42.592067: step 78270, loss = 1.06 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 03:05:49.716799: step 78280, loss = 1.11 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 03:05:56.831241: step 78290, loss = 1.04 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 03:06:04.011280: step 78300, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 03:06:13.711235: step 78310, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:06:20.761623: step 78320, loss = 1.02 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 03:06:27.786413: step 78330, loss = 1.02 (47.6 examples/sec; 0.673 sec/batch)
2018-10-17 03:06:34.835617: step 78340, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 03:06:41.916819: step 78350, loss = 1.01 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 03:06:48.982381: step 78360, loss = 1.07 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 03:06:56.091900: step 78370, loss = 1.06 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 03:07:03.139918: step 78380, loss = 1.02 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 03:07:10.208353: step 78390, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 03:07:17.249613: step 78400, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:07:26.832305: step 78410, loss = 1.11 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 03:07:33.978477: step 78420, loss = 1.01 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 03:07:41.068776: step 78430, loss = 1.13 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 03:07:48.087365: step 78440, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:07:55.176696: step 78450, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 03:08:02.254250: step 78460, loss = 1.14 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 03:08:09.320934: step 78470, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 03:08:16.473775: step 78480, loss = 1.06 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 03:08:23.523210: step 78490, loss = 1.06 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 03:08:30.591614: step 78500, loss = 1.08 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 03:08:41.049875: step 78510, loss = 1.10 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 03:08:48.114033: step 78520, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:08:55.227888: step 78530, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 03:09:02.342045: step 78540, loss = 0.99 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 03:09:09.437954: step 78550, loss = 1.07 (42.7 examples/sec; 0.750 sec/batch)
2018-10-17 03:09:16.485891: step 78560, loss = 1.05 (44.7 examples/sec; 0.717 sec/batch)
2018-10-17 03:09:23.609002: step 78570, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:09:30.696836: step 78580, loss = 1.12 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 03:09:37.755777: step 78590, loss = 1.00 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 03:09:44.828143: step 78600, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:09:54.471387: step 78610, loss = 1.14 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 03:10:01.634218: step 78620, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 03:10:08.687767: step 78630, loss = 1.03 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 03:10:15.763993: step 78640, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 03:10:22.926877: step 78650, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 03:10:30.059972: step 78660, loss = 1.19 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 03:10:37.149651: step 78670, loss = 1.11 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:10:44.313912: step 78680, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:10:51.358979: step 78690, loss = 1.02 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 03:10:58.465918: step 78700, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 03:11:08.676239: step 78710, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 03:11:15.591363: step 78720, loss = 1.12 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 03:11:22.773400: step 78730, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 03:11:29.856385: step 78740, loss = 1.03 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 03:11:36.934924: step 78750, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 03:11:44.120010: step 78760, loss = 0.99 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 03:11:51.177100: step 78770, loss = 1.05 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 03:11:58.277301: step 78780, loss = 1.01 (46.3 examples/sec; 0.690 sec/batch)
2018-10-17 03:12:05.458626: step 78790, loss = 0.99 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 03:12:12.492596: step 78800, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:12:22.097761: step 78810, loss = 1.05 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 03:12:29.169304: step 78820, loss = 1.07 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 03:12:36.229228: step 78830, loss = 1.17 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 03:12:43.410439: step 78840, loss = 1.08 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 03:12:50.535969: step 78850, loss = 1.24 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 03:12:57.587017: step 78860, loss = 1.07 (47.8 examples/sec; 0.670 sec/batch)
2018-10-17 03:13:04.683046: step 78870, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:13:11.815188: step 78880, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:13:18.904873: step 78890, loss = 1.11 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:13:25.965260: step 78900, loss = 1.08 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 03:13:35.613114: step 78910, loss = 1.13 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 03:13:42.678726: step 78920, loss = 1.03 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 03:13:49.801965: step 78930, loss = 1.04 (46.3 examples/sec; 0.690 sec/batch)
2018-10-17 03:13:56.896199: step 78940, loss = 1.05 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 03:14:04.001076: step 78950, loss = 1.17 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:14:11.077173: step 78960, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 03:14:18.228758: step 78970, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 03:14:25.272060: step 78980, loss = 1.00 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 03:14:32.360628: step 78990, loss = 1.03 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 03:14:39.524749: step 79000, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 03:14:49.137416: step 79010, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 03:14:56.226270: step 79020, loss = 1.01 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 03:15:03.309271: step 79030, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 03:15:10.420679: step 79040, loss = 1.01 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 03:15:17.429325: step 79050, loss = 1.04 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 03:15:24.527691: step 79060, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 03:15:31.631276: step 79070, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 03:15:38.767791: step 79080, loss = 1.12 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 03:15:45.913140: step 79090, loss = 0.99 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 03:15:52.967001: step 79100, loss = 1.09 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 03:16:02.872897: step 79110, loss = 1.02 (47.0 examples/sec; 0.680 sec/batch)
2018-10-17 03:16:09.939061: step 79120, loss = 1.12 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 03:16:17.016695: step 79130, loss = 1.02 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 03:16:24.088415: step 79140, loss = 1.10 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 03:16:31.187786: step 79150, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 03:16:38.349460: step 79160, loss = 1.02 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 03:16:45.442369: step 79170, loss = 1.06 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:16:52.572165: step 79180, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 03:16:59.645676: step 79190, loss = 1.10 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 03:17:06.761385: step 79200, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 03:17:16.601390: step 79210, loss = 1.03 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 03:17:23.723430: step 79220, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 03:17:30.805882: step 79230, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 03:17:37.898450: step 79240, loss = 1.10 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 03:17:44.966332: step 79250, loss = 1.16 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:17:52.053320: step 79260, loss = 1.05 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 03:17:59.131235: step 79270, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 03:18:06.248156: step 79280, loss = 0.99 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 03:18:13.330149: step 79290, loss = 1.14 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 03:18:20.365678: step 79300, loss = 1.08 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 03:18:30.040859: step 79310, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:18:37.027690: step 79320, loss = 1.06 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:18:44.141580: step 79330, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 03:18:51.222308: step 79340, loss = 1.09 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 03:18:58.338176: step 79350, loss = 0.99 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 03:19:05.519528: step 79360, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:19:12.533446: step 79370, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 03:19:19.616093: step 79380, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 03:19:26.665528: step 79390, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 03:19:33.747253: step 79400, loss = 1.00 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 03:19:43.390193: step 79410, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:19:50.483647: step 79420, loss = 1.09 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 03:19:57.581474: step 79430, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 03:20:04.626014: step 79440, loss = 1.20 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 03:20:11.729177: step 79450, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 03:20:18.933082: step 79460, loss = 1.06 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 03:20:26.092267: step 79470, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 03:20:33.221598: step 79480, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:20:40.431946: step 79490, loss = 1.16 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 03:20:47.546949: step 79500, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 03:20:57.336474: step 79510, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 03:21:04.492091: step 79520, loss = 1.14 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 03:21:11.533457: step 79530, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 03:21:18.637345: step 79540, loss = 1.19 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 03:21:25.652220: step 79550, loss = 0.99 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:21:32.769690: step 79560, loss = 1.08 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 03:21:39.895290: step 79570, loss = 1.12 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 03:21:46.956116: step 79580, loss = 1.03 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 03:21:53.986383: step 79590, loss = 1.01 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 03:22:01.063570: step 79600, loss = 1.10 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 03:22:10.690419: step 79610, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 03:22:17.734839: step 79620, loss = 1.04 (43.3 examples/sec; 0.738 sec/batch)
2018-10-17 03:22:24.852489: step 79630, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 03:22:31.920516: step 79640, loss = 1.05 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 03:22:38.973867: step 79650, loss = 1.01 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:22:46.062652: step 79660, loss = 1.05 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 03:22:53.192344: step 79670, loss = 1.18 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 03:23:00.300667: step 79680, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:23:07.448462: step 79690, loss = 1.13 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 03:23:14.521295: step 79700, loss = 1.06 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 03:23:24.290587: step 79710, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 03:23:31.406029: step 79720, loss = 1.03 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 03:23:38.511085: step 79730, loss = 0.99 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 03:23:45.590401: step 79740, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 03:23:52.741163: step 79750, loss = 1.00 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 03:23:59.791249: step 79760, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 03:24:06.877372: step 79770, loss = 1.08 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 03:24:13.938922: step 79780, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:24:21.125614: step 79790, loss = 1.06 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 03:24:28.152067: step 79800, loss = 1.04 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 03:24:37.819703: step 79810, loss = 1.09 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 03:24:44.820465: step 79820, loss = 1.02 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 03:24:51.986164: step 79830, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 03:24:59.107254: step 79840, loss = 1.21 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 03:25:06.282619: step 79850, loss = 1.08 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 03:25:13.391446: step 79860, loss = 1.04 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 03:25:20.483962: step 79870, loss = 1.14 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 03:25:27.618191: step 79880, loss = 1.10 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 03:25:34.686475: step 79890, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:25:41.833004: step 79900, loss = 0.99 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 03:25:51.535777: step 79910, loss = 1.04 (47.8 examples/sec; 0.669 sec/batch)
2018-10-17 03:25:58.532622: step 79920, loss = 1.10 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 03:26:05.615358: step 79930, loss = 1.16 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 03:26:12.649485: step 79940, loss = 1.13 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 03:26:19.791627: step 79950, loss = 1.09 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 03:26:26.910323: step 79960, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 03:26:33.998677: step 79970, loss = 1.11 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 03:26:41.061327: step 79980, loss = 1.01 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 03:26:48.166702: step 79990, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 03:26:55.279109: step 80000, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 03:27:07.822783: step 80010, loss = 1.04 (48.5 examples/sec; 0.660 sec/batch)
2018-10-17 03:27:14.497378: step 80020, loss = 1.21 (47.6 examples/sec; 0.672 sec/batch)
2018-10-17 03:27:21.620594: step 80030, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 03:27:28.795874: step 80040, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 03:27:35.844731: step 80050, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:27:42.961392: step 80060, loss = 1.09 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 03:27:50.132423: step 80070, loss = 1.01 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 03:27:57.247971: step 80080, loss = 1.16 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 03:28:04.339150: step 80090, loss = 1.07 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 03:28:11.434769: step 80100, loss = 1.06 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 03:28:21.174436: step 80110, loss = 1.04 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 03:28:28.240424: step 80120, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:28:35.346964: step 80130, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:28:42.504301: step 80140, loss = 1.04 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 03:28:49.557984: step 80150, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 03:28:56.641035: step 80160, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 03:29:03.740625: step 80170, loss = 1.04 (42.8 examples/sec; 0.747 sec/batch)
2018-10-17 03:29:10.856510: step 80180, loss = 1.08 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:29:17.909975: step 80190, loss = 1.11 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 03:29:24.977338: step 80200, loss = 1.04 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 03:29:34.914421: step 80210, loss = 1.00 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 03:29:42.063737: step 80220, loss = 1.07 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 03:29:49.142142: step 80230, loss = 1.05 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 03:29:56.274209: step 80240, loss = 1.00 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 03:30:03.340789: step 80250, loss = 1.07 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 03:30:10.468665: step 80260, loss = 1.01 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 03:30:17.515394: step 80270, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 03:30:24.575977: step 80280, loss = 1.00 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 03:30:31.663136: step 80290, loss = 1.13 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 03:30:38.727706: step 80300, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 03:30:48.471658: step 80310, loss = 1.11 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 03:30:55.621135: step 80320, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 03:31:02.720461: step 80330, loss = 1.06 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 03:31:09.912329: step 80340, loss = 1.00 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 03:31:16.954629: step 80350, loss = 0.99 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 03:31:23.965886: step 80360, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 03:31:31.117389: step 80370, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 03:31:38.220519: step 80380, loss = 1.05 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 03:31:45.343934: step 80390, loss = 0.99 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 03:31:52.536511: step 80400, loss = 1.13 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 03:32:02.174148: step 80410, loss = 1.18 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 03:32:09.188069: step 80420, loss = 1.15 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 03:32:16.289430: step 80430, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:32:23.288999: step 80440, loss = 1.00 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 03:32:30.419603: step 80450, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:32:37.524346: step 80460, loss = 1.11 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 03:32:44.606501: step 80470, loss = 1.10 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:32:51.756972: step 80480, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 03:32:58.899705: step 80490, loss = 1.03 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 03:33:05.966710: step 80500, loss = 1.21 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 03:33:15.570468: step 80510, loss = 1.06 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:33:22.606393: step 80520, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 03:33:29.703304: step 80530, loss = 1.02 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 03:33:36.787247: step 80540, loss = 1.12 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:33:43.853492: step 80550, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 03:33:50.881080: step 80560, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 03:33:57.979909: step 80570, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:34:05.047372: step 80580, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 03:34:12.220044: step 80590, loss = 1.21 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 03:34:19.293351: step 80600, loss = 1.15 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 03:34:29.140931: step 80610, loss = 1.00 (47.5 examples/sec; 0.674 sec/batch)
2018-10-17 03:34:36.194777: step 80620, loss = 1.07 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 03:34:43.268172: step 80630, loss = 1.00 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 03:34:50.401848: step 80640, loss = 1.05 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 03:34:57.373720: step 80650, loss = 1.09 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 03:35:04.407409: step 80660, loss = 1.01 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 03:35:11.635696: step 80670, loss = 1.01 (45.6 examples/sec; 0.703 sec/batch)
2018-10-17 03:35:18.680923: step 80680, loss = 1.05 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 03:35:25.656960: step 80690, loss = 1.07 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 03:35:32.759755: step 80700, loss = 1.03 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 03:35:42.403685: step 80710, loss = 1.14 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 03:35:49.567833: step 80720, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 03:35:56.637609: step 80730, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 03:36:03.719171: step 80740, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 03:36:10.879349: step 80750, loss = 1.18 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 03:36:17.969828: step 80760, loss = 1.06 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 03:36:25.044578: step 80770, loss = 1.03 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 03:36:32.147634: step 80780, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 03:36:39.210759: step 80790, loss = 1.11 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 03:36:46.303747: step 80800, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 03:36:55.912802: step 80810, loss = 1.11 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:37:02.948166: step 80820, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 03:37:10.064001: step 80830, loss = 1.00 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 03:37:17.132343: step 80840, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 03:37:24.264977: step 80850, loss = 1.04 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 03:37:31.403046: step 80860, loss = 1.20 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 03:37:38.430262: step 80870, loss = 1.02 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 03:37:45.531010: step 80880, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 03:37:52.623774: step 80890, loss = 1.02 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 03:37:59.718759: step 80900, loss = 0.99 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 03:38:09.454846: step 80910, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 03:38:16.494848: step 80920, loss = 1.02 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 03:38:23.542454: step 80930, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:38:30.703063: step 80940, loss = 1.05 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 03:38:37.710014: step 80950, loss = 1.05 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 03:38:44.810963: step 80960, loss = 1.09 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 03:38:51.899069: step 80970, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:38:58.957735: step 80980, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 03:39:06.016924: step 80990, loss = 1.09 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 03:39:13.166578: step 81000, loss = 1.16 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 03:39:22.759178: step 81010, loss = 1.02 (46.5 examples/sec; 0.687 sec/batch)
2018-10-17 03:39:29.846571: step 81020, loss = 1.12 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:39:36.927690: step 81030, loss = 1.08 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 03:39:44.010093: step 81040, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:39:51.119534: step 81050, loss = 1.06 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 03:39:58.154984: step 81060, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:40:05.243973: step 81070, loss = 1.05 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 03:40:12.332879: step 81080, loss = 1.05 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 03:40:19.448120: step 81090, loss = 1.02 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 03:40:26.523534: step 81100, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:40:36.774940: step 81110, loss = 1.03 (47.4 examples/sec; 0.674 sec/batch)
2018-10-17 03:40:43.827496: step 81120, loss = 1.08 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 03:40:50.933310: step 81130, loss = 1.00 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 03:40:57.996297: step 81140, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:41:05.158817: step 81150, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:41:12.277723: step 81160, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:41:19.303374: step 81170, loss = 1.03 (43.6 examples/sec; 0.735 sec/batch)
2018-10-17 03:41:26.347165: step 81180, loss = 1.19 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:41:33.446063: step 81190, loss = 1.11 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 03:41:40.467651: step 81200, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 03:41:50.098950: step 81210, loss = 1.09 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 03:41:57.258453: step 81220, loss = 1.01 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 03:42:04.309258: step 81230, loss = 1.04 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 03:42:11.343479: step 81240, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 03:42:18.369580: step 81250, loss = 1.12 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 03:42:25.481149: step 81260, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 03:42:32.642614: step 81270, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:42:39.796912: step 81280, loss = 1.12 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 03:42:46.959562: step 81290, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 03:42:54.082305: step 81300, loss = 1.03 (43.0 examples/sec; 0.743 sec/batch)
2018-10-17 03:43:04.155676: step 81310, loss = 1.14 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 03:43:11.205564: step 81320, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 03:43:18.257536: step 81330, loss = 1.14 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 03:43:25.382360: step 81340, loss = 1.17 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 03:43:32.563355: step 81350, loss = 1.08 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 03:43:39.557226: step 81360, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 03:43:46.628665: step 81370, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 03:43:53.747088: step 81380, loss = 1.02 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 03:44:00.724542: step 81390, loss = 1.20 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:44:07.893359: step 81400, loss = 1.02 (44.0 examples/sec; 0.726 sec/batch)
2018-10-17 03:44:17.552382: step 81410, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:44:24.616297: step 81420, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 03:44:31.682342: step 81430, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 03:44:38.824375: step 81440, loss = 1.01 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 03:44:45.940768: step 81450, loss = 1.07 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 03:44:53.056736: step 81460, loss = 1.11 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 03:45:00.110242: step 81470, loss = 1.00 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 03:45:07.118311: step 81480, loss = 1.11 (46.8 examples/sec; 0.683 sec/batch)
2018-10-17 03:45:14.203656: step 81490, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:45:21.247981: step 81500, loss = 1.03 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 03:45:30.954355: step 81510, loss = 1.03 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 03:45:38.006349: step 81520, loss = 1.15 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 03:45:45.089325: step 81530, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 03:45:52.234366: step 81540, loss = 1.05 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 03:45:59.350699: step 81550, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 03:46:06.378985: step 81560, loss = 1.16 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 03:46:13.417092: step 81570, loss = 1.03 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 03:46:20.504265: step 81580, loss = 1.14 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 03:46:27.586056: step 81590, loss = 1.08 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:46:34.632172: step 81600, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 03:46:44.255516: step 81610, loss = 1.04 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 03:46:51.293454: step 81620, loss = 1.12 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 03:46:58.320235: step 81630, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 03:47:05.328193: step 81640, loss = 1.01 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 03:47:12.442100: step 81650, loss = 1.04 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 03:47:19.495453: step 81660, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:47:26.641580: step 81670, loss = 1.07 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 03:47:33.698536: step 81680, loss = 1.08 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 03:47:40.825030: step 81690, loss = 1.11 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:47:47.813015: step 81700, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 03:47:57.437577: step 81710, loss = 1.07 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 03:48:04.448328: step 81720, loss = 1.10 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 03:48:11.604588: step 81730, loss = 0.99 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 03:48:18.642234: step 81740, loss = 1.16 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 03:48:25.742367: step 81750, loss = 1.03 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 03:48:32.801946: step 81760, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 03:48:39.834671: step 81770, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:48:46.912637: step 81780, loss = 1.20 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 03:48:53.953405: step 81790, loss = 1.12 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 03:49:01.112269: step 81800, loss = 1.10 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 03:49:11.383943: step 81810, loss = 1.11 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 03:49:18.374932: step 81820, loss = 1.16 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 03:49:25.556144: step 81830, loss = 1.13 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:49:32.567051: step 81840, loss = 1.09 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 03:49:39.643886: step 81850, loss = 1.24 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:49:46.719709: step 81860, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:49:53.727127: step 81870, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:50:00.891749: step 81880, loss = 1.06 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 03:50:07.991622: step 81890, loss = 1.03 (44.5 examples/sec; 0.718 sec/batch)
2018-10-17 03:50:15.066629: step 81900, loss = 1.06 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 03:50:24.802028: step 81910, loss = 1.11 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:50:31.969456: step 81920, loss = 1.08 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 03:50:39.039888: step 81930, loss = 1.00 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 03:50:46.077481: step 81940, loss = 1.00 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 03:50:53.184677: step 81950, loss = 1.10 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 03:51:00.297186: step 81960, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 03:51:07.396055: step 81970, loss = 1.15 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 03:51:14.423409: step 81980, loss = 1.07 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:51:21.548970: step 81990, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 03:51:28.654422: step 82000, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 03:51:38.655883: step 82010, loss = 1.01 (48.2 examples/sec; 0.663 sec/batch)
2018-10-17 03:51:45.735393: step 82020, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 03:51:52.813299: step 82030, loss = 1.00 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 03:51:59.953719: step 82040, loss = 1.00 (44.5 examples/sec; 0.718 sec/batch)
2018-10-17 03:52:07.093998: step 82050, loss = 1.27 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 03:52:14.240274: step 82060, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 03:52:21.337848: step 82070, loss = 0.99 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 03:52:28.389128: step 82080, loss = 0.99 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 03:52:35.408566: step 82090, loss = 1.02 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 03:52:42.475405: step 82100, loss = 1.02 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 03:52:52.304810: step 82110, loss = 1.03 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 03:52:59.371768: step 82120, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 03:53:06.534908: step 82130, loss = 1.12 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 03:53:13.681238: step 82140, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:53:20.781727: step 82150, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 03:53:27.847676: step 82160, loss = 1.07 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 03:53:34.996324: step 82170, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 03:53:42.009794: step 82180, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 03:53:49.087298: step 82190, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 03:53:56.109547: step 82200, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 03:54:06.284203: step 82210, loss = 1.05 (47.6 examples/sec; 0.672 sec/batch)
2018-10-17 03:54:13.312756: step 82220, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 03:54:20.412787: step 82230, loss = 1.01 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 03:54:27.438517: step 82240, loss = 1.08 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 03:54:34.457215: step 82250, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 03:54:41.546665: step 82260, loss = 1.04 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 03:54:48.646363: step 82270, loss = 1.04 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 03:54:55.735773: step 82280, loss = 1.30 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:55:02.806647: step 82290, loss = 1.07 (47.8 examples/sec; 0.669 sec/batch)
2018-10-17 03:55:09.846934: step 82300, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:55:19.545830: step 82310, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 03:55:26.542333: step 82320, loss = 1.13 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 03:55:33.672777: step 82330, loss = 1.10 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 03:55:40.692847: step 82340, loss = 1.19 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 03:55:47.676321: step 82350, loss = 1.11 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 03:55:54.791916: step 82360, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 03:56:01.897300: step 82370, loss = 1.02 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 03:56:08.962765: step 82380, loss = 1.06 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 03:56:16.118727: step 82390, loss = 1.03 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 03:56:23.138658: step 82400, loss = 1.15 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 03:56:32.660519: step 82410, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 03:56:39.749217: step 82420, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:56:46.883790: step 82430, loss = 1.19 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 03:56:53.895181: step 82440, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 03:57:01.028434: step 82450, loss = 1.01 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 03:57:08.045878: step 82460, loss = 1.02 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 03:57:15.122559: step 82470, loss = 1.04 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 03:57:22.177860: step 82480, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 03:57:29.416184: step 82490, loss = 1.09 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 03:57:36.504609: step 82500, loss = 1.00 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 03:57:46.087663: step 82510, loss = 1.16 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 03:57:53.159038: step 82520, loss = 1.17 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:58:00.220134: step 82530, loss = 1.07 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 03:58:07.155025: step 82540, loss = 1.00 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 03:58:14.169122: step 82550, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:58:21.332737: step 82560, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 03:58:28.407959: step 82570, loss = 1.02 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 03:58:35.499684: step 82580, loss = 1.03 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 03:58:42.556908: step 82590, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 03:58:49.635386: step 82600, loss = 1.08 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 03:58:59.302018: step 82610, loss = 1.00 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 03:59:06.341654: step 82620, loss = 1.09 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 03:59:13.350163: step 82630, loss = 1.09 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 03:59:20.463145: step 82640, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 03:59:27.455931: step 82650, loss = 1.15 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 03:59:34.538271: step 82660, loss = 1.08 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 03:59:41.549109: step 82670, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 03:59:48.510550: step 82680, loss = 1.05 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 03:59:55.580121: step 82690, loss = 1.08 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 04:00:02.582637: step 82700, loss = 1.05 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 04:00:12.143000: step 82710, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:00:19.195111: step 82720, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 04:00:26.258787: step 82730, loss = 1.14 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:00:33.313080: step 82740, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:00:40.351037: step 82750, loss = 1.11 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 04:00:47.425737: step 82760, loss = 1.04 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 04:00:54.488682: step 82770, loss = 1.09 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 04:01:01.502696: step 82780, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:01:08.508856: step 82790, loss = 1.01 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 04:01:15.621694: step 82800, loss = 1.08 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:01:25.186000: step 82810, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:01:32.218686: step 82820, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:01:39.266801: step 82830, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 04:01:46.250337: step 82840, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:01:53.296022: step 82850, loss = 1.05 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 04:02:00.281884: step 82860, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 04:02:07.280611: step 82870, loss = 1.11 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 04:02:14.366014: step 82880, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:02:21.421306: step 82890, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 04:02:28.388905: step 82900, loss = 1.03 (47.5 examples/sec; 0.674 sec/batch)
2018-10-17 04:02:37.987240: step 82910, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:02:44.995272: step 82920, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:02:52.061349: step 82930, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 04:02:59.022491: step 82940, loss = 1.04 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:03:06.050052: step 82950, loss = 1.03 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 04:03:13.091187: step 82960, loss = 1.01 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 04:03:20.203711: step 82970, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 04:03:27.238687: step 82980, loss = 1.07 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 04:03:34.279406: step 82990, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:03:41.297916: step 83000, loss = 1.05 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 04:03:51.090520: step 83010, loss = 1.02 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 04:03:58.190377: step 83020, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 04:04:05.265787: step 83030, loss = 1.03 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 04:04:12.320659: step 83040, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 04:04:19.442814: step 83050, loss = 1.18 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 04:04:26.514799: step 83060, loss = 1.00 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 04:04:33.604176: step 83070, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:04:40.655992: step 83080, loss = 1.02 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 04:04:47.704240: step 83090, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 04:04:54.693870: step 83100, loss = 1.20 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 04:05:04.307381: step 83110, loss = 1.00 (48.0 examples/sec; 0.667 sec/batch)
2018-10-17 04:05:11.320519: step 83120, loss = 0.99 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 04:05:18.350477: step 83130, loss = 0.99 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 04:05:25.395387: step 83140, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 04:05:32.562663: step 83150, loss = 1.06 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 04:05:39.568898: step 83160, loss = 0.99 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 04:05:46.613889: step 83170, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:05:53.746356: step 83180, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 04:06:00.796472: step 83190, loss = 1.18 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 04:06:07.842925: step 83200, loss = 1.02 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 04:06:17.578976: step 83210, loss = 1.18 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:06:24.608575: step 83220, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 04:06:31.666468: step 83230, loss = 1.09 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 04:06:38.796635: step 83240, loss = 1.09 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 04:06:45.800966: step 83250, loss = 1.04 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 04:06:52.851542: step 83260, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:06:59.919963: step 83270, loss = 1.09 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:07:07.053971: step 83280, loss = 1.03 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 04:07:14.012562: step 83290, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:07:21.036165: step 83300, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:07:30.875192: step 83310, loss = 1.06 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 04:07:37.896135: step 83320, loss = 1.17 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 04:07:45.003025: step 83330, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 04:07:52.119069: step 83340, loss = 1.13 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 04:07:59.181544: step 83350, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:08:06.277587: step 83360, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 04:08:13.406493: step 83370, loss = 1.11 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 04:08:20.449536: step 83380, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 04:08:27.509540: step 83390, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 04:08:34.613525: step 83400, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 04:08:44.353281: step 83410, loss = 0.99 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 04:08:51.380539: step 83420, loss = 1.19 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:08:58.491885: step 83430, loss = 1.12 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 04:09:05.559196: step 83440, loss = 1.31 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 04:09:12.589404: step 83450, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 04:09:19.713354: step 83460, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 04:09:26.809712: step 83470, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 04:09:33.863806: step 83480, loss = 1.12 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 04:09:40.899389: step 83490, loss = 1.13 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 04:09:47.973835: step 83500, loss = 1.03 (47.5 examples/sec; 0.673 sec/batch)
2018-10-17 04:09:57.563445: step 83510, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 04:10:04.551443: step 83520, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:10:11.612011: step 83530, loss = 1.11 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:10:18.745575: step 83540, loss = 0.99 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 04:10:25.879144: step 83550, loss = 0.99 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 04:10:32.915795: step 83560, loss = 1.03 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 04:10:39.976535: step 83570, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 04:10:47.068540: step 83580, loss = 1.00 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 04:10:54.067831: step 83590, loss = 1.03 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 04:11:01.101108: step 83600, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 04:11:11.811323: step 83610, loss = 1.10 (47.9 examples/sec; 0.668 sec/batch)
2018-10-17 04:11:18.791488: step 83620, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 04:11:25.956782: step 83630, loss = 1.15 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 04:11:33.094751: step 83640, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 04:11:40.135100: step 83650, loss = 1.10 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 04:11:47.222572: step 83660, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 04:11:54.249971: step 83670, loss = 1.11 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 04:12:01.401712: step 83680, loss = 0.99 (43.3 examples/sec; 0.740 sec/batch)
2018-10-17 04:12:08.507616: step 83690, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:12:15.562783: step 83700, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 04:12:25.678562: step 83710, loss = 1.00 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 04:12:32.733105: step 83720, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 04:12:39.812852: step 83730, loss = 0.99 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 04:12:46.865949: step 83740, loss = 1.04 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 04:12:53.986034: step 83750, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 04:13:01.047354: step 83760, loss = 1.20 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 04:13:08.191627: step 83770, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 04:13:15.163679: step 83780, loss = 1.23 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 04:13:22.304586: step 83790, loss = 1.06 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 04:13:29.375361: step 83800, loss = 1.00 (42.9 examples/sec; 0.746 sec/batch)
2018-10-17 04:13:38.904663: step 83810, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:13:46.001336: step 83820, loss = 1.09 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 04:13:53.095099: step 83830, loss = 1.12 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 04:14:00.207868: step 83840, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 04:14:07.326324: step 83850, loss = 1.04 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 04:14:14.390301: step 83860, loss = 1.17 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:14:21.419829: step 83870, loss = 1.27 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 04:14:28.507374: step 83880, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 04:14:35.537886: step 83890, loss = 1.06 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 04:14:42.545190: step 83900, loss = 1.05 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 04:14:52.332708: step 83910, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:14:59.321437: step 83920, loss = 0.99 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 04:15:06.398101: step 83930, loss = 1.01 (47.5 examples/sec; 0.674 sec/batch)
2018-10-17 04:15:13.478516: step 83940, loss = 1.02 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 04:15:20.551628: step 83950, loss = 1.01 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 04:15:27.695937: step 83960, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 04:15:34.819411: step 83970, loss = 1.04 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 04:15:41.889788: step 83980, loss = 1.13 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 04:15:48.991276: step 83990, loss = 1.01 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 04:15:56.128258: step 84000, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 04:16:05.685385: step 84010, loss = 1.05 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 04:16:12.771196: step 84020, loss = 1.00 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 04:16:19.900055: step 84030, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 04:16:26.942389: step 84040, loss = 1.01 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 04:16:34.046169: step 84050, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 04:16:41.160079: step 84060, loss = 1.05 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 04:16:48.168937: step 84070, loss = 1.05 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 04:16:55.224131: step 84080, loss = 1.16 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 04:17:02.337006: step 84090, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 04:17:09.539971: step 84100, loss = 1.03 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 04:17:19.340780: step 84110, loss = 1.00 (47.6 examples/sec; 0.672 sec/batch)
2018-10-17 04:17:26.395464: step 84120, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 04:17:33.518537: step 84130, loss = 1.02 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 04:17:40.663129: step 84140, loss = 1.05 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 04:17:47.725640: step 84150, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 04:17:54.873057: step 84160, loss = 1.04 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 04:18:01.906139: step 84170, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 04:18:08.960205: step 84180, loss = 1.08 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:18:15.978159: step 84190, loss = 1.00 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 04:18:23.096867: step 84200, loss = 0.99 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 04:18:32.763391: step 84210, loss = 1.00 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 04:18:39.846834: step 84220, loss = 1.05 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 04:18:46.867224: step 84230, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:18:53.966152: step 84240, loss = 1.07 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 04:19:01.052674: step 84250, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 04:19:08.191294: step 84260, loss = 1.00 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 04:19:15.289420: step 84270, loss = 1.01 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 04:19:22.396641: step 84280, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:19:29.463535: step 84290, loss = 1.20 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 04:19:36.499519: step 84300, loss = 1.08 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 04:19:46.095818: step 84310, loss = 1.03 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 04:19:53.211493: step 84320, loss = 1.06 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 04:20:00.218081: step 84330, loss = 1.29 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 04:20:07.299278: step 84340, loss = 1.28 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 04:20:14.386382: step 84350, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:20:21.433493: step 84360, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 04:20:28.541210: step 84370, loss = 1.15 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 04:20:35.573836: step 84380, loss = 1.01 (46.1 examples/sec; 0.693 sec/batch)
2018-10-17 04:20:42.662561: step 84390, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 04:20:49.690851: step 84400, loss = 1.00 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 04:20:59.325228: step 84410, loss = 1.07 (46.8 examples/sec; 0.683 sec/batch)
2018-10-17 04:21:06.463492: step 84420, loss = 0.99 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 04:21:13.511440: step 84430, loss = 1.13 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:21:20.656261: step 84440, loss = 1.05 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 04:21:27.775088: step 84450, loss = 1.16 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 04:21:34.849126: step 84460, loss = 1.15 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 04:21:41.921790: step 84470, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:21:48.951897: step 84480, loss = 1.00 (47.3 examples/sec; 0.676 sec/batch)
2018-10-17 04:21:55.951830: step 84490, loss = 1.16 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 04:22:03.048118: step 84500, loss = 1.03 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 04:22:12.641852: step 84510, loss = 1.14 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 04:22:19.708031: step 84520, loss = 1.14 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:22:26.799117: step 84530, loss = 1.14 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 04:22:33.801878: step 84540, loss = 1.13 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 04:22:40.932029: step 84550, loss = 1.05 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 04:22:48.056616: step 84560, loss = 1.05 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 04:22:55.208782: step 84570, loss = 1.05 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 04:23:02.221528: step 84580, loss = 1.02 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 04:23:09.311783: step 84590, loss = 1.13 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 04:23:16.337958: step 84600, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:23:26.003852: step 84610, loss = 1.21 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 04:23:33.058965: step 84620, loss = 1.04 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:23:40.143372: step 84630, loss = 1.05 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 04:23:47.212758: step 84640, loss = 1.13 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:23:54.344433: step 84650, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 04:24:01.395643: step 84660, loss = 1.03 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:24:08.486430: step 84670, loss = 1.18 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:24:15.618188: step 84680, loss = 1.01 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 04:24:22.750528: step 84690, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 04:24:29.910148: step 84700, loss = 0.99 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 04:24:39.574995: step 84710, loss = 1.12 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 04:24:46.717307: step 84720, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 04:24:53.736101: step 84730, loss = 1.15 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 04:25:00.796332: step 84740, loss = 1.02 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 04:25:07.914044: step 84750, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 04:25:14.900676: step 84760, loss = 0.99 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 04:25:21.977707: step 84770, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 04:25:29.053986: step 84780, loss = 1.26 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 04:25:36.118899: step 84790, loss = 1.16 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:25:43.222656: step 84800, loss = 1.04 (45.6 examples/sec; 0.703 sec/batch)
2018-10-17 04:25:53.068067: step 84810, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 04:26:00.133411: step 84820, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:26:07.136935: step 84830, loss = 1.05 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 04:26:14.216928: step 84840, loss = 1.01 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 04:26:21.236575: step 84850, loss = 1.01 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 04:26:28.386487: step 84860, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:26:35.432492: step 84870, loss = 1.02 (47.8 examples/sec; 0.670 sec/batch)
2018-10-17 04:26:42.544523: step 84880, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:26:49.641465: step 84890, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 04:26:56.764815: step 84900, loss = 1.12 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 04:27:07.074375: step 84910, loss = 0.99 (47.8 examples/sec; 0.669 sec/batch)
2018-10-17 04:27:14.023677: step 84920, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 04:27:21.077125: step 84930, loss = 1.08 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 04:27:28.120074: step 84940, loss = 1.02 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 04:27:35.166009: step 84950, loss = 1.06 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:27:42.290425: step 84960, loss = 1.07 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:27:49.409877: step 84970, loss = 1.00 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 04:27:56.485062: step 84980, loss = 1.01 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 04:28:03.571360: step 84990, loss = 1.11 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 04:28:10.611086: step 85000, loss = 1.10 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 04:28:23.914172: step 85010, loss = 1.03 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 04:28:30.721138: step 85020, loss = 1.05 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 04:28:37.645326: step 85030, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 04:28:44.770803: step 85040, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 04:28:51.896100: step 85050, loss = 1.01 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 04:28:58.953774: step 85060, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:29:06.122741: step 85070, loss = 1.06 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 04:29:13.245535: step 85080, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 04:29:20.295612: step 85090, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 04:29:27.354611: step 85100, loss = 1.03 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 04:29:36.964728: step 85110, loss = 0.99 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 04:29:43.979306: step 85120, loss = 1.21 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 04:29:51.072474: step 85130, loss = 1.01 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 04:29:58.142569: step 85140, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 04:30:05.233667: step 85150, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:30:12.351210: step 85160, loss = 1.14 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 04:30:19.464084: step 85170, loss = 1.00 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 04:30:26.702627: step 85180, loss = 1.02 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 04:30:33.762923: step 85190, loss = 1.09 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:30:40.826296: step 85200, loss = 1.11 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 04:30:50.580995: step 85210, loss = 1.07 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 04:30:57.629664: step 85220, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 04:31:04.702218: step 85230, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 04:31:11.797635: step 85240, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 04:31:18.886870: step 85250, loss = 1.05 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 04:31:25.956680: step 85260, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 04:31:33.073503: step 85270, loss = 1.07 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 04:31:40.201979: step 85280, loss = 1.05 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:31:47.274143: step 85290, loss = 1.01 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 04:31:54.393324: step 85300, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 04:32:04.357814: step 85310, loss = 1.07 (47.9 examples/sec; 0.668 sec/batch)
2018-10-17 04:32:11.384676: step 85320, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:32:18.466530: step 85330, loss = 1.04 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 04:32:25.555170: step 85340, loss = 1.01 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 04:32:32.564509: step 85350, loss = 1.07 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 04:32:39.592302: step 85360, loss = 1.24 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 04:32:46.673100: step 85370, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 04:32:53.722315: step 85380, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 04:33:00.773161: step 85390, loss = 0.99 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 04:33:07.838282: step 85400, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 04:33:17.486791: step 85410, loss = 1.07 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:33:24.605782: step 85420, loss = 1.16 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 04:33:31.621873: step 85430, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 04:33:38.667002: step 85440, loss = 1.03 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 04:33:45.797617: step 85450, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 04:33:52.944860: step 85460, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:34:00.005959: step 85470, loss = 0.99 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 04:34:07.052450: step 85480, loss = 1.05 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 04:34:14.039940: step 85490, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 04:34:21.196370: step 85500, loss = 1.17 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 04:34:30.895461: step 85510, loss = 1.07 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 04:34:38.021089: step 85520, loss = 1.02 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 04:34:44.968875: step 85530, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:34:52.084881: step 85540, loss = 1.04 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 04:34:59.187388: step 85550, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 04:35:06.246131: step 85560, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 04:35:13.368130: step 85570, loss = 1.10 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 04:35:20.448526: step 85580, loss = 1.17 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 04:35:27.539147: step 85590, loss = 1.05 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 04:35:34.600442: step 85600, loss = 1.06 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:35:44.339318: step 85610, loss = 1.11 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 04:35:51.472556: step 85620, loss = 1.04 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 04:35:58.482741: step 85630, loss = 1.00 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 04:36:05.596071: step 85640, loss = 1.11 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:36:12.716969: step 85650, loss = 1.10 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 04:36:19.805612: step 85660, loss = 1.03 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 04:36:26.940346: step 85670, loss = 1.02 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 04:36:33.979845: step 85680, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 04:36:41.146914: step 85690, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 04:36:48.277748: step 85700, loss = 1.09 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 04:36:58.283671: step 85710, loss = 1.01 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 04:37:05.399548: step 85720, loss = 1.03 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 04:37:12.420689: step 85730, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:37:19.452150: step 85740, loss = 1.06 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 04:37:26.469983: step 85750, loss = 1.08 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 04:37:33.596923: step 85760, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 04:37:40.585317: step 85770, loss = 1.25 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 04:37:47.703602: step 85780, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 04:37:54.863457: step 85790, loss = 1.07 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 04:38:01.958397: step 85800, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 04:38:11.857054: step 85810, loss = 1.15 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 04:38:18.928756: step 85820, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 04:38:26.004986: step 85830, loss = 1.13 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 04:38:33.028728: step 85840, loss = 0.99 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 04:38:40.127025: step 85850, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 04:38:47.250149: step 85860, loss = 1.18 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 04:38:54.272119: step 85870, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:39:01.359645: step 85880, loss = 1.06 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 04:39:08.426324: step 85890, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 04:39:15.546841: step 85900, loss = 1.01 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 04:39:25.123746: step 85910, loss = 0.99 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 04:39:32.116773: step 85920, loss = 1.00 (47.2 examples/sec; 0.677 sec/batch)
2018-10-17 04:39:39.145906: step 85930, loss = 1.02 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 04:39:46.296766: step 85940, loss = 1.05 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 04:39:53.339154: step 85950, loss = 0.99 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 04:40:00.376921: step 85960, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 04:40:07.396213: step 85970, loss = 1.04 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 04:40:14.389056: step 85980, loss = 1.03 (47.3 examples/sec; 0.677 sec/batch)
2018-10-17 04:40:21.498898: step 85990, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 04:40:28.550967: step 86000, loss = 1.08 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 04:40:38.162349: step 86010, loss = 1.00 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 04:40:45.209611: step 86020, loss = 1.05 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:40:52.352489: step 86030, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:40:59.385234: step 86040, loss = 1.12 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 04:41:06.469242: step 86050, loss = 1.03 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 04:41:13.547525: step 86060, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 04:41:20.629809: step 86070, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:41:27.777981: step 86080, loss = 1.09 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 04:41:34.784089: step 86090, loss = 1.07 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:41:41.963302: step 86100, loss = 1.09 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 04:41:51.563073: step 86110, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 04:41:58.565882: step 86120, loss = 1.01 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 04:42:05.618578: step 86130, loss = 1.05 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 04:42:12.745881: step 86140, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:42:19.744690: step 86150, loss = 1.13 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 04:42:26.807438: step 86160, loss = 1.07 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 04:42:33.838471: step 86170, loss = 1.20 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 04:42:40.892712: step 86180, loss = 1.08 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:42:47.914942: step 86190, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:42:55.006205: step 86200, loss = 1.06 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 04:43:04.518204: step 86210, loss = 1.03 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 04:43:11.645290: step 86220, loss = 1.04 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 04:43:18.663788: step 86230, loss = 1.09 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 04:43:25.815261: step 86240, loss = 1.06 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 04:43:32.874558: step 86250, loss = 1.15 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 04:43:39.969831: step 86260, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:43:47.022503: step 86270, loss = 1.08 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 04:43:54.113323: step 86280, loss = 1.02 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 04:44:01.235150: step 86290, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 04:44:08.244613: step 86300, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 04:44:17.885854: step 86310, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 04:44:25.016121: step 86320, loss = 1.06 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 04:44:32.062416: step 86330, loss = 1.06 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 04:44:39.086755: step 86340, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:44:46.161853: step 86350, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 04:44:53.217935: step 86360, loss = 1.09 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 04:45:00.294290: step 86370, loss = 1.06 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 04:45:07.339755: step 86380, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 04:45:14.354461: step 86390, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 04:45:21.399705: step 86400, loss = 1.12 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:45:31.113238: step 86410, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 04:45:38.165124: step 86420, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 04:45:45.266080: step 86430, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 04:45:52.320171: step 86440, loss = 1.11 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 04:45:59.341237: step 86450, loss = 1.05 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 04:46:06.409347: step 86460, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 04:46:13.570486: step 86470, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 04:46:20.662074: step 86480, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:46:27.734303: step 86490, loss = 1.12 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 04:46:34.833889: step 86500, loss = 1.01 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 04:46:44.846806: step 86510, loss = 1.08 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 04:46:51.918623: step 86520, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 04:46:58.936325: step 86530, loss = 1.10 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 04:47:06.017336: step 86540, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 04:47:13.115477: step 86550, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:47:20.249380: step 86560, loss = 1.00 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 04:47:27.212550: step 86570, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 04:47:34.320390: step 86580, loss = 1.34 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 04:47:41.431837: step 86590, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 04:47:48.540168: step 86600, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:47:58.174349: step 86610, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:48:05.189602: step 86620, loss = 1.02 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 04:48:12.258356: step 86630, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 04:48:19.364079: step 86640, loss = 1.03 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 04:48:26.421767: step 86650, loss = 1.10 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 04:48:33.518513: step 86660, loss = 1.03 (45.7 examples/sec; 0.699 sec/batch)
2018-10-17 04:48:40.544000: step 86670, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 04:48:47.639824: step 86680, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 04:48:54.622238: step 86690, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:49:01.660185: step 86700, loss = 1.18 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 04:49:11.284615: step 86710, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:49:18.333219: step 86720, loss = 1.06 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 04:49:25.401527: step 86730, loss = 1.11 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 04:49:32.418353: step 86740, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 04:49:39.450942: step 86750, loss = 1.05 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 04:49:46.510822: step 86760, loss = 1.05 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 04:49:53.594324: step 86770, loss = 1.18 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 04:50:00.567815: step 86780, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 04:50:07.764056: step 86790, loss = 1.01 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 04:50:14.846529: step 86800, loss = 1.10 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 04:50:24.433510: step 86810, loss = 1.03 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 04:50:31.487446: step 86820, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 04:50:38.607304: step 86830, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 04:50:45.649573: step 86840, loss = 1.08 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 04:50:52.764032: step 86850, loss = 1.04 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 04:50:59.854894: step 86860, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 04:51:06.901573: step 86870, loss = 0.99 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 04:51:14.021197: step 86880, loss = 1.04 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 04:51:21.093462: step 86890, loss = 1.05 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 04:51:28.176973: step 86900, loss = 1.05 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 04:51:38.460891: step 86910, loss = 1.00 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 04:51:45.476404: step 86920, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 04:51:52.631802: step 86930, loss = 1.10 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 04:51:59.680259: step 86940, loss = 1.05 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 04:52:06.739453: step 86950, loss = 1.00 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 04:52:13.781506: step 86960, loss = 1.03 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 04:52:20.945592: step 86970, loss = 1.04 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 04:52:28.093476: step 86980, loss = 1.05 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:52:35.184697: step 86990, loss = 1.12 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 04:52:42.259121: step 87000, loss = 1.12 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 04:52:51.865852: step 87010, loss = 1.02 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 04:52:58.844155: step 87020, loss = 1.10 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 04:53:05.918821: step 87030, loss = 1.20 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 04:53:12.975273: step 87040, loss = 0.99 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 04:53:20.093105: step 87050, loss = 1.22 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 04:53:27.194404: step 87060, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 04:53:34.179126: step 87070, loss = 1.08 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 04:53:41.304295: step 87080, loss = 1.21 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 04:53:48.286586: step 87090, loss = 0.99 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:53:55.346875: step 87100, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 04:54:05.102207: step 87110, loss = 0.99 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:54:12.148176: step 87120, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 04:54:19.243986: step 87130, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 04:54:26.342084: step 87140, loss = 1.27 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 04:54:33.448896: step 87150, loss = 1.03 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 04:54:40.525167: step 87160, loss = 1.04 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 04:54:47.658771: step 87170, loss = 1.12 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 04:54:54.739324: step 87180, loss = 1.18 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 04:55:01.844113: step 87190, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:55:08.933788: step 87200, loss = 1.02 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 04:55:18.707329: step 87210, loss = 1.04 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 04:55:25.821667: step 87220, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:55:32.810500: step 87230, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 04:55:39.844042: step 87240, loss = 1.05 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 04:55:46.922989: step 87250, loss = 1.06 (47.7 examples/sec; 0.670 sec/batch)
2018-10-17 04:55:54.091712: step 87260, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 04:56:01.202120: step 87270, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 04:56:08.261474: step 87280, loss = 1.14 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 04:56:15.383651: step 87290, loss = 1.09 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 04:56:22.439207: step 87300, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 04:56:32.573354: step 87310, loss = 1.00 (48.8 examples/sec; 0.656 sec/batch)
2018-10-17 04:56:39.618387: step 87320, loss = 1.12 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 04:56:46.713847: step 87330, loss = 1.07 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 04:56:53.864001: step 87340, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 04:57:00.964319: step 87350, loss = 1.09 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 04:57:08.001634: step 87360, loss = 1.03 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 04:57:15.085373: step 87370, loss = 1.03 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 04:57:22.177912: step 87380, loss = 1.07 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 04:57:29.215311: step 87390, loss = 1.09 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 04:57:36.268142: step 87400, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:57:45.998244: step 87410, loss = 1.02 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 04:57:53.098732: step 87420, loss = 0.99 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 04:58:00.167138: step 87430, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 04:58:07.219830: step 87440, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 04:58:14.213804: step 87450, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 04:58:21.295300: step 87460, loss = 1.11 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 04:58:28.422277: step 87470, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 04:58:35.542732: step 87480, loss = 0.99 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 04:58:42.596193: step 87490, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 04:58:49.740585: step 87500, loss = 1.21 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 04:59:00.262694: step 87510, loss = 0.99 (47.3 examples/sec; 0.677 sec/batch)
2018-10-17 04:59:07.183341: step 87520, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 04:59:14.189767: step 87530, loss = 1.14 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 04:59:21.219252: step 87540, loss = 1.09 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 04:59:28.360849: step 87550, loss = 1.03 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 04:59:35.467349: step 87560, loss = 1.07 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 04:59:42.550216: step 87570, loss = 1.07 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 04:59:49.699018: step 87580, loss = 1.02 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 04:59:56.849022: step 87590, loss = 1.01 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 05:00:03.892753: step 87600, loss = 1.07 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 05:00:13.895569: step 87610, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 05:00:20.922651: step 87620, loss = 1.09 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:00:27.945891: step 87630, loss = 0.99 (44.0 examples/sec; 0.726 sec/batch)
2018-10-17 05:00:35.019263: step 87640, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 05:00:42.172457: step 87650, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:00:49.195248: step 87660, loss = 1.07 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 05:00:56.231540: step 87670, loss = 1.02 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 05:01:03.328160: step 87680, loss = 1.14 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 05:01:10.351692: step 87690, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 05:01:17.473196: step 87700, loss = 1.05 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:01:27.068453: step 87710, loss = 1.10 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 05:01:34.102395: step 87720, loss = 1.09 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 05:01:41.153641: step 87730, loss = 1.08 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 05:01:48.258403: step 87740, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 05:01:55.361395: step 87750, loss = 1.01 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 05:02:02.442271: step 87760, loss = 1.02 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 05:02:09.437071: step 87770, loss = 1.05 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 05:02:16.505236: step 87780, loss = 1.03 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:02:23.559220: step 87790, loss = 1.20 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 05:02:30.618971: step 87800, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 05:02:40.246471: step 87810, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 05:02:47.282819: step 87820, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 05:02:54.310150: step 87830, loss = 1.07 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 05:03:01.317168: step 87840, loss = 1.08 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 05:03:08.367703: step 87850, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:03:15.464619: step 87860, loss = 1.01 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 05:03:22.511685: step 87870, loss = 1.08 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 05:03:29.482164: step 87880, loss = 1.02 (45.9 examples/sec; 0.696 sec/batch)
2018-10-17 05:03:36.610232: step 87890, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 05:03:43.654911: step 87900, loss = 1.04 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:03:53.327929: step 87910, loss = 1.07 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 05:04:00.455288: step 87920, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 05:04:07.457201: step 87930, loss = 1.04 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 05:04:14.515386: step 87940, loss = 1.01 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 05:04:21.524898: step 87950, loss = 1.02 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:04:28.553584: step 87960, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 05:04:35.616393: step 87970, loss = 1.01 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 05:04:42.721971: step 87980, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:04:49.800553: step 87990, loss = 1.06 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 05:04:56.817956: step 88000, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 05:05:06.419470: step 88010, loss = 1.03 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 05:05:13.428945: step 88020, loss = 1.17 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 05:05:20.508413: step 88030, loss = 1.02 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 05:05:27.588664: step 88040, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 05:05:34.671862: step 88050, loss = 1.06 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 05:05:41.728858: step 88060, loss = 1.15 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:05:48.883275: step 88070, loss = 1.01 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 05:05:55.893320: step 88080, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 05:06:02.889088: step 88090, loss = 1.00 (44.5 examples/sec; 0.718 sec/batch)
2018-10-17 05:06:09.948066: step 88100, loss = 1.09 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 05:06:19.621467: step 88110, loss = 1.04 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 05:06:26.652253: step 88120, loss = 1.05 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 05:06:33.691613: step 88130, loss = 1.22 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 05:06:40.796587: step 88140, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 05:06:47.831753: step 88150, loss = 1.07 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 05:06:54.848102: step 88160, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 05:07:01.864613: step 88170, loss = 1.01 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 05:07:08.955487: step 88180, loss = 1.22 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 05:07:15.992956: step 88190, loss = 1.00 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 05:07:23.044290: step 88200, loss = 0.99 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 05:07:32.694371: step 88210, loss = 1.06 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 05:07:39.755685: step 88220, loss = 1.00 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 05:07:46.820566: step 88230, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 05:07:53.933410: step 88240, loss = 1.05 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 05:08:01.010687: step 88250, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 05:08:08.122833: step 88260, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:08:15.225658: step 88270, loss = 1.07 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 05:08:22.253294: step 88280, loss = 1.15 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 05:08:29.363046: step 88290, loss = 1.01 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 05:08:36.433091: step 88300, loss = 1.02 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 05:08:45.943325: step 88310, loss = 1.00 (47.8 examples/sec; 0.669 sec/batch)
2018-10-17 05:08:53.030980: step 88320, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 05:09:00.085645: step 88330, loss = 1.16 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 05:09:07.114770: step 88340, loss = 1.17 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 05:09:14.289097: step 88350, loss = 0.99 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 05:09:21.333320: step 88360, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 05:09:28.424009: step 88370, loss = 1.07 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 05:09:35.470468: step 88380, loss = 1.02 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 05:09:42.544666: step 88390, loss = 1.10 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 05:09:49.583765: step 88400, loss = 1.16 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 05:10:00.022870: step 88410, loss = 1.02 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 05:10:07.000390: step 88420, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 05:10:14.022878: step 88430, loss = 1.11 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:10:21.080277: step 88440, loss = 1.12 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 05:10:28.177557: step 88450, loss = 1.08 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:10:35.229667: step 88460, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:10:42.319307: step 88470, loss = 1.04 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 05:10:49.355356: step 88480, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 05:10:56.457379: step 88490, loss = 1.21 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:11:03.577147: step 88500, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 05:11:13.310066: step 88510, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 05:11:20.324527: step 88520, loss = 1.08 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:11:27.439645: step 88530, loss = 0.99 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 05:11:34.493210: step 88540, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 05:11:41.608613: step 88550, loss = 1.00 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 05:11:48.706543: step 88560, loss = 1.12 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 05:11:55.728845: step 88570, loss = 1.01 (45.7 examples/sec; 0.699 sec/batch)
2018-10-17 05:12:02.829137: step 88580, loss = 1.02 (44.5 examples/sec; 0.718 sec/batch)
2018-10-17 05:12:09.884463: step 88590, loss = 1.00 (45.9 examples/sec; 0.696 sec/batch)
2018-10-17 05:12:16.891792: step 88600, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 05:12:26.605609: step 88610, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 05:12:33.673312: step 88620, loss = 1.00 (44.0 examples/sec; 0.726 sec/batch)
2018-10-17 05:12:40.660529: step 88630, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:12:47.833009: step 88640, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:12:54.918932: step 88650, loss = 1.03 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 05:13:02.054733: step 88660, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 05:13:09.138515: step 88670, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:13:16.197661: step 88680, loss = 1.12 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 05:13:23.241629: step 88690, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 05:13:30.280745: step 88700, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:13:39.876398: step 88710, loss = 1.31 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 05:13:46.997715: step 88720, loss = 1.04 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 05:13:54.083827: step 88730, loss = 1.01 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 05:14:01.216751: step 88740, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 05:14:08.248349: step 88750, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:14:15.327049: step 88760, loss = 1.01 (42.9 examples/sec; 0.745 sec/batch)
2018-10-17 05:14:22.405777: step 88770, loss = 0.99 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:14:29.487285: step 88780, loss = 1.09 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 05:14:36.572193: step 88790, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 05:14:43.599918: step 88800, loss = 1.00 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 05:14:53.191082: step 88810, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 05:15:00.299856: step 88820, loss = 1.00 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 05:15:07.349792: step 88830, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 05:15:14.436674: step 88840, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 05:15:21.519271: step 88850, loss = 1.27 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 05:15:28.615864: step 88860, loss = 1.01 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 05:15:35.707007: step 88870, loss = 1.01 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 05:15:42.779328: step 88880, loss = 1.22 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 05:15:49.853001: step 88890, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 05:15:56.957124: step 88900, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 05:16:06.966538: step 88910, loss = 1.03 (48.3 examples/sec; 0.663 sec/batch)
2018-10-17 05:16:13.958823: step 88920, loss = 0.99 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 05:16:20.995747: step 88930, loss = 0.99 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 05:16:28.166057: step 88940, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 05:16:35.264230: step 88950, loss = 1.11 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 05:16:42.318498: step 88960, loss = 1.10 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 05:16:49.435667: step 88970, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 05:16:56.534402: step 88980, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:17:03.606051: step 88990, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 05:17:10.629392: step 89000, loss = 1.10 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 05:17:20.334590: step 89010, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 05:17:27.347278: step 89020, loss = 1.19 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:17:34.398183: step 89030, loss = 1.00 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 05:17:41.500197: step 89040, loss = 1.13 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:17:48.585246: step 89050, loss = 1.13 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 05:17:55.748674: step 89060, loss = 1.06 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 05:18:02.807133: step 89070, loss = 1.01 (45.9 examples/sec; 0.696 sec/batch)
2018-10-17 05:18:09.955002: step 89080, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 05:18:17.067386: step 89090, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 05:18:24.216335: step 89100, loss = 1.03 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 05:18:34.866409: step 89110, loss = 1.07 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 05:18:41.859579: step 89120, loss = 1.10 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 05:18:49.066282: step 89130, loss = 1.12 (44.0 examples/sec; 0.726 sec/batch)
2018-10-17 05:18:56.106174: step 89140, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 05:19:03.211745: step 89150, loss = 0.99 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 05:19:10.269619: step 89160, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:19:17.397995: step 89170, loss = 1.05 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 05:19:24.431699: step 89180, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 05:19:31.555152: step 89190, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 05:19:38.642207: step 89200, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 05:19:48.650498: step 89210, loss = 1.04 (47.2 examples/sec; 0.677 sec/batch)
2018-10-17 05:19:55.823421: step 89220, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 05:20:02.924144: step 89230, loss = 1.05 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 05:20:09.993433: step 89240, loss = 1.02 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 05:20:17.024473: step 89250, loss = 1.00 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 05:20:24.113752: step 89260, loss = 1.06 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 05:20:31.160297: step 89270, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:20:38.193117: step 89280, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 05:20:45.323912: step 89290, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:20:52.417062: step 89300, loss = 1.01 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 05:21:02.092350: step 89310, loss = 0.99 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 05:21:09.161442: step 89320, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:21:16.179670: step 89330, loss = 1.35 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 05:21:23.203051: step 89340, loss = 1.02 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 05:21:30.318485: step 89350, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 05:21:37.370802: step 89360, loss = 1.13 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 05:21:44.495159: step 89370, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 05:21:51.567677: step 89380, loss = 1.10 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 05:21:58.645999: step 89390, loss = 1.01 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 05:22:05.749784: step 89400, loss = 1.07 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:22:15.521236: step 89410, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 05:22:22.598316: step 89420, loss = 1.05 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 05:22:29.751268: step 89430, loss = 1.00 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 05:22:36.828646: step 89440, loss = 0.99 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 05:22:43.982618: step 89450, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:22:51.046656: step 89460, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 05:22:58.214493: step 89470, loss = 1.11 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:23:05.316487: step 89480, loss = 1.02 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 05:23:12.408563: step 89490, loss = 1.02 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 05:23:19.512130: step 89500, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:23:29.224763: step 89510, loss = 1.14 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 05:23:36.264762: step 89520, loss = 1.07 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:23:43.265760: step 89530, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 05:23:50.322394: step 89540, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 05:23:57.359380: step 89550, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:24:04.402584: step 89560, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:24:11.491760: step 89570, loss = 1.12 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 05:24:18.528319: step 89580, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 05:24:25.594570: step 89590, loss = 1.07 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 05:24:32.724616: step 89600, loss = 1.09 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 05:24:42.429941: step 89610, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 05:24:49.463852: step 89620, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 05:24:56.577061: step 89630, loss = 0.99 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 05:25:03.631933: step 89640, loss = 0.99 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 05:25:10.717122: step 89650, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 05:25:17.853684: step 89660, loss = 1.05 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 05:25:24.898563: step 89670, loss = 1.03 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 05:25:31.982912: step 89680, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 05:25:39.102457: step 89690, loss = 1.03 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 05:25:46.158526: step 89700, loss = 1.18 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 05:25:55.853291: step 89710, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 05:26:02.878424: step 89720, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:26:09.933308: step 89730, loss = 1.03 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 05:26:16.941339: step 89740, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 05:26:24.145323: step 89750, loss = 1.02 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 05:26:31.239862: step 89760, loss = 1.08 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 05:26:38.330831: step 89770, loss = 1.10 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 05:26:45.462004: step 89780, loss = 1.00 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 05:26:52.547779: step 89790, loss = 1.01 (46.8 examples/sec; 0.683 sec/batch)
2018-10-17 05:26:59.661144: step 89800, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 05:27:09.398315: step 89810, loss = 1.17 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:27:16.474365: step 89820, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 05:27:23.543107: step 89830, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:27:30.605865: step 89840, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:27:37.624522: step 89850, loss = 1.08 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 05:27:44.761798: step 89860, loss = 1.09 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 05:27:51.882143: step 89870, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 05:27:58.951278: step 89880, loss = 1.12 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 05:28:06.053255: step 89890, loss = 1.14 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 05:28:13.122213: step 89900, loss = 1.08 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:28:22.860935: step 89910, loss = 1.05 (47.3 examples/sec; 0.677 sec/batch)
2018-10-17 05:28:29.880235: step 89920, loss = 1.02 (47.6 examples/sec; 0.673 sec/batch)
2018-10-17 05:28:37.008396: step 89930, loss = 1.03 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 05:28:44.046430: step 89940, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:28:51.099212: step 89950, loss = 1.04 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 05:28:58.171271: step 89960, loss = 1.01 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 05:29:05.220381: step 89970, loss = 1.05 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:29:12.310586: step 89980, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:29:19.411431: step 89990, loss = 1.00 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 05:29:26.518089: step 90000, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 05:29:39.590885: step 90010, loss = 1.18 (46.8 examples/sec; 0.683 sec/batch)
2018-10-17 05:29:46.425042: step 90020, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 05:29:53.487634: step 90030, loss = 1.15 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 05:30:00.574171: step 90040, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 05:30:07.706202: step 90050, loss = 1.00 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 05:30:14.762236: step 90060, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:30:21.881587: step 90070, loss = 1.10 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 05:30:28.992229: step 90080, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 05:30:36.097942: step 90090, loss = 0.99 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 05:30:43.201680: step 90100, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 05:30:52.839336: step 90110, loss = 1.06 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 05:30:59.858133: step 90120, loss = 1.08 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 05:31:06.962933: step 90130, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 05:31:14.068477: step 90140, loss = 1.15 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 05:31:21.163352: step 90150, loss = 1.20 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 05:31:28.175065: step 90160, loss = 1.03 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 05:31:35.239509: step 90170, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 05:31:42.370673: step 90180, loss = 1.18 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 05:31:49.445212: step 90190, loss = 1.12 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 05:31:56.469154: step 90200, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:32:06.270489: step 90210, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:32:13.360931: step 90220, loss = 1.04 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 05:32:20.433807: step 90230, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 05:32:27.521447: step 90240, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:32:34.614143: step 90250, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 05:32:41.599976: step 90260, loss = 1.15 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 05:32:48.678094: step 90270, loss = 1.01 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 05:32:55.773065: step 90280, loss = 1.09 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 05:33:02.872368: step 90290, loss = 1.06 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 05:33:09.911857: step 90300, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 05:33:20.133645: step 90310, loss = 1.15 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 05:33:27.135698: step 90320, loss = 1.05 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 05:33:34.264129: step 90330, loss = 1.06 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 05:33:41.309376: step 90340, loss = 1.20 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 05:33:48.431389: step 90350, loss = 1.02 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 05:33:55.562109: step 90360, loss = 1.10 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 05:34:02.725006: step 90370, loss = 1.06 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 05:34:09.796753: step 90380, loss = 1.10 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 05:34:16.942567: step 90390, loss = 1.07 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 05:34:24.046563: step 90400, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 05:34:33.626077: step 90410, loss = 1.04 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 05:34:40.694192: step 90420, loss = 1.09 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 05:34:47.805029: step 90430, loss = 1.26 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 05:34:54.910924: step 90440, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 05:35:01.884060: step 90450, loss = 1.19 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 05:35:08.965122: step 90460, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 05:35:16.215086: step 90470, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:35:23.265745: step 90480, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 05:35:30.292961: step 90490, loss = 1.08 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 05:35:37.343811: step 90500, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 05:35:46.880740: step 90510, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 05:35:54.022832: step 90520, loss = 1.05 (47.9 examples/sec; 0.669 sec/batch)
2018-10-17 05:36:01.190263: step 90530, loss = 1.03 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 05:36:08.220381: step 90540, loss = 1.08 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 05:36:15.374388: step 90550, loss = 1.04 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 05:36:22.494929: step 90560, loss = 1.09 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 05:36:29.548158: step 90570, loss = 1.15 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 05:36:36.648208: step 90580, loss = 1.06 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 05:36:43.672391: step 90590, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 05:36:50.715592: step 90600, loss = 1.08 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 05:37:00.311362: step 90610, loss = 1.15 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:37:07.331821: step 90620, loss = 1.22 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 05:37:14.375270: step 90630, loss = 1.10 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 05:37:21.435356: step 90640, loss = 1.09 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 05:37:28.577986: step 90650, loss = 1.04 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 05:37:35.600105: step 90660, loss = 1.04 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 05:37:42.665660: step 90670, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 05:37:49.743441: step 90680, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 05:37:56.818351: step 90690, loss = 1.03 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 05:38:04.036806: step 90700, loss = 1.03 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 05:38:13.771525: step 90710, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 05:38:20.861972: step 90720, loss = 1.00 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 05:38:27.835612: step 90730, loss = 0.99 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 05:38:34.938176: step 90740, loss = 1.16 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 05:38:41.997783: step 90750, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 05:38:49.001727: step 90760, loss = 1.22 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 05:38:56.040738: step 90770, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 05:39:03.050846: step 90780, loss = 1.01 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 05:39:10.120520: step 90790, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 05:39:17.097988: step 90800, loss = 1.08 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 05:39:26.659411: step 90810, loss = 1.18 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 05:39:33.706700: step 90820, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 05:39:40.710859: step 90830, loss = 1.06 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 05:39:47.763151: step 90840, loss = 1.11 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 05:39:54.808979: step 90850, loss = 1.05 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:40:01.927233: step 90860, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 05:40:08.960098: step 90870, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 05:40:16.063230: step 90880, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 05:40:23.199709: step 90890, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:40:30.258095: step 90900, loss = 1.14 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:40:40.470612: step 90910, loss = 1.06 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 05:40:47.455243: step 90920, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 05:40:54.506017: step 90930, loss = 1.12 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 05:41:01.625372: step 90940, loss = 1.07 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:41:08.704613: step 90950, loss = 1.03 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 05:41:15.845267: step 90960, loss = 1.07 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 05:41:22.912045: step 90970, loss = 1.16 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 05:41:29.982665: step 90980, loss = 1.08 (44.5 examples/sec; 0.718 sec/batch)
2018-10-17 05:41:37.075620: step 90990, loss = 1.08 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 05:41:44.155082: step 91000, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 05:41:54.401654: step 91010, loss = 1.04 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 05:42:01.446857: step 91020, loss = 1.04 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 05:42:08.566251: step 91030, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 05:42:15.634754: step 91040, loss = 1.06 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 05:42:22.748835: step 91050, loss = 1.11 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 05:42:29.868243: step 91060, loss = 1.10 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 05:42:36.934558: step 91070, loss = 1.10 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 05:42:43.963925: step 91080, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 05:42:50.990159: step 91090, loss = 1.03 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 05:42:58.114860: step 91100, loss = 1.06 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 05:43:07.806403: step 91110, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 05:43:14.850184: step 91120, loss = 1.29 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:43:21.948245: step 91130, loss = 1.06 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 05:43:29.069603: step 91140, loss = 1.02 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 05:43:36.121578: step 91150, loss = 1.07 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 05:43:43.207071: step 91160, loss = 0.99 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 05:43:50.327595: step 91170, loss = 1.02 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 05:43:57.372557: step 91180, loss = 1.06 (43.3 examples/sec; 0.738 sec/batch)
2018-10-17 05:44:04.423170: step 91190, loss = 1.02 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 05:44:11.508993: step 91200, loss = 1.13 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 05:44:21.167368: step 91210, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:44:28.242522: step 91220, loss = 1.05 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:44:35.396668: step 91230, loss = 1.12 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 05:44:42.417083: step 91240, loss = 0.99 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 05:44:49.576591: step 91250, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 05:44:56.699770: step 91260, loss = 1.14 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 05:45:03.727018: step 91270, loss = 1.16 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:45:10.789367: step 91280, loss = 1.07 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 05:45:17.952519: step 91290, loss = 1.09 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 05:45:24.955900: step 91300, loss = 1.03 (44.7 examples/sec; 0.717 sec/batch)
2018-10-17 05:45:34.545170: step 91310, loss = 1.11 (47.9 examples/sec; 0.668 sec/batch)
2018-10-17 05:45:41.666258: step 91320, loss = 1.06 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 05:45:48.819762: step 91330, loss = 1.07 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 05:45:55.881335: step 91340, loss = 1.09 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 05:46:02.909694: step 91350, loss = 1.16 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 05:46:09.947571: step 91360, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:46:17.086075: step 91370, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 05:46:24.100362: step 91380, loss = 0.99 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 05:46:31.189240: step 91390, loss = 1.19 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 05:46:38.237899: step 91400, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 05:46:47.874761: step 91410, loss = 1.02 (46.5 examples/sec; 0.687 sec/batch)
2018-10-17 05:46:54.905465: step 91420, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 05:47:01.983021: step 91430, loss = 1.06 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 05:47:09.021150: step 91440, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 05:47:16.116416: step 91450, loss = 0.99 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 05:47:23.221454: step 91460, loss = 1.01 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 05:47:30.292279: step 91470, loss = 1.05 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 05:47:37.433796: step 91480, loss = 1.22 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 05:47:44.444044: step 91490, loss = 1.04 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 05:47:51.488326: step 91500, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 05:48:01.103881: step 91510, loss = 1.03 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 05:48:08.142529: step 91520, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 05:48:15.207022: step 91530, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 05:48:22.273701: step 91540, loss = 0.99 (43.3 examples/sec; 0.740 sec/batch)
2018-10-17 05:48:29.347416: step 91550, loss = 1.04 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 05:48:36.387804: step 91560, loss = 1.27 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 05:48:43.504699: step 91570, loss = 1.12 (47.3 examples/sec; 0.677 sec/batch)
2018-10-17 05:48:50.566948: step 91580, loss = 1.06 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 05:48:57.661028: step 91590, loss = 1.20 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 05:49:04.652699: step 91600, loss = 1.06 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 05:49:14.376325: step 91610, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 05:49:21.404078: step 91620, loss = 1.10 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 05:49:28.502340: step 91630, loss = 1.14 (46.1 examples/sec; 0.693 sec/batch)
2018-10-17 05:49:35.527566: step 91640, loss = 1.07 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 05:49:42.623366: step 91650, loss = 1.14 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:49:49.681725: step 91660, loss = 1.11 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:49:56.775293: step 91670, loss = 0.99 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 05:50:03.802326: step 91680, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:50:10.972391: step 91690, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 05:50:18.042152: step 91700, loss = 1.03 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:50:28.159052: step 91710, loss = 1.09 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 05:50:35.269053: step 91720, loss = 1.10 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:50:42.269673: step 91730, loss = 1.02 (44.4 examples/sec; 0.722 sec/batch)
2018-10-17 05:50:49.313768: step 91740, loss = 1.06 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 05:50:56.412370: step 91750, loss = 0.99 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 05:51:03.423165: step 91760, loss = 1.06 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 05:51:10.461391: step 91770, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 05:51:17.530891: step 91780, loss = 1.01 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 05:51:24.514223: step 91790, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 05:51:31.538828: step 91800, loss = 0.99 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 05:51:41.172067: step 91810, loss = 1.03 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 05:51:48.179961: step 91820, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 05:51:55.322385: step 91830, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 05:52:02.377716: step 91840, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 05:52:09.376516: step 91850, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:52:16.409044: step 91860, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 05:52:23.444660: step 91870, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 05:52:30.571560: step 91880, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 05:52:37.571631: step 91890, loss = 1.12 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 05:52:44.697830: step 91900, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:52:54.867952: step 91910, loss = 1.06 (47.4 examples/sec; 0.676 sec/batch)
2018-10-17 05:53:01.804607: step 91920, loss = 1.07 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 05:53:08.947320: step 91930, loss = 1.06 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 05:53:16.131357: step 91940, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 05:53:23.148824: step 91950, loss = 1.00 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 05:53:30.165421: step 91960, loss = 1.03 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 05:53:37.329761: step 91970, loss = 1.06 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 05:53:44.344385: step 91980, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:53:51.411983: step 91990, loss = 1.14 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 05:53:58.407497: step 92000, loss = 1.16 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 05:54:08.337743: step 92010, loss = 1.05 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 05:54:15.399836: step 92020, loss = 1.05 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 05:54:22.371880: step 92030, loss = 1.05 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 05:54:29.529852: step 92040, loss = 1.22 (43.0 examples/sec; 0.743 sec/batch)
2018-10-17 05:54:36.624498: step 92050, loss = 1.03 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 05:54:43.616612: step 92060, loss = 1.10 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 05:54:50.739839: step 92070, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 05:54:57.785702: step 92080, loss = 1.09 (47.3 examples/sec; 0.677 sec/batch)
2018-10-17 05:55:04.877980: step 92090, loss = 1.00 (45.6 examples/sec; 0.703 sec/batch)
2018-10-17 05:55:11.836226: step 92100, loss = 1.02 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 05:55:21.653456: step 92110, loss = 1.11 (47.5 examples/sec; 0.674 sec/batch)
2018-10-17 05:55:28.738541: step 92120, loss = 0.99 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 05:55:35.770649: step 92130, loss = 1.00 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 05:55:42.819279: step 92140, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 05:55:49.901759: step 92150, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 05:55:56.888284: step 92160, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 05:56:03.956350: step 92170, loss = 0.99 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 05:56:10.926880: step 92180, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 05:56:17.966218: step 92190, loss = 1.04 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 05:56:25.052995: step 92200, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 05:56:34.725584: step 92210, loss = 1.11 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 05:56:41.788307: step 92220, loss = 1.14 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 05:56:48.831369: step 92230, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:56:55.867761: step 92240, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 05:57:03.003793: step 92250, loss = 1.02 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 05:57:10.027604: step 92260, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 05:57:17.082016: step 92270, loss = 1.05 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 05:57:24.130948: step 92280, loss = 1.08 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 05:57:31.267652: step 92290, loss = 1.13 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 05:57:38.350522: step 92300, loss = 1.10 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 05:57:47.963639: step 92310, loss = 1.04 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 05:57:54.980420: step 92320, loss = 1.03 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 05:58:02.110950: step 92330, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 05:58:09.163315: step 92340, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 05:58:16.312897: step 92350, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 05:58:23.405716: step 92360, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 05:58:30.497883: step 92370, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 05:58:37.583021: step 92380, loss = 1.08 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 05:58:44.661036: step 92390, loss = 1.10 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 05:58:51.707758: step 92400, loss = 1.08 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 05:59:01.488167: step 92410, loss = 1.22 (46.8 examples/sec; 0.683 sec/batch)
2018-10-17 05:59:08.577429: step 92420, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 05:59:15.596874: step 92430, loss = 1.08 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 05:59:22.660515: step 92440, loss = 1.13 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 05:59:29.747963: step 92450, loss = 1.11 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 05:59:36.772964: step 92460, loss = 1.04 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 05:59:43.749175: step 92470, loss = 1.06 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 05:59:50.830256: step 92480, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 05:59:57.960565: step 92490, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 06:00:04.987097: step 92500, loss = 1.03 (47.2 examples/sec; 0.679 sec/batch)
2018-10-17 06:00:14.701301: step 92510, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 06:00:21.787162: step 92520, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 06:00:28.852784: step 92530, loss = 1.10 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 06:00:35.924294: step 92540, loss = 1.15 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 06:00:42.992610: step 92550, loss = 1.10 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 06:00:50.042286: step 92560, loss = 0.99 (46.3 examples/sec; 0.690 sec/batch)
2018-10-17 06:00:57.158815: step 92570, loss = 1.02 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 06:01:04.283923: step 92580, loss = 1.00 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 06:01:11.295207: step 92590, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:01:18.349505: step 92600, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 06:01:28.421605: step 92610, loss = 1.03 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 06:01:35.489393: step 92620, loss = 0.99 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 06:01:42.590912: step 92630, loss = 1.05 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 06:01:49.726646: step 92640, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 06:01:56.850361: step 92650, loss = 0.99 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 06:02:03.872506: step 92660, loss = 1.02 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 06:02:11.032656: step 92670, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 06:02:18.156665: step 92680, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 06:02:25.223588: step 92690, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 06:02:32.260847: step 92700, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 06:02:41.839847: step 92710, loss = 1.13 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 06:02:48.890436: step 92720, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:02:55.924442: step 92730, loss = 1.06 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 06:03:02.932512: step 92740, loss = 1.08 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 06:03:09.958060: step 92750, loss = 0.99 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 06:03:17.068680: step 92760, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 06:03:24.195285: step 92770, loss = 1.06 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 06:03:31.280590: step 92780, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 06:03:38.338693: step 92790, loss = 1.01 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 06:03:45.498721: step 92800, loss = 1.45 (43.6 examples/sec; 0.735 sec/batch)
2018-10-17 06:03:55.027206: step 92810, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 06:04:02.113503: step 92820, loss = 1.19 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 06:04:09.147395: step 92830, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 06:04:16.175436: step 92840, loss = 1.05 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 06:04:23.296586: step 92850, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 06:04:30.357339: step 92860, loss = 1.04 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 06:04:37.402221: step 92870, loss = 1.00 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 06:04:44.407465: step 92880, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 06:04:51.424897: step 92890, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 06:04:58.530448: step 92900, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 06:05:08.157192: step 92910, loss = 1.17 (47.0 examples/sec; 0.680 sec/batch)
2018-10-17 06:05:15.243126: step 92920, loss = 1.02 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 06:05:22.211421: step 92930, loss = 1.02 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 06:05:29.232366: step 92940, loss = 1.07 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:05:36.306575: step 92950, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 06:05:43.372857: step 92960, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 06:05:50.491519: step 92970, loss = 1.09 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 06:05:57.573354: step 92980, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 06:06:04.689070: step 92990, loss = 1.05 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 06:06:11.736687: step 93000, loss = 1.09 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 06:06:21.549724: step 93010, loss = 1.10 (47.4 examples/sec; 0.676 sec/batch)
2018-10-17 06:06:28.665167: step 93020, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 06:06:35.768903: step 93030, loss = 0.99 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 06:06:42.864599: step 93040, loss = 0.99 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 06:06:49.960440: step 93050, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 06:06:56.990169: step 93060, loss = 1.09 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:07:04.077040: step 93070, loss = 1.01 (42.7 examples/sec; 0.750 sec/batch)
2018-10-17 06:07:11.074461: step 93080, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 06:07:18.190202: step 93090, loss = 1.07 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 06:07:25.347231: step 93100, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 06:07:35.105848: step 93110, loss = 1.01 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 06:07:42.175403: step 93120, loss = 1.13 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 06:07:49.237712: step 93130, loss = 1.08 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 06:07:56.310368: step 93140, loss = 1.09 (43.3 examples/sec; 0.738 sec/batch)
2018-10-17 06:08:03.415457: step 93150, loss = 1.05 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 06:08:10.463843: step 93160, loss = 1.10 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 06:08:17.483972: step 93170, loss = 0.99 (47.3 examples/sec; 0.677 sec/batch)
2018-10-17 06:08:24.526303: step 93180, loss = 1.16 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 06:08:31.562799: step 93190, loss = 1.11 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:08:38.655112: step 93200, loss = 1.01 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 06:08:48.216677: step 93210, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 06:08:55.233682: step 93220, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 06:09:02.345205: step 93230, loss = 1.04 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 06:09:09.476106: step 93240, loss = 1.01 (44.5 examples/sec; 0.718 sec/batch)
2018-10-17 06:09:16.447825: step 93250, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 06:09:23.519993: step 93260, loss = 1.07 (46.8 examples/sec; 0.683 sec/batch)
2018-10-17 06:09:30.583035: step 93270, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 06:09:37.643438: step 93280, loss = 1.07 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 06:09:44.743379: step 93290, loss = 1.08 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 06:09:51.802499: step 93300, loss = 1.07 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 06:10:01.803368: step 93310, loss = 1.05 (47.4 examples/sec; 0.674 sec/batch)
2018-10-17 06:10:08.871011: step 93320, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 06:10:15.959269: step 93330, loss = 1.10 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 06:10:22.970697: step 93340, loss = 1.11 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:10:30.034596: step 93350, loss = 1.02 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 06:10:37.084518: step 93360, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 06:10:44.154759: step 93370, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 06:10:51.255790: step 93380, loss = 1.02 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 06:10:58.330152: step 93390, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 06:11:05.285337: step 93400, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:11:15.112951: step 93410, loss = 1.17 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 06:11:22.202109: step 93420, loss = 1.14 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 06:11:29.256081: step 93430, loss = 1.11 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 06:11:36.427169: step 93440, loss = 1.02 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 06:11:43.529141: step 93450, loss = 1.04 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 06:11:50.588752: step 93460, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 06:11:57.713696: step 93470, loss = 1.11 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 06:12:04.713116: step 93480, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 06:12:11.834355: step 93490, loss = 1.03 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 06:12:18.856140: step 93500, loss = 1.03 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 06:12:28.842042: step 93510, loss = 1.01 (47.8 examples/sec; 0.670 sec/batch)
2018-10-17 06:12:35.899090: step 93520, loss = 1.03 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 06:12:42.938219: step 93530, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 06:12:49.953871: step 93540, loss = 1.01 (48.2 examples/sec; 0.664 sec/batch)
2018-10-17 06:12:56.957252: step 93550, loss = 0.99 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 06:13:03.987774: step 93560, loss = 1.17 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:13:11.169256: step 93570, loss = 1.19 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 06:13:18.248794: step 93580, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:13:25.320675: step 93590, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 06:13:32.429394: step 93600, loss = 1.15 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 06:13:41.974126: step 93610, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:13:48.947051: step 93620, loss = 1.07 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 06:13:56.068814: step 93630, loss = 1.04 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 06:14:03.136832: step 93640, loss = 1.05 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 06:14:10.210214: step 93650, loss = 1.01 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 06:14:17.305982: step 93660, loss = 1.00 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 06:14:24.347487: step 93670, loss = 1.09 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 06:14:31.407698: step 93680, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 06:14:38.459457: step 93690, loss = 1.05 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 06:14:45.592590: step 93700, loss = 1.17 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:14:55.302313: step 93710, loss = 1.08 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 06:15:02.314862: step 93720, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 06:15:09.309630: step 93730, loss = 1.09 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 06:15:16.344493: step 93740, loss = 1.03 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 06:15:23.415673: step 93750, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 06:15:30.438800: step 93760, loss = 1.05 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 06:15:37.497168: step 93770, loss = 1.10 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 06:15:44.546874: step 93780, loss = 1.08 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 06:15:51.592545: step 93790, loss = 1.20 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 06:15:58.610011: step 93800, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 06:16:08.278000: step 93810, loss = 1.10 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 06:16:15.310975: step 93820, loss = 1.09 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 06:16:22.430624: step 93830, loss = 1.01 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 06:16:29.546046: step 93840, loss = 1.03 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 06:16:36.598860: step 93850, loss = 1.07 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 06:16:43.656043: step 93860, loss = 1.03 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 06:16:50.742666: step 93870, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 06:16:57.771317: step 93880, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 06:17:04.843610: step 93890, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 06:17:11.861960: step 93900, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:17:21.451291: step 93910, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 06:17:28.437072: step 93920, loss = 1.05 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 06:17:35.536143: step 93930, loss = 1.04 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 06:17:42.566348: step 93940, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 06:17:49.641309: step 93950, loss = 1.05 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 06:17:56.695355: step 93960, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 06:18:03.843254: step 93970, loss = 0.99 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 06:18:10.921920: step 93980, loss = 1.04 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 06:18:17.995110: step 93990, loss = 1.05 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 06:18:25.162387: step 94000, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 06:18:34.879609: step 94010, loss = 1.11 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 06:18:41.898531: step 94020, loss = 0.99 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 06:18:48.915041: step 94030, loss = 1.08 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 06:18:56.004429: step 94040, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 06:19:03.041959: step 94050, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:19:10.110688: step 94060, loss = 0.99 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 06:19:17.134909: step 94070, loss = 1.04 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 06:19:24.220970: step 94080, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:19:31.272719: step 94090, loss = 1.03 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 06:19:38.388453: step 94100, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 06:19:48.013220: step 94110, loss = 0.99 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 06:19:55.073094: step 94120, loss = 1.04 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 06:20:02.155273: step 94130, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 06:20:09.224012: step 94140, loss = 1.25 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 06:20:16.307323: step 94150, loss = 1.08 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 06:20:23.414187: step 94160, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 06:20:30.429709: step 94170, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 06:20:37.600128: step 94180, loss = 0.99 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 06:20:44.714615: step 94190, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:20:51.765811: step 94200, loss = 1.13 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:21:01.313325: step 94210, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:21:08.401035: step 94220, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 06:21:15.504634: step 94230, loss = 1.07 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:21:22.595724: step 94240, loss = 0.99 (43.6 examples/sec; 0.735 sec/batch)
2018-10-17 06:21:29.677361: step 94250, loss = 1.03 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 06:21:36.787079: step 94260, loss = 1.01 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 06:21:43.828531: step 94270, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 06:21:50.879066: step 94280, loss = 1.00 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 06:21:57.848869: step 94290, loss = 1.07 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 06:22:04.914484: step 94300, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 06:22:14.530698: step 94310, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 06:22:21.583966: step 94320, loss = 1.00 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 06:22:28.676131: step 94330, loss = 1.21 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:22:35.762733: step 94340, loss = 1.01 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 06:22:42.769329: step 94350, loss = 0.99 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 06:22:49.918179: step 94360, loss = 1.07 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 06:22:57.012393: step 94370, loss = 1.07 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 06:23:04.063695: step 94380, loss = 1.12 (44.7 examples/sec; 0.717 sec/batch)
2018-10-17 06:23:11.106022: step 94390, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 06:23:18.115547: step 94400, loss = 1.22 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 06:23:27.635120: step 94410, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 06:23:34.598645: step 94420, loss = 1.03 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 06:23:41.638749: step 94430, loss = 1.06 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 06:23:48.750461: step 94440, loss = 1.11 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 06:23:55.862519: step 94450, loss = 1.02 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 06:24:02.982132: step 94460, loss = 1.17 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:24:10.017931: step 94470, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 06:24:17.117022: step 94480, loss = 1.14 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 06:24:24.179498: step 94490, loss = 1.14 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 06:24:31.203655: step 94500, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:24:40.997193: step 94510, loss = 1.12 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 06:24:48.075932: step 94520, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:24:55.081830: step 94530, loss = 1.01 (46.8 examples/sec; 0.683 sec/batch)
2018-10-17 06:25:02.211917: step 94540, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:25:09.262066: step 94550, loss = 1.01 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 06:25:16.384373: step 94560, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 06:25:23.488853: step 94570, loss = 1.14 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 06:25:30.563784: step 94580, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 06:25:37.664890: step 94590, loss = 1.09 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 06:25:44.739004: step 94600, loss = 1.07 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:25:54.450654: step 94610, loss = 0.99 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 06:26:01.507454: step 94620, loss = 0.99 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 06:26:08.610519: step 94630, loss = 1.22 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 06:26:15.713163: step 94640, loss = 1.21 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 06:26:22.738468: step 94650, loss = 1.07 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:26:29.817873: step 94660, loss = 1.03 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 06:26:36.866412: step 94670, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 06:26:43.872592: step 94680, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 06:26:50.964434: step 94690, loss = 1.01 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 06:26:57.969243: step 94700, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 06:27:07.599174: step 94710, loss = 1.04 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 06:27:14.742413: step 94720, loss = 1.23 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 06:27:21.836459: step 94730, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 06:27:28.877127: step 94740, loss = 1.05 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 06:27:35.995959: step 94750, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:27:43.104271: step 94760, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 06:27:50.187760: step 94770, loss = 1.12 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 06:27:57.269326: step 94780, loss = 1.16 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 06:28:04.327702: step 94790, loss = 1.05 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 06:28:11.460917: step 94800, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 06:28:21.025232: step 94810, loss = 1.00 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 06:28:28.069541: step 94820, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 06:28:35.130439: step 94830, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 06:28:42.225586: step 94840, loss = 1.03 (43.2 examples/sec; 0.742 sec/batch)
2018-10-17 06:28:49.283290: step 94850, loss = 1.18 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 06:28:56.417358: step 94860, loss = 0.99 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 06:29:03.448769: step 94870, loss = 1.07 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 06:29:10.499510: step 94880, loss = 1.02 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 06:29:17.543402: step 94890, loss = 1.03 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 06:29:24.643256: step 94900, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 06:29:34.434963: step 94910, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 06:29:41.507039: step 94920, loss = 1.04 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 06:29:48.549897: step 94930, loss = 1.05 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 06:29:55.596299: step 94940, loss = 1.14 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 06:30:02.660497: step 94950, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 06:30:09.787061: step 94960, loss = 1.00 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 06:30:16.821453: step 94970, loss = 1.11 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:30:23.876690: step 94980, loss = 1.02 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 06:30:31.085484: step 94990, loss = 1.07 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 06:30:38.185434: step 95000, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 06:30:51.417810: step 95010, loss = 1.07 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 06:30:58.196245: step 95020, loss = 1.00 (47.3 examples/sec; 0.676 sec/batch)
2018-10-17 06:31:05.151987: step 95030, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 06:31:12.324846: step 95040, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 06:31:19.432431: step 95050, loss = 1.12 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 06:31:26.508056: step 95060, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 06:31:33.628424: step 95070, loss = 1.10 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 06:31:40.787810: step 95080, loss = 1.00 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 06:31:47.839324: step 95090, loss = 1.00 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 06:31:54.979237: step 95100, loss = 1.06 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 06:32:04.543267: step 95110, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:32:11.673102: step 95120, loss = 1.08 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 06:32:18.778426: step 95130, loss = 1.13 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 06:32:25.885762: step 95140, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 06:32:32.926463: step 95150, loss = 1.18 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 06:32:40.052920: step 95160, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 06:32:47.128587: step 95170, loss = 1.02 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 06:32:54.172103: step 95180, loss = 1.12 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 06:33:01.217328: step 95190, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 06:33:08.282983: step 95200, loss = 1.10 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 06:33:17.973591: step 95210, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 06:33:25.051881: step 95220, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:33:32.121849: step 95230, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:33:39.183235: step 95240, loss = 1.11 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 06:33:46.323356: step 95250, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 06:33:53.532257: step 95260, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 06:34:00.554490: step 95270, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 06:34:07.642533: step 95280, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 06:34:14.730035: step 95290, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 06:34:21.805240: step 95300, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 06:34:31.479044: step 95310, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 06:34:38.534717: step 95320, loss = 1.16 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 06:34:45.613297: step 95330, loss = 1.19 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 06:34:52.610030: step 95340, loss = 1.23 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 06:34:59.622684: step 95350, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 06:35:06.688089: step 95360, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 06:35:13.758850: step 95370, loss = 1.06 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 06:35:20.843326: step 95380, loss = 1.07 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 06:35:27.873765: step 95390, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 06:35:34.921472: step 95400, loss = 1.08 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 06:35:44.527832: step 95410, loss = 0.99 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 06:35:51.650025: step 95420, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 06:35:58.719694: step 95430, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 06:36:05.732016: step 95440, loss = 1.03 (47.3 examples/sec; 0.677 sec/batch)
2018-10-17 06:36:12.795061: step 95450, loss = 1.08 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 06:36:19.786230: step 95460, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:36:26.775374: step 95470, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 06:36:33.831902: step 95480, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 06:36:40.845787: step 95490, loss = 1.09 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:36:47.937262: step 95500, loss = 1.02 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 06:36:57.613300: step 95510, loss = 1.09 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 06:37:04.700867: step 95520, loss = 1.02 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 06:37:11.704143: step 95530, loss = 1.13 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:37:18.744719: step 95540, loss = 1.16 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 06:37:25.828262: step 95550, loss = 1.09 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 06:37:32.923177: step 95560, loss = 1.03 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 06:37:40.012361: step 95570, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 06:37:47.199892: step 95580, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 06:37:54.279542: step 95590, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 06:38:01.342310: step 95600, loss = 1.01 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 06:38:10.954924: step 95610, loss = 1.02 (47.9 examples/sec; 0.668 sec/batch)
2018-10-17 06:38:17.942083: step 95620, loss = 1.11 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 06:38:25.004956: step 95630, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 06:38:32.140165: step 95640, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 06:38:39.232092: step 95650, loss = 1.15 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:38:46.233024: step 95660, loss = 1.11 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 06:38:53.286057: step 95670, loss = 1.05 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 06:39:00.362580: step 95680, loss = 1.13 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 06:39:07.435894: step 95690, loss = 1.07 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 06:39:14.538510: step 95700, loss = 1.15 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 06:39:25.099447: step 95710, loss = 1.04 (47.8 examples/sec; 0.670 sec/batch)
2018-10-17 06:39:32.079187: step 95720, loss = 1.02 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 06:39:39.192450: step 95730, loss = 1.32 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 06:39:46.298738: step 95740, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:39:53.300633: step 95750, loss = 1.05 (45.9 examples/sec; 0.696 sec/batch)
2018-10-17 06:40:00.429362: step 95760, loss = 1.12 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 06:40:07.406826: step 95770, loss = 1.04 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 06:40:14.551869: step 95780, loss = 1.04 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 06:40:21.637676: step 95790, loss = 0.99 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 06:40:28.686767: step 95800, loss = 1.06 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 06:40:38.333529: step 95810, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 06:40:45.369238: step 95820, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 06:40:52.430496: step 95830, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 06:40:59.545110: step 95840, loss = 1.13 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 06:41:06.626465: step 95850, loss = 1.05 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 06:41:13.624610: step 95860, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 06:41:20.674240: step 95870, loss = 1.04 (43.3 examples/sec; 0.740 sec/batch)
2018-10-17 06:41:27.765467: step 95880, loss = 1.08 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 06:41:34.872852: step 95890, loss = 1.15 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 06:41:41.936769: step 95900, loss = 1.07 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 06:41:51.549179: step 95910, loss = 1.08 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 06:41:58.600032: step 95920, loss = 1.12 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 06:42:05.714569: step 95930, loss = 0.99 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 06:42:12.831198: step 95940, loss = 1.21 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 06:42:19.871039: step 95950, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 06:42:26.884639: step 95960, loss = 1.04 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 06:42:34.105219: step 95970, loss = 1.09 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 06:42:41.308462: step 95980, loss = 1.03 (42.7 examples/sec; 0.749 sec/batch)
2018-10-17 06:42:48.404912: step 95990, loss = 1.06 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 06:42:55.536080: step 96000, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 06:43:06.105736: step 96010, loss = 1.00 (47.3 examples/sec; 0.677 sec/batch)
2018-10-17 06:43:13.049482: step 96020, loss = 1.02 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 06:43:20.163153: step 96030, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 06:43:27.275829: step 96040, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:43:34.348754: step 96050, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 06:43:41.469859: step 96060, loss = 1.15 (42.8 examples/sec; 0.748 sec/batch)
2018-10-17 06:43:48.629668: step 96070, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 06:43:55.734870: step 96080, loss = 1.06 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 06:44:02.800590: step 96090, loss = 1.06 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 06:44:09.834664: step 96100, loss = 1.01 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 06:44:19.491837: step 96110, loss = 1.03 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 06:44:26.588866: step 96120, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 06:44:33.593318: step 96130, loss = 1.01 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 06:44:40.639200: step 96140, loss = 1.15 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 06:44:47.772557: step 96150, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 06:44:54.876353: step 96160, loss = 1.05 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 06:45:01.908478: step 96170, loss = 1.07 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 06:45:09.059531: step 96180, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 06:45:16.119295: step 96190, loss = 1.04 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 06:45:23.150605: step 96200, loss = 1.04 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 06:45:32.756568: step 96210, loss = 1.02 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 06:45:39.806031: step 96220, loss = 1.06 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 06:45:46.860944: step 96230, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:45:53.840183: step 96240, loss = 1.12 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 06:46:00.870292: step 96250, loss = 1.01 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 06:46:07.978532: step 96260, loss = 1.15 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 06:46:14.995490: step 96270, loss = 1.16 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 06:46:22.156327: step 96280, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 06:46:29.201198: step 96290, loss = 1.13 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 06:46:36.216002: step 96300, loss = 1.03 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 06:46:45.892746: step 96310, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 06:46:53.005506: step 96320, loss = 1.20 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 06:47:00.127394: step 96330, loss = 1.25 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 06:47:07.186760: step 96340, loss = 1.00 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 06:47:14.287998: step 96350, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 06:47:21.426278: step 96360, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 06:47:28.471940: step 96370, loss = 1.07 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 06:47:35.590368: step 96380, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:47:42.740625: step 96390, loss = 1.16 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 06:47:49.821374: step 96400, loss = 1.04 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 06:47:59.608853: step 96410, loss = 1.00 (48.3 examples/sec; 0.663 sec/batch)
2018-10-17 06:48:06.661441: step 96420, loss = 0.99 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 06:48:13.742804: step 96430, loss = 1.09 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 06:48:20.852692: step 96440, loss = 1.06 (47.8 examples/sec; 0.670 sec/batch)
2018-10-17 06:48:27.966412: step 96450, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 06:48:35.084139: step 96460, loss = 1.03 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 06:48:42.140490: step 96470, loss = 1.00 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 06:48:49.188526: step 96480, loss = 1.07 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 06:48:56.302010: step 96490, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:49:03.301036: step 96500, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 06:49:13.011077: step 96510, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 06:49:20.018379: step 96520, loss = 1.00 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 06:49:27.132244: step 96530, loss = 1.03 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:49:34.169439: step 96540, loss = 1.13 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:49:41.163573: step 96550, loss = 1.03 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 06:49:48.222442: step 96560, loss = 1.02 (43.0 examples/sec; 0.744 sec/batch)
2018-10-17 06:49:55.262740: step 96570, loss = 1.10 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 06:50:02.389926: step 96580, loss = 1.11 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 06:50:09.496031: step 96590, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 06:50:16.546828: step 96600, loss = 1.16 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 06:50:26.499983: step 96610, loss = 0.99 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 06:50:33.449642: step 96620, loss = 0.99 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 06:50:40.519562: step 96630, loss = 1.05 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 06:50:47.510520: step 96640, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:50:54.542577: step 96650, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:51:01.591105: step 96660, loss = 1.04 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 06:51:08.707196: step 96670, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 06:51:15.781351: step 96680, loss = 1.06 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 06:51:22.824446: step 96690, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 06:51:29.861770: step 96700, loss = 1.10 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 06:51:39.505424: step 96710, loss = 0.99 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 06:51:46.539037: step 96720, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 06:51:53.546362: step 96730, loss = 1.00 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 06:52:00.573572: step 96740, loss = 1.07 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:52:07.639341: step 96750, loss = 1.17 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 06:52:14.690040: step 96760, loss = 1.05 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 06:52:21.745321: step 96770, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 06:52:28.760992: step 96780, loss = 1.08 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 06:52:35.921409: step 96790, loss = 1.06 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 06:52:43.021164: step 96800, loss = 1.13 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 06:52:52.578208: step 96810, loss = 1.02 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 06:52:59.604763: step 96820, loss = 1.09 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 06:53:06.682652: step 96830, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 06:53:13.722566: step 96840, loss = 1.04 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 06:53:20.784797: step 96850, loss = 1.04 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 06:53:27.811472: step 96860, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 06:53:34.889044: step 96870, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 06:53:41.960014: step 96880, loss = 1.08 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 06:53:49.042632: step 96890, loss = 1.01 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 06:53:56.011612: step 96900, loss = 1.10 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:54:05.746711: step 96910, loss = 1.08 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 06:54:12.765628: step 96920, loss = 1.05 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 06:54:19.825465: step 96930, loss = 1.11 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 06:54:26.991860: step 96940, loss = 0.99 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 06:54:33.979806: step 96950, loss = 1.06 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 06:54:41.007162: step 96960, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:54:48.023135: step 96970, loss = 0.99 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 06:54:55.095298: step 96980, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 06:55:02.113651: step 96990, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 06:55:09.189293: step 97000, loss = 1.02 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 06:55:18.760369: step 97010, loss = 1.11 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 06:55:25.818329: step 97020, loss = 1.10 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 06:55:32.886217: step 97030, loss = 1.09 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 06:55:39.885382: step 97040, loss = 1.35 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:55:46.916847: step 97050, loss = 1.08 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 06:55:53.956626: step 97060, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 06:56:01.028320: step 97070, loss = 1.07 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 06:56:07.996735: step 97080, loss = 1.14 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 06:56:15.118465: step 97090, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 06:56:22.188193: step 97100, loss = 1.04 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 06:56:32.245713: step 97110, loss = 1.07 (48.0 examples/sec; 0.667 sec/batch)
2018-10-17 06:56:39.260468: step 97120, loss = 1.00 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 06:56:46.333304: step 97130, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 06:56:53.311862: step 97140, loss = 1.16 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 06:57:00.334204: step 97150, loss = 1.03 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 06:57:07.286861: step 97160, loss = 1.05 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 06:57:14.386777: step 97170, loss = 1.02 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 06:57:21.584059: step 97180, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 06:57:28.723442: step 97190, loss = 1.01 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 06:57:35.865397: step 97200, loss = 1.12 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 06:57:45.438950: step 97210, loss = 1.01 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 06:57:52.508953: step 97220, loss = 1.00 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 06:57:59.494388: step 97230, loss = 1.16 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 06:58:06.535189: step 97240, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 06:58:13.592785: step 97250, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 06:58:20.724927: step 97260, loss = 1.08 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 06:58:27.739112: step 97270, loss = 1.01 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 06:58:34.795598: step 97280, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 06:58:41.789400: step 97290, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 06:58:48.853748: step 97300, loss = 1.05 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 06:58:58.641399: step 97310, loss = 1.06 (47.6 examples/sec; 0.672 sec/batch)
2018-10-17 06:59:05.622772: step 97320, loss = 0.99 (48.1 examples/sec; 0.665 sec/batch)
2018-10-17 06:59:12.768520: step 97330, loss = 1.03 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 06:59:19.879321: step 97340, loss = 1.08 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 06:59:26.989701: step 97350, loss = 1.10 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 06:59:34.136337: step 97360, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 06:59:41.128301: step 97370, loss = 1.11 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 06:59:48.177241: step 97380, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 06:59:55.396847: step 97390, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 07:00:02.404175: step 97400, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:00:12.009020: step 97410, loss = 1.16 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 07:00:19.032340: step 97420, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 07:00:26.077076: step 97430, loss = 1.02 (44.7 examples/sec; 0.717 sec/batch)
2018-10-17 07:00:33.231414: step 97440, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 07:00:40.381903: step 97450, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 07:00:47.407928: step 97460, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:00:54.537076: step 97470, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:01:01.568254: step 97480, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 07:01:08.642528: step 97490, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 07:01:15.716523: step 97500, loss = 1.09 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 07:01:25.611024: step 97510, loss = 1.00 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 07:01:32.650884: step 97520, loss = 1.18 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 07:01:39.717323: step 97530, loss = 1.07 (46.1 examples/sec; 0.693 sec/batch)
2018-10-17 07:01:46.702542: step 97540, loss = 1.11 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 07:01:53.749151: step 97550, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 07:02:00.763811: step 97560, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 07:02:07.800068: step 97570, loss = 1.05 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:02:14.920579: step 97580, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:02:21.949455: step 97590, loss = 1.14 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 07:02:29.038753: step 97600, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:02:38.644425: step 97610, loss = 1.00 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 07:02:45.771015: step 97620, loss = 1.01 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 07:02:52.801212: step 97630, loss = 1.05 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:02:59.862724: step 97640, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 07:03:06.905941: step 97650, loss = 1.03 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 07:03:13.947939: step 97660, loss = 1.17 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 07:03:20.950101: step 97670, loss = 1.04 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 07:03:28.053640: step 97680, loss = 1.22 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 07:03:35.121964: step 97690, loss = 1.04 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 07:03:42.216187: step 97700, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 07:03:51.819702: step 97710, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:03:58.885242: step 97720, loss = 1.08 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 07:04:05.956258: step 97730, loss = 1.06 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 07:04:12.995573: step 97740, loss = 1.03 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 07:04:20.039850: step 97750, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:04:27.119202: step 97760, loss = 1.09 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 07:04:34.108028: step 97770, loss = 1.02 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 07:04:41.229775: step 97780, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:04:48.277099: step 97790, loss = 1.04 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 07:04:55.280958: step 97800, loss = 1.05 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 07:05:04.885404: step 97810, loss = 1.08 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 07:05:11.903058: step 97820, loss = 1.09 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 07:05:18.955954: step 97830, loss = 1.00 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 07:05:26.037267: step 97840, loss = 1.02 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 07:05:33.124413: step 97850, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 07:05:40.133310: step 97860, loss = 1.08 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 07:05:47.205887: step 97870, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 07:05:54.209110: step 97880, loss = 1.05 (47.3 examples/sec; 0.676 sec/batch)
2018-10-17 07:06:01.308928: step 97890, loss = 1.19 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 07:06:08.305612: step 97900, loss = 1.07 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 07:06:17.974180: step 97910, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:06:24.906044: step 97920, loss = 1.01 (48.0 examples/sec; 0.667 sec/batch)
2018-10-17 07:06:32.003140: step 97930, loss = 1.31 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 07:06:39.016183: step 97940, loss = 1.03 (47.5 examples/sec; 0.674 sec/batch)
2018-10-17 07:06:46.124254: step 97950, loss = 1.26 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 07:06:53.128382: step 97960, loss = 1.05 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:07:00.226613: step 97970, loss = 1.18 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 07:07:07.264933: step 97980, loss = 1.16 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 07:07:14.282029: step 97990, loss = 1.01 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 07:07:21.332639: step 98000, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 07:07:30.925145: step 98010, loss = 1.12 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 07:07:37.983675: step 98020, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 07:07:45.003942: step 98030, loss = 1.04 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 07:07:52.080654: step 98040, loss = 1.07 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 07:07:59.075542: step 98050, loss = 1.01 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 07:08:06.109905: step 98060, loss = 1.11 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 07:08:13.214281: step 98070, loss = 1.03 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 07:08:20.353093: step 98080, loss = 1.07 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 07:08:27.431001: step 98090, loss = 1.04 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 07:08:34.516363: step 98100, loss = 1.10 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 07:08:44.149946: step 98110, loss = 1.10 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:08:51.181396: step 98120, loss = 1.04 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 07:08:58.165660: step 98130, loss = 1.10 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 07:09:05.208287: step 98140, loss = 1.20 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:09:12.295056: step 98150, loss = 1.17 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 07:09:19.335475: step 98160, loss = 1.02 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 07:09:26.374835: step 98170, loss = 1.14 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 07:09:33.397437: step 98180, loss = 1.07 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 07:09:40.482281: step 98190, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 07:09:47.582440: step 98200, loss = 1.01 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 07:09:57.235001: step 98210, loss = 1.04 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 07:10:04.301994: step 98220, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 07:10:11.348943: step 98230, loss = 0.99 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 07:10:18.388620: step 98240, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 07:10:25.495609: step 98250, loss = 1.13 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 07:10:32.632158: step 98260, loss = 0.99 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 07:10:39.641072: step 98270, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:10:46.726567: step 98280, loss = 1.00 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 07:10:53.859893: step 98290, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 07:11:00.922470: step 98300, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:11:10.748884: step 98310, loss = 1.01 (48.0 examples/sec; 0.666 sec/batch)
2018-10-17 07:11:17.852664: step 98320, loss = 1.04 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 07:11:24.964448: step 98330, loss = 1.13 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 07:11:32.001462: step 98340, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 07:11:39.072716: step 98350, loss = 0.99 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 07:11:46.050400: step 98360, loss = 1.00 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 07:11:53.151743: step 98370, loss = 1.07 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 07:12:00.211632: step 98380, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:12:07.257869: step 98390, loss = 1.19 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 07:12:14.309512: step 98400, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:12:23.912783: step 98410, loss = 1.04 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 07:12:30.968980: step 98420, loss = 1.07 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 07:12:38.088864: step 98430, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 07:12:45.155178: step 98440, loss = 1.02 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 07:12:52.296596: step 98450, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 07:12:59.412513: step 98460, loss = 1.07 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:13:06.516470: step 98470, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 07:13:13.593914: step 98480, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 07:13:20.691058: step 98490, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:13:27.780280: step 98500, loss = 1.13 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 07:13:37.449347: step 98510, loss = 1.03 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 07:13:44.443127: step 98520, loss = 1.09 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 07:13:51.574524: step 98530, loss = 1.03 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 07:13:58.574143: step 98540, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:14:05.733030: step 98550, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:14:12.772947: step 98560, loss = 1.10 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 07:14:19.880964: step 98570, loss = 0.99 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 07:14:26.919350: step 98580, loss = 1.06 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 07:14:33.999677: step 98590, loss = 1.13 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 07:14:41.078064: step 98600, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 07:14:50.642781: step 98610, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 07:14:57.653320: step 98620, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 07:15:04.667033: step 98630, loss = 1.00 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 07:15:11.741947: step 98640, loss = 1.18 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:15:18.724973: step 98650, loss = 1.11 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 07:15:25.712634: step 98660, loss = 1.16 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 07:15:32.812818: step 98670, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 07:15:39.910290: step 98680, loss = 1.26 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 07:15:46.997320: step 98690, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:15:54.070111: step 98700, loss = 1.10 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 07:16:03.766104: step 98710, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 07:16:10.745386: step 98720, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:16:17.808286: step 98730, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 07:16:24.914632: step 98740, loss = 1.22 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:16:31.957188: step 98750, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 07:16:39.023737: step 98760, loss = 1.07 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 07:16:46.041103: step 98770, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 07:16:53.089575: step 98780, loss = 0.99 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:17:00.176014: step 98790, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 07:17:07.166391: step 98800, loss = 1.02 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 07:17:16.761288: step 98810, loss = 1.03 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 07:17:23.791155: step 98820, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:17:30.837696: step 98830, loss = 1.04 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 07:17:37.837448: step 98840, loss = 1.13 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:17:44.953248: step 98850, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 07:17:52.035169: step 98860, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:17:59.020062: step 98870, loss = 1.15 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 07:18:06.037833: step 98880, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 07:18:13.036456: step 98890, loss = 1.27 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 07:18:20.078011: step 98900, loss = 1.12 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 07:18:29.657076: step 98910, loss = 1.05 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 07:18:36.668346: step 98920, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 07:18:43.699876: step 98930, loss = 1.08 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 07:18:50.806724: step 98940, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 07:18:57.848074: step 98950, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 07:19:04.886388: step 98960, loss = 1.05 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 07:19:11.927935: step 98970, loss = 1.04 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 07:19:18.957382: step 98980, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:19:25.999473: step 98990, loss = 1.06 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 07:19:33.034094: step 99000, loss = 1.09 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 07:19:43.353181: step 99010, loss = 1.12 (47.9 examples/sec; 0.668 sec/batch)
2018-10-17 07:19:50.315064: step 99020, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 07:19:57.317625: step 99030, loss = 1.01 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 07:20:04.369938: step 99040, loss = 1.03 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 07:20:11.428784: step 99050, loss = 1.14 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 07:20:18.426955: step 99060, loss = 1.04 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 07:20:25.547424: step 99070, loss = 1.05 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 07:20:32.623472: step 99080, loss = 1.05 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 07:20:39.666040: step 99090, loss = 1.04 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 07:20:46.709941: step 99100, loss = 1.20 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 07:20:56.279821: step 99110, loss = 1.01 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 07:21:03.469329: step 99120, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 07:21:10.506477: step 99130, loss = 1.06 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:21:17.589529: step 99140, loss = 1.06 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 07:21:24.548593: step 99150, loss = 1.21 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 07:21:31.562606: step 99160, loss = 1.06 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 07:21:38.616486: step 99170, loss = 1.14 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 07:21:45.691706: step 99180, loss = 1.03 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 07:21:52.818304: step 99190, loss = 1.02 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 07:21:59.924185: step 99200, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:22:09.532988: step 99210, loss = 1.03 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 07:22:16.547818: step 99220, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 07:22:23.610606: step 99230, loss = 1.04 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 07:22:30.671599: step 99240, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:22:37.706652: step 99250, loss = 1.12 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 07:22:44.738880: step 99260, loss = 1.04 (47.6 examples/sec; 0.673 sec/batch)
2018-10-17 07:22:51.809755: step 99270, loss = 1.08 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 07:22:58.893640: step 99280, loss = 1.08 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 07:23:05.979958: step 99290, loss = 1.15 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 07:23:13.010297: step 99300, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:23:22.669211: step 99310, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 07:23:29.689216: step 99320, loss = 1.08 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 07:23:36.675558: step 99330, loss = 1.18 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 07:23:43.768303: step 99340, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:23:50.872878: step 99350, loss = 1.18 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 07:23:57.903914: step 99360, loss = 1.02 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 07:24:04.998052: step 99370, loss = 1.05 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 07:24:12.096522: step 99380, loss = 1.11 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 07:24:19.112149: step 99390, loss = 1.02 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 07:24:26.232038: step 99400, loss = 0.99 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 07:24:35.911429: step 99410, loss = 1.05 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 07:24:43.007620: step 99420, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 07:24:50.044624: step 99430, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 07:24:57.169317: step 99440, loss = 1.02 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 07:25:04.200346: step 99450, loss = 1.12 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 07:25:11.278808: step 99460, loss = 0.99 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 07:25:18.454654: step 99470, loss = 0.99 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 07:25:25.573626: step 99480, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 07:25:32.671765: step 99490, loss = 1.07 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 07:25:39.731758: step 99500, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 07:25:49.318143: step 99510, loss = 1.00 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 07:25:56.309908: step 99520, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 07:26:03.413404: step 99530, loss = 1.14 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 07:26:10.501231: step 99540, loss = 1.01 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 07:26:17.589370: step 99550, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 07:26:24.662745: step 99560, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:26:31.672054: step 99570, loss = 1.08 (47.0 examples/sec; 0.680 sec/batch)
2018-10-17 07:26:38.721506: step 99580, loss = 1.03 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 07:26:45.745631: step 99590, loss = 1.14 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:26:52.775637: step 99600, loss = 1.01 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 07:27:02.561956: step 99610, loss = 1.08 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 07:27:09.534905: step 99620, loss = 1.10 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 07:27:16.615434: step 99630, loss = 1.00 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 07:27:23.648124: step 99640, loss = 1.18 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 07:27:30.665006: step 99650, loss = 1.04 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 07:27:37.728908: step 99660, loss = 1.04 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 07:27:44.816276: step 99670, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:27:51.823214: step 99680, loss = 1.07 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 07:27:58.853148: step 99690, loss = 1.11 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:28:05.836762: step 99700, loss = 1.11 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 07:28:15.327269: step 99710, loss = 1.08 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 07:28:22.352569: step 99720, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 07:28:29.350426: step 99730, loss = 1.02 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 07:28:36.389794: step 99740, loss = 1.09 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 07:28:43.480778: step 99750, loss = 1.04 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 07:28:50.545081: step 99760, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 07:28:57.562415: step 99770, loss = 1.08 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 07:29:04.580885: step 99780, loss = 1.10 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:29:11.723802: step 99790, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 07:29:18.858654: step 99800, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:29:28.944314: step 99810, loss = 1.17 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 07:29:35.944118: step 99820, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 07:29:43.027548: step 99830, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 07:29:50.069117: step 99840, loss = 1.12 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 07:29:57.112774: step 99850, loss = 1.20 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:30:04.097101: step 99860, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:30:11.223478: step 99870, loss = 1.06 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 07:30:18.239412: step 99880, loss = 1.09 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 07:30:25.414199: step 99890, loss = 1.08 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 07:30:32.514791: step 99900, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 07:30:42.043499: step 99910, loss = 1.04 (48.1 examples/sec; 0.665 sec/batch)
2018-10-17 07:30:49.110646: step 99920, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 07:30:56.132668: step 99930, loss = 1.07 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 07:31:03.134273: step 99940, loss = 0.99 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 07:31:10.135345: step 99950, loss = 1.06 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 07:31:17.292579: step 99960, loss = 1.06 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 07:31:24.366357: step 99970, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 07:31:31.453787: step 99980, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 07:31:38.487477: step 99990, loss = 1.10 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 07:31:45.527971: step 100000, loss = 1.08 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 07:31:58.163493: step 100010, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 07:32:04.964119: step 100020, loss = 1.00 (48.4 examples/sec; 0.661 sec/batch)
2018-10-17 07:32:11.941196: step 100030, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 07:32:19.063405: step 100040, loss = 1.02 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 07:32:26.124690: step 100050, loss = 1.12 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:32:33.319796: step 100060, loss = 1.03 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 07:32:40.475790: step 100070, loss = 1.11 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 07:32:47.504358: step 100080, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 07:32:54.566640: step 100090, loss = 1.04 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 07:33:01.694638: step 100100, loss = 0.99 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 07:33:11.375116: step 100110, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 07:33:18.417271: step 100120, loss = 1.15 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 07:33:25.422796: step 100130, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:33:32.462881: step 100140, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:33:39.570290: step 100150, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 07:33:46.637904: step 100160, loss = 1.25 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 07:33:53.717919: step 100170, loss = 1.19 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 07:34:00.804090: step 100180, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:34:07.839726: step 100190, loss = 0.99 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 07:34:14.855820: step 100200, loss = 1.02 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 07:34:24.530129: step 100210, loss = 1.14 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:34:31.601173: step 100220, loss = 1.13 (42.8 examples/sec; 0.747 sec/batch)
2018-10-17 07:34:38.687697: step 100230, loss = 1.04 (42.8 examples/sec; 0.748 sec/batch)
2018-10-17 07:34:45.760770: step 100240, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:34:52.795450: step 100250, loss = 1.16 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 07:34:59.865212: step 100260, loss = 1.09 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 07:35:07.031847: step 100270, loss = 1.02 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 07:35:14.090823: step 100280, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:35:21.199242: step 100290, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 07:35:28.287496: step 100300, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:35:38.306582: step 100310, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 07:35:45.435129: step 100320, loss = 0.99 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 07:35:52.613057: step 100330, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:35:59.666487: step 100340, loss = 1.11 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 07:36:06.713279: step 100350, loss = 1.11 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:36:13.767798: step 100360, loss = 1.09 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:36:20.825029: step 100370, loss = 1.06 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 07:36:27.898667: step 100380, loss = 1.02 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 07:36:34.944297: step 100390, loss = 1.05 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 07:36:42.005941: step 100400, loss = 1.10 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 07:36:51.966123: step 100410, loss = 1.00 (48.9 examples/sec; 0.655 sec/batch)
2018-10-17 07:36:59.101474: step 100420, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 07:37:06.197175: step 100430, loss = 1.05 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 07:37:13.297071: step 100440, loss = 0.99 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 07:37:20.360519: step 100450, loss = 1.02 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 07:37:27.415360: step 100460, loss = 1.12 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:37:34.415705: step 100470, loss = 1.17 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 07:37:41.508088: step 100480, loss = 1.06 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 07:37:48.588360: step 100490, loss = 1.02 (48.0 examples/sec; 0.667 sec/batch)
2018-10-17 07:37:55.606429: step 100500, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:38:05.308624: step 100510, loss = 1.13 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 07:38:12.306591: step 100520, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 07:38:19.442160: step 100530, loss = 1.18 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 07:38:26.484083: step 100540, loss = 1.05 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 07:38:33.680111: step 100550, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 07:38:40.748833: step 100560, loss = 1.15 (46.5 examples/sec; 0.687 sec/batch)
2018-10-17 07:38:47.887369: step 100570, loss = 1.13 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 07:38:54.932127: step 100580, loss = 1.09 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 07:39:02.021885: step 100590, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 07:39:09.138913: step 100600, loss = 1.07 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 07:39:19.020385: step 100610, loss = 1.03 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 07:39:26.074631: step 100620, loss = 1.09 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 07:39:33.199045: step 100630, loss = 1.13 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:39:40.228642: step 100640, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:39:47.361680: step 100650, loss = 1.16 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 07:39:54.419922: step 100660, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 07:40:01.554011: step 100670, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 07:40:08.593572: step 100680, loss = 1.00 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 07:40:15.647579: step 100690, loss = 1.15 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 07:40:22.720317: step 100700, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 07:40:32.390161: step 100710, loss = 1.06 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 07:40:39.454045: step 100720, loss = 0.99 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 07:40:46.541452: step 100730, loss = 1.12 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 07:40:53.641460: step 100740, loss = 1.08 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 07:41:00.719566: step 100750, loss = 1.04 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 07:41:07.875534: step 100760, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 07:41:14.977470: step 100770, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 07:41:22.038562: step 100780, loss = 1.09 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 07:41:29.111078: step 100790, loss = 1.06 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 07:41:36.153218: step 100800, loss = 0.99 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 07:41:46.128124: step 100810, loss = 1.18 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 07:41:53.134091: step 100820, loss = 1.02 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 07:42:00.225048: step 100830, loss = 1.02 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 07:42:07.203146: step 100840, loss = 1.16 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 07:42:14.254535: step 100850, loss = 1.05 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 07:42:21.277606: step 100860, loss = 1.02 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 07:42:28.342758: step 100870, loss = 1.06 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 07:42:35.406534: step 100880, loss = 1.09 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 07:42:42.480282: step 100890, loss = 1.00 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 07:42:49.468018: step 100900, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:42:59.134495: step 100910, loss = 1.01 (42.9 examples/sec; 0.745 sec/batch)
2018-10-17 07:43:06.142104: step 100920, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 07:43:13.241399: step 100930, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 07:43:20.320342: step 100940, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 07:43:27.473091: step 100950, loss = 1.06 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:43:34.479384: step 100960, loss = 1.10 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 07:43:41.530008: step 100970, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:43:48.525275: step 100980, loss = 1.09 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 07:43:55.636847: step 100990, loss = 1.27 (45.6 examples/sec; 0.703 sec/batch)
2018-10-17 07:44:02.753801: step 101000, loss = 1.02 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 07:44:12.547244: step 101010, loss = 1.24 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 07:44:19.541717: step 101020, loss = 1.00 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 07:44:26.586975: step 101030, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 07:44:33.646467: step 101040, loss = 1.14 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:44:40.684434: step 101050, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 07:44:47.741983: step 101060, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:44:54.929059: step 101070, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 07:45:01.998141: step 101080, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:45:09.089462: step 101090, loss = 1.02 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 07:45:16.221607: step 101100, loss = 1.16 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 07:45:25.799165: step 101110, loss = 1.00 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 07:45:32.787814: step 101120, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 07:45:39.862915: step 101130, loss = 1.08 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 07:45:46.921641: step 101140, loss = 1.01 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 07:45:54.060715: step 101150, loss = 1.01 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 07:46:01.108510: step 101160, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 07:46:08.187279: step 101170, loss = 1.13 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 07:46:15.301742: step 101180, loss = 1.16 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 07:46:22.343471: step 101190, loss = 1.02 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 07:46:29.519215: step 101200, loss = 1.08 (43.0 examples/sec; 0.744 sec/batch)
2018-10-17 07:46:39.136400: step 101210, loss = 1.03 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:46:46.111180: step 101220, loss = 1.10 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 07:46:53.182811: step 101230, loss = 1.13 (43.3 examples/sec; 0.740 sec/batch)
2018-10-17 07:47:00.297616: step 101240, loss = 1.08 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 07:47:07.384731: step 101250, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 07:47:14.396967: step 101260, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 07:47:21.524268: step 101270, loss = 1.04 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 07:47:28.591001: step 101280, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 07:47:35.658270: step 101290, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 07:47:42.737261: step 101300, loss = 0.99 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 07:47:52.536227: step 101310, loss = 1.10 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 07:47:59.570528: step 101320, loss = 1.09 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 07:48:06.526581: step 101330, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 07:48:13.645981: step 101340, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:48:20.687384: step 101350, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:48:27.642994: step 101360, loss = 1.12 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:48:34.727164: step 101370, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:48:41.775848: step 101380, loss = 1.02 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 07:48:48.892786: step 101390, loss = 1.07 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 07:48:56.007618: step 101400, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 07:49:05.714475: step 101410, loss = 1.05 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:49:12.798746: step 101420, loss = 1.06 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 07:49:19.938568: step 101430, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 07:49:26.969687: step 101440, loss = 1.09 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 07:49:34.107152: step 101450, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 07:49:41.130901: step 101460, loss = 1.10 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 07:49:48.228841: step 101470, loss = 1.01 (44.7 examples/sec; 0.717 sec/batch)
2018-10-17 07:49:55.394816: step 101480, loss = 1.00 (43.3 examples/sec; 0.738 sec/batch)
2018-10-17 07:50:02.557688: step 101490, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:50:09.628278: step 101500, loss = 1.06 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 07:50:19.234623: step 101510, loss = 1.02 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 07:50:26.286943: step 101520, loss = 1.05 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 07:50:33.450817: step 101530, loss = 1.01 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 07:50:40.517001: step 101540, loss = 1.00 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 07:50:47.598845: step 101550, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:50:54.719467: step 101560, loss = 1.01 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 07:51:01.771941: step 101570, loss = 1.09 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 07:51:08.846082: step 101580, loss = 1.10 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 07:51:15.912246: step 101590, loss = 1.05 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 07:51:22.960160: step 101600, loss = 1.23 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 07:51:32.659525: step 101610, loss = 1.05 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 07:51:39.705198: step 101620, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 07:51:46.837515: step 101630, loss = 1.06 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 07:51:53.872714: step 101640, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:52:00.958749: step 101650, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 07:52:08.073611: step 101660, loss = 1.11 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 07:52:15.090181: step 101670, loss = 1.05 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 07:52:22.168696: step 101680, loss = 1.09 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 07:52:29.231748: step 101690, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 07:52:36.356033: step 101700, loss = 1.07 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 07:52:46.024265: step 101710, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:52:53.102821: step 101720, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 07:53:00.146657: step 101730, loss = 1.17 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 07:53:07.208368: step 101740, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 07:53:14.362947: step 101750, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 07:53:21.454099: step 101760, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 07:53:28.482227: step 101770, loss = 1.14 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 07:53:35.598166: step 101780, loss = 1.18 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 07:53:42.711397: step 101790, loss = 1.07 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 07:53:49.780412: step 101800, loss = 1.09 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 07:53:59.448677: step 101810, loss = 1.13 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 07:54:06.559112: step 101820, loss = 0.99 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 07:54:13.660411: step 101830, loss = 1.00 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 07:54:20.734334: step 101840, loss = 1.01 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 07:54:27.794689: step 101850, loss = 1.10 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 07:54:34.778230: step 101860, loss = 1.09 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:54:41.879031: step 101870, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 07:54:48.868345: step 101880, loss = 1.17 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 07:54:55.977315: step 101890, loss = 1.07 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 07:55:03.096316: step 101900, loss = 1.03 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 07:55:12.875250: step 101910, loss = 1.07 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 07:55:19.849136: step 101920, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:55:26.904094: step 101930, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 07:55:33.976455: step 101940, loss = 1.09 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:55:41.055931: step 101950, loss = 1.04 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 07:55:48.162814: step 101960, loss = 1.10 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 07:55:55.183317: step 101970, loss = 1.07 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 07:56:02.271493: step 101980, loss = 1.07 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 07:56:09.296519: step 101990, loss = 1.02 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 07:56:16.360333: step 102000, loss = 1.00 (47.0 examples/sec; 0.682 sec/batch)
2018-10-17 07:56:25.854418: step 102010, loss = 0.99 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 07:56:32.936369: step 102020, loss = 0.99 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 07:56:40.162967: step 102030, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 07:56:47.232273: step 102040, loss = 1.06 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 07:56:54.310976: step 102050, loss = 1.02 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 07:57:01.399420: step 102060, loss = 1.07 (43.6 examples/sec; 0.735 sec/batch)
2018-10-17 07:57:08.472479: step 102070, loss = 1.14 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 07:57:15.552510: step 102080, loss = 1.07 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:57:22.685576: step 102090, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 07:57:29.723554: step 102100, loss = 1.26 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 07:57:39.416025: step 102110, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 07:57:46.499426: step 102120, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 07:57:53.497849: step 102130, loss = 1.08 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 07:58:00.559159: step 102140, loss = 0.99 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:58:07.567138: step 102150, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 07:58:14.626428: step 102160, loss = 1.15 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 07:58:21.772219: step 102170, loss = 1.13 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 07:58:28.830631: step 102180, loss = 1.14 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 07:58:35.931909: step 102190, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 07:58:42.993373: step 102200, loss = 1.01 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 07:58:52.491224: step 102210, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 07:58:59.598545: step 102220, loss = 1.06 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 07:59:06.695070: step 102230, loss = 1.01 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 07:59:13.753322: step 102240, loss = 1.07 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 07:59:20.784018: step 102250, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:59:27.813025: step 102260, loss = 0.99 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 07:59:34.816550: step 102270, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 07:59:41.883044: step 102280, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 07:59:49.012543: step 102290, loss = 1.09 (43.2 examples/sec; 0.742 sec/batch)
2018-10-17 07:59:56.075598: step 102300, loss = 1.13 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 08:00:05.586248: step 102310, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 08:00:12.682252: step 102320, loss = 1.13 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 08:00:19.713451: step 102330, loss = 1.08 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 08:00:26.792601: step 102340, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 08:00:33.909342: step 102350, loss = 0.99 (43.0 examples/sec; 0.744 sec/batch)
2018-10-17 08:00:40.965133: step 102360, loss = 1.12 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 08:00:48.040913: step 102370, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:00:55.145408: step 102380, loss = 1.36 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 08:01:02.144104: step 102390, loss = 1.09 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 08:01:09.243036: step 102400, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 08:01:18.861323: step 102410, loss = 1.00 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 08:01:25.961893: step 102420, loss = 1.00 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 08:01:33.049062: step 102430, loss = 1.15 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:01:40.183084: step 102440, loss = 1.08 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 08:01:47.296695: step 102450, loss = 1.17 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 08:01:54.308753: step 102460, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 08:02:01.357925: step 102470, loss = 1.00 (47.8 examples/sec; 0.670 sec/batch)
2018-10-17 08:02:08.451451: step 102480, loss = 1.11 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:02:15.516020: step 102490, loss = 1.11 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 08:02:22.576110: step 102500, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 08:02:32.185147: step 102510, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 08:02:39.263992: step 102520, loss = 1.09 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 08:02:46.324871: step 102530, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 08:02:53.464162: step 102540, loss = 1.04 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 08:03:00.476756: step 102550, loss = 1.09 (47.6 examples/sec; 0.672 sec/batch)
2018-10-17 08:03:07.554254: step 102560, loss = 1.04 (45.6 examples/sec; 0.703 sec/batch)
2018-10-17 08:03:14.610902: step 102570, loss = 1.07 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:03:21.684856: step 102580, loss = 1.03 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 08:03:28.818559: step 102590, loss = 1.06 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:03:35.901749: step 102600, loss = 1.10 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 08:03:46.405605: step 102610, loss = 1.08 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 08:03:53.351903: step 102620, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 08:04:00.465267: step 102630, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:04:07.488939: step 102640, loss = 1.00 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 08:04:14.516817: step 102650, loss = 0.99 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 08:04:21.532128: step 102660, loss = 1.09 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 08:04:28.599718: step 102670, loss = 1.02 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 08:04:35.663461: step 102680, loss = 1.06 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 08:04:42.766877: step 102690, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 08:04:49.805689: step 102700, loss = 1.07 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 08:04:59.384952: step 102710, loss = 1.12 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:05:06.459446: step 102720, loss = 0.99 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 08:05:13.540971: step 102730, loss = 1.22 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 08:05:20.582942: step 102740, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 08:05:27.678052: step 102750, loss = 1.05 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 08:05:34.792115: step 102760, loss = 1.08 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 08:05:41.901533: step 102770, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 08:05:49.022186: step 102780, loss = 0.99 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 08:05:56.124241: step 102790, loss = 1.04 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 08:06:03.198788: step 102800, loss = 1.00 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 08:06:13.264380: step 102810, loss = 1.06 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 08:06:20.278730: step 102820, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:06:27.287496: step 102830, loss = 1.02 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 08:06:34.361310: step 102840, loss = 1.06 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:06:41.420162: step 102850, loss = 1.24 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 08:06:48.492945: step 102860, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:06:55.607726: step 102870, loss = 1.03 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 08:07:02.732753: step 102880, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 08:07:09.819883: step 102890, loss = 1.08 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 08:07:16.898275: step 102900, loss = 1.14 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 08:07:26.681743: step 102910, loss = 1.00 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 08:07:33.650771: step 102920, loss = 1.07 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 08:07:40.700653: step 102930, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:07:47.806750: step 102940, loss = 1.06 (47.9 examples/sec; 0.668 sec/batch)
2018-10-17 08:07:54.910944: step 102950, loss = 1.12 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 08:08:02.039136: step 102960, loss = 1.25 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 08:08:09.189540: step 102970, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 08:08:16.275819: step 102980, loss = 1.11 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 08:08:23.329069: step 102990, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 08:08:30.412738: step 103000, loss = 1.01 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 08:08:40.291424: step 103010, loss = 1.09 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 08:08:47.395367: step 103020, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 08:08:54.441513: step 103030, loss = 1.13 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 08:09:01.504620: step 103040, loss = 1.01 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 08:09:08.665264: step 103050, loss = 1.06 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 08:09:15.699645: step 103060, loss = 1.03 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:09:22.818889: step 103070, loss = 1.05 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 08:09:29.907165: step 103080, loss = 1.07 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:09:37.059595: step 103090, loss = 1.00 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 08:09:44.227943: step 103100, loss = 1.14 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 08:09:54.150494: step 103110, loss = 0.99 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 08:10:01.157912: step 103120, loss = 1.20 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 08:10:08.221441: step 103130, loss = 1.09 (46.1 examples/sec; 0.693 sec/batch)
2018-10-17 08:10:15.285537: step 103140, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 08:10:22.347459: step 103150, loss = 1.07 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 08:10:29.426986: step 103160, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 08:10:36.505728: step 103170, loss = 1.02 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 08:10:43.546894: step 103180, loss = 0.99 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 08:10:50.694512: step 103190, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:10:57.760817: step 103200, loss = 1.04 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 08:11:07.418605: step 103210, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 08:11:14.484028: step 103220, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 08:11:21.573481: step 103230, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:11:28.729803: step 103240, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:11:35.861658: step 103250, loss = 1.05 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 08:11:42.912831: step 103260, loss = 1.05 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 08:11:50.103363: step 103270, loss = 1.13 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 08:11:57.094927: step 103280, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:12:04.115936: step 103290, loss = 1.01 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:12:11.171259: step 103300, loss = 1.00 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 08:12:20.840240: step 103310, loss = 1.04 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 08:12:27.865689: step 103320, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 08:12:34.875152: step 103330, loss = 1.01 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 08:12:41.973259: step 103340, loss = 1.12 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:12:48.999332: step 103350, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 08:12:56.090768: step 103360, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 08:13:03.160719: step 103370, loss = 1.00 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 08:13:10.252787: step 103380, loss = 1.07 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 08:13:17.336658: step 103390, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:13:24.411262: step 103400, loss = 1.06 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 08:13:33.998872: step 103410, loss = 1.06 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 08:13:41.077091: step 103420, loss = 1.12 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 08:13:48.124123: step 103430, loss = 1.31 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 08:13:55.215284: step 103440, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:14:02.284938: step 103450, loss = 1.17 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 08:14:09.406634: step 103460, loss = 1.05 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 08:14:16.497044: step 103470, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 08:14:23.609403: step 103480, loss = 0.99 (47.2 examples/sec; 0.677 sec/batch)
2018-10-17 08:14:30.678079: step 103490, loss = 1.08 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 08:14:37.842829: step 103500, loss = 1.06 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 08:14:47.422482: step 103510, loss = 1.18 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 08:14:54.481115: step 103520, loss = 1.15 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 08:15:01.554132: step 103530, loss = 1.02 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 08:15:08.666685: step 103540, loss = 1.05 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 08:15:15.728811: step 103550, loss = 1.10 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 08:15:22.824245: step 103560, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 08:15:29.908090: step 103570, loss = 1.05 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 08:15:37.032896: step 103580, loss = 1.21 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 08:15:44.154922: step 103590, loss = 1.11 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 08:15:51.224900: step 103600, loss = 0.99 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 08:16:00.853180: step 103610, loss = 1.03 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 08:16:07.981629: step 103620, loss = 1.19 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:16:15.049648: step 103630, loss = 1.02 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 08:16:22.193129: step 103640, loss = 0.99 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 08:16:29.226165: step 103650, loss = 1.25 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 08:16:36.306137: step 103660, loss = 1.04 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 08:16:43.390501: step 103670, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 08:16:50.443128: step 103680, loss = 0.99 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 08:16:57.535150: step 103690, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:17:04.515234: step 103700, loss = 1.02 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 08:17:14.093805: step 103710, loss = 1.11 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 08:17:21.131004: step 103720, loss = 1.02 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 08:17:28.181562: step 103730, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 08:17:35.243100: step 103740, loss = 1.06 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 08:17:42.324503: step 103750, loss = 1.09 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 08:17:49.398045: step 103760, loss = 1.01 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 08:17:56.418593: step 103770, loss = 1.05 (47.2 examples/sec; 0.677 sec/batch)
2018-10-17 08:18:03.499616: step 103780, loss = 1.22 (47.4 examples/sec; 0.676 sec/batch)
2018-10-17 08:18:10.514013: step 103790, loss = 1.05 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 08:18:17.593863: step 103800, loss = 1.08 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 08:18:27.180925: step 103810, loss = 1.13 (47.4 examples/sec; 0.675 sec/batch)
2018-10-17 08:18:34.156383: step 103820, loss = 1.13 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 08:18:41.185755: step 103830, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 08:18:48.271526: step 103840, loss = 1.05 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 08:18:55.325557: step 103850, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:19:02.378659: step 103860, loss = 1.04 (47.6 examples/sec; 0.673 sec/batch)
2018-10-17 08:19:09.421114: step 103870, loss = 1.09 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 08:19:16.463802: step 103880, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:19:23.572186: step 103890, loss = 1.00 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 08:19:30.593099: step 103900, loss = 1.05 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 08:19:40.648399: step 103910, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 08:19:47.777933: step 103920, loss = 0.99 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 08:19:54.937842: step 103930, loss = 1.10 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 08:20:01.989591: step 103940, loss = 1.15 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 08:20:09.013065: step 103950, loss = 1.07 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 08:20:16.099865: step 103960, loss = 1.01 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 08:20:23.161635: step 103970, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 08:20:30.262816: step 103980, loss = 1.02 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 08:20:37.310813: step 103990, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:20:44.367336: step 104000, loss = 1.01 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 08:20:53.972599: step 104010, loss = 1.05 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 08:21:00.991365: step 104020, loss = 1.01 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 08:21:08.000280: step 104030, loss = 1.11 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 08:21:15.082721: step 104040, loss = 1.01 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 08:21:22.143783: step 104050, loss = 1.06 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 08:21:29.234614: step 104060, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 08:21:36.312491: step 104070, loss = 1.02 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 08:21:43.471839: step 104080, loss = 1.18 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 08:21:50.574149: step 104090, loss = 1.22 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 08:21:57.654310: step 104100, loss = 0.99 (47.5 examples/sec; 0.674 sec/batch)
2018-10-17 08:22:07.292327: step 104110, loss = 1.21 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 08:22:14.360924: step 104120, loss = 1.01 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 08:22:21.391273: step 104130, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 08:22:28.439638: step 104140, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 08:22:35.403455: step 104150, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 08:22:42.486871: step 104160, loss = 1.06 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 08:22:49.543681: step 104170, loss = 1.04 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 08:22:56.613351: step 104180, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 08:23:03.713713: step 104190, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 08:23:10.692850: step 104200, loss = 1.03 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 08:23:20.659789: step 104210, loss = 1.04 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 08:23:27.693151: step 104220, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:23:34.740162: step 104230, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 08:23:41.753257: step 104240, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 08:23:48.816465: step 104250, loss = 1.12 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:23:55.809266: step 104260, loss = 1.11 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 08:24:02.956491: step 104270, loss = 1.14 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:24:10.109770: step 104280, loss = 1.10 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 08:24:17.208251: step 104290, loss = 1.06 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 08:24:24.234115: step 104300, loss = 1.06 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 08:24:34.710702: step 104310, loss = 1.14 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 08:24:41.663017: step 104320, loss = 1.00 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 08:24:48.744351: step 104330, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:24:55.819636: step 104340, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 08:25:02.909076: step 104350, loss = 1.03 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 08:25:09.987931: step 104360, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 08:25:17.053823: step 104370, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 08:25:24.123057: step 104380, loss = 1.04 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 08:25:31.177290: step 104390, loss = 1.17 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 08:25:38.243922: step 104400, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 08:25:48.218332: step 104410, loss = 1.02 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 08:25:55.325254: step 104420, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:26:02.367342: step 104430, loss = 0.99 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 08:26:09.368964: step 104440, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 08:26:16.422405: step 104450, loss = 1.05 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 08:26:23.468250: step 104460, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 08:26:30.588112: step 104470, loss = 0.99 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 08:26:37.596544: step 104480, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 08:26:44.673316: step 104490, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:26:51.760592: step 104500, loss = 1.05 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 08:27:01.403837: step 104510, loss = 1.13 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 08:27:08.489870: step 104520, loss = 1.09 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 08:27:15.585529: step 104530, loss = 1.00 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 08:27:22.638425: step 104540, loss = 1.04 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 08:27:29.745441: step 104550, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:27:36.816779: step 104560, loss = 1.29 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 08:27:43.899388: step 104570, loss = 1.00 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 08:27:50.947418: step 104580, loss = 1.12 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 08:27:58.024268: step 104590, loss = 1.19 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 08:28:05.093200: step 104600, loss = 1.05 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 08:28:14.670700: step 104610, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 08:28:21.664044: step 104620, loss = 1.00 (48.1 examples/sec; 0.666 sec/batch)
2018-10-17 08:28:28.762021: step 104630, loss = 1.02 (46.8 examples/sec; 0.683 sec/batch)
2018-10-17 08:28:35.770534: step 104640, loss = 1.01 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 08:28:42.904776: step 104650, loss = 1.10 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 08:28:50.027656: step 104660, loss = 1.18 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 08:28:57.095916: step 104670, loss = 1.13 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 08:29:04.166321: step 104680, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:29:11.244244: step 104690, loss = 1.04 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 08:29:18.285539: step 104700, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 08:29:27.889722: step 104710, loss = 1.04 (47.8 examples/sec; 0.669 sec/batch)
2018-10-17 08:29:34.956461: step 104720, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:29:41.963032: step 104730, loss = 1.08 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 08:29:49.099858: step 104740, loss = 1.10 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 08:29:56.269838: step 104750, loss = 1.12 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 08:30:03.322077: step 104760, loss = 1.04 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:30:10.354052: step 104770, loss = 1.09 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 08:30:17.470992: step 104780, loss = 1.03 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 08:30:24.607482: step 104790, loss = 1.11 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 08:30:31.639327: step 104800, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 08:30:41.304625: step 104810, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:30:48.346122: step 104820, loss = 1.17 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 08:30:55.436805: step 104830, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:31:02.456345: step 104840, loss = 1.11 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 08:31:09.463366: step 104850, loss = 1.16 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 08:31:16.576086: step 104860, loss = 1.00 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 08:31:23.590791: step 104870, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 08:31:30.654112: step 104880, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:31:37.725099: step 104890, loss = 1.03 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:31:44.784788: step 104900, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 08:31:54.386148: step 104910, loss = 1.05 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 08:32:01.457247: step 104920, loss = 1.12 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:32:08.647465: step 104930, loss = 1.06 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 08:32:15.707488: step 104940, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:32:22.743639: step 104950, loss = 1.16 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 08:32:29.837092: step 104960, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 08:32:36.872872: step 104970, loss = 1.02 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 08:32:43.923235: step 104980, loss = 1.05 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 08:32:51.033294: step 104990, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 08:32:58.065098: step 105000, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:33:11.353281: step 105010, loss = 1.11 (47.9 examples/sec; 0.668 sec/batch)
2018-10-17 08:33:18.189079: step 105020, loss = 1.00 (47.8 examples/sec; 0.670 sec/batch)
2018-10-17 08:33:25.045215: step 105030, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 08:33:32.136405: step 105040, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:33:39.270626: step 105050, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:33:46.503349: step 105060, loss = 1.01 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 08:33:53.603208: step 105070, loss = 1.24 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:34:00.751975: step 105080, loss = 1.06 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:34:07.772805: step 105090, loss = 0.99 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:34:14.920749: step 105100, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 08:34:24.560967: step 105110, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 08:34:31.572771: step 105120, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:34:38.666803: step 105130, loss = 1.02 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 08:34:45.750231: step 105140, loss = 1.25 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 08:34:52.818157: step 105150, loss = 1.04 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 08:34:59.967791: step 105160, loss = 1.14 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 08:35:06.981050: step 105170, loss = 1.05 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 08:35:14.123125: step 105180, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 08:35:21.194212: step 105190, loss = 1.00 (45.7 examples/sec; 0.699 sec/batch)
2018-10-17 08:35:28.315090: step 105200, loss = 1.06 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:35:38.101771: step 105210, loss = 1.02 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 08:35:45.149452: step 105220, loss = 1.02 (42.5 examples/sec; 0.752 sec/batch)
2018-10-17 08:35:52.226295: step 105230, loss = 1.02 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 08:35:59.265479: step 105240, loss = 1.00 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 08:36:06.384041: step 105250, loss = 1.15 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 08:36:13.493700: step 105260, loss = 1.02 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 08:36:20.591557: step 105270, loss = 1.19 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 08:36:27.712189: step 105280, loss = 1.15 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 08:36:34.808484: step 105290, loss = 1.08 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 08:36:41.874075: step 105300, loss = 1.00 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 08:36:51.775578: step 105310, loss = 1.01 (47.8 examples/sec; 0.669 sec/batch)
2018-10-17 08:36:58.865644: step 105320, loss = 1.05 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 08:37:05.965009: step 105330, loss = 1.15 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 08:37:13.061860: step 105340, loss = 1.05 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 08:37:20.174684: step 105350, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 08:37:27.201334: step 105360, loss = 1.01 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 08:37:34.347771: step 105370, loss = 1.12 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 08:37:41.426122: step 105380, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 08:37:48.520653: step 105390, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:37:55.601028: step 105400, loss = 1.00 (45.7 examples/sec; 0.699 sec/batch)
2018-10-17 08:38:05.282480: step 105410, loss = 1.28 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 08:38:12.320906: step 105420, loss = 1.13 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 08:38:19.273527: step 105430, loss = 1.08 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 08:38:26.347591: step 105440, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 08:38:33.445097: step 105450, loss = 1.03 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 08:38:40.466854: step 105460, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 08:38:47.609439: step 105470, loss = 1.15 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:38:54.659192: step 105480, loss = 1.04 (46.5 examples/sec; 0.687 sec/batch)
2018-10-17 08:39:01.785459: step 105490, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:39:08.791179: step 105500, loss = 1.18 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 08:39:18.354073: step 105510, loss = 1.05 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 08:39:25.368237: step 105520, loss = 1.06 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:39:32.474883: step 105530, loss = 1.03 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 08:39:39.637953: step 105540, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 08:39:46.725241: step 105550, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 08:39:53.778109: step 105560, loss = 1.03 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 08:40:00.777128: step 105570, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:40:07.904529: step 105580, loss = 1.03 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 08:40:14.976745: step 105590, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:40:22.078557: step 105600, loss = 1.11 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 08:40:31.689400: step 105610, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 08:40:38.692321: step 105620, loss = 1.08 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 08:40:45.768228: step 105630, loss = 1.15 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 08:40:52.799098: step 105640, loss = 1.13 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 08:40:59.822125: step 105650, loss = 1.05 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 08:41:06.864669: step 105660, loss = 1.08 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:41:13.989319: step 105670, loss = 1.00 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 08:41:21.043342: step 105680, loss = 1.16 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 08:41:28.209850: step 105690, loss = 1.05 (43.3 examples/sec; 0.738 sec/batch)
2018-10-17 08:41:35.258338: step 105700, loss = 0.99 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 08:41:44.870464: step 105710, loss = 1.09 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 08:41:51.901310: step 105720, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:41:58.944227: step 105730, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:42:05.982949: step 105740, loss = 1.17 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 08:42:13.063210: step 105750, loss = 1.04 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 08:42:20.184143: step 105760, loss = 1.23 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 08:42:27.317197: step 105770, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 08:42:34.341086: step 105780, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 08:42:41.355446: step 105790, loss = 1.18 (44.4 examples/sec; 0.722 sec/batch)
2018-10-17 08:42:48.491988: step 105800, loss = 1.09 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 08:42:58.530761: step 105810, loss = 1.07 (46.8 examples/sec; 0.683 sec/batch)
2018-10-17 08:43:05.560091: step 105820, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 08:43:12.579691: step 105830, loss = 1.04 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 08:43:19.670083: step 105840, loss = 1.14 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 08:43:26.834885: step 105850, loss = 1.12 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:43:33.870708: step 105860, loss = 1.05 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:43:40.977677: step 105870, loss = 1.12 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 08:43:47.982686: step 105880, loss = 0.99 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 08:43:55.097873: step 105890, loss = 1.00 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 08:44:02.138515: step 105900, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 08:44:11.797167: step 105910, loss = 1.14 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 08:44:18.890368: step 105920, loss = 1.03 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 08:44:25.975338: step 105930, loss = 1.07 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 08:44:33.033378: step 105940, loss = 1.06 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 08:44:40.108580: step 105950, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 08:44:47.296984: step 105960, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 08:44:54.414371: step 105970, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 08:45:01.472494: step 105980, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 08:45:08.589345: step 105990, loss = 1.04 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 08:45:15.670990: step 106000, loss = 1.17 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 08:45:25.358868: step 106010, loss = 1.18 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 08:45:32.473116: step 106020, loss = 1.07 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 08:45:39.626960: step 106030, loss = 1.15 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 08:45:46.772519: step 106040, loss = 1.13 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 08:45:53.832551: step 106050, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:46:00.898043: step 106060, loss = 1.00 (45.7 examples/sec; 0.699 sec/batch)
2018-10-17 08:46:08.057579: step 106070, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 08:46:15.134555: step 106080, loss = 1.10 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 08:46:22.163598: step 106090, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 08:46:29.224843: step 106100, loss = 1.22 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 08:46:39.043159: step 106110, loss = 1.20 (47.8 examples/sec; 0.669 sec/batch)
2018-10-17 08:46:46.089959: step 106120, loss = 1.11 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 08:46:53.172852: step 106130, loss = 0.99 (47.6 examples/sec; 0.673 sec/batch)
2018-10-17 08:47:00.366068: step 106140, loss = 1.04 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 08:47:07.386787: step 106150, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 08:47:14.454869: step 106160, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 08:47:21.517836: step 106170, loss = 1.05 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:47:28.556104: step 106180, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 08:47:35.673732: step 106190, loss = 1.09 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 08:47:42.686179: step 106200, loss = 1.03 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 08:47:53.042193: step 106210, loss = 1.14 (47.0 examples/sec; 0.680 sec/batch)
2018-10-17 08:48:00.139535: step 106220, loss = 1.03 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 08:48:07.274251: step 106230, loss = 0.99 (43.3 examples/sec; 0.738 sec/batch)
2018-10-17 08:48:14.425968: step 106240, loss = 1.01 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 08:48:21.505852: step 106250, loss = 1.16 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 08:48:28.602472: step 106260, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:48:35.663703: step 106270, loss = 1.08 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 08:48:42.735630: step 106280, loss = 1.10 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 08:48:49.803200: step 106290, loss = 0.99 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 08:48:56.923201: step 106300, loss = 1.08 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 08:49:07.265515: step 106310, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 08:49:14.228360: step 106320, loss = 1.11 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 08:49:21.391219: step 106330, loss = 1.09 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 08:49:28.465003: step 106340, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 08:49:35.588576: step 106350, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:49:42.681968: step 106360, loss = 1.07 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 08:49:49.768440: step 106370, loss = 1.03 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 08:49:56.819485: step 106380, loss = 1.11 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 08:50:03.892271: step 106390, loss = 0.99 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 08:50:11.007325: step 106400, loss = 1.12 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 08:50:20.698063: step 106410, loss = 0.99 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 08:50:27.664039: step 106420, loss = 1.06 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 08:50:34.818023: step 106430, loss = 1.06 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 08:50:41.876339: step 106440, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 08:50:48.923947: step 106450, loss = 1.01 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 08:50:55.974299: step 106460, loss = 1.10 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 08:51:03.009062: step 106470, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 08:51:10.083406: step 106480, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:51:17.134393: step 106490, loss = 1.10 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 08:51:24.178286: step 106500, loss = 1.03 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 08:51:34.170363: step 106510, loss = 1.03 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 08:51:41.163430: step 106520, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 08:51:48.261007: step 106530, loss = 1.10 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 08:51:55.336272: step 106540, loss = 1.15 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:52:02.436452: step 106550, loss = 1.03 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 08:52:09.469036: step 106560, loss = 1.06 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 08:52:16.459521: step 106570, loss = 1.07 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 08:52:23.545910: step 106580, loss = 1.11 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:52:30.596067: step 106590, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 08:52:37.665686: step 106600, loss = 1.01 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 08:52:47.482567: step 106610, loss = 1.18 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 08:52:54.552028: step 106620, loss = 1.20 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 08:53:01.655835: step 106630, loss = 1.01 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 08:53:08.831002: step 106640, loss = 1.09 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 08:53:15.875389: step 106650, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:53:22.998623: step 106660, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 08:53:30.096280: step 106670, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 08:53:37.123547: step 106680, loss = 1.03 (47.1 examples/sec; 0.680 sec/batch)
2018-10-17 08:53:44.226198: step 106690, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 08:53:51.284830: step 106700, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 08:54:01.976289: step 106710, loss = 1.01 (47.9 examples/sec; 0.669 sec/batch)
2018-10-17 08:54:08.922786: step 106720, loss = 0.99 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 08:54:15.959566: step 106730, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:54:23.074122: step 106740, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:54:30.111273: step 106750, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 08:54:37.217790: step 106760, loss = 1.12 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 08:54:44.254034: step 106770, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 08:54:51.343360: step 106780, loss = 1.00 (44.4 examples/sec; 0.722 sec/batch)
2018-10-17 08:54:58.419217: step 106790, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 08:55:05.441488: step 106800, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 08:55:15.036718: step 106810, loss = 1.13 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 08:55:22.125068: step 106820, loss = 0.99 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 08:55:29.214027: step 106830, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 08:55:36.258409: step 106840, loss = 1.15 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:55:43.405924: step 106850, loss = 1.13 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 08:55:50.529727: step 106860, loss = 1.18 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 08:55:57.681091: step 106870, loss = 1.00 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 08:56:04.716556: step 106880, loss = 1.23 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:56:11.856988: step 106890, loss = 1.02 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 08:56:18.867171: step 106900, loss = 1.08 (48.3 examples/sec; 0.663 sec/batch)
2018-10-17 08:56:28.542861: step 106910, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 08:56:35.572572: step 106920, loss = 1.05 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 08:56:42.600046: step 106930, loss = 1.14 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 08:56:49.681570: step 106940, loss = 1.09 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 08:56:56.763602: step 106950, loss = 1.02 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 08:57:03.839812: step 106960, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:57:10.893935: step 106970, loss = 0.99 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 08:57:17.941245: step 106980, loss = 1.09 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 08:57:25.036767: step 106990, loss = 1.04 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 08:57:32.098549: step 107000, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 08:57:41.741725: step 107010, loss = 1.12 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 08:57:48.784467: step 107020, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 08:57:55.856842: step 107030, loss = 1.08 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 08:58:02.973204: step 107040, loss = 1.05 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 08:58:09.975175: step 107050, loss = 1.06 (45.7 examples/sec; 0.699 sec/batch)
2018-10-17 08:58:17.055647: step 107060, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 08:58:24.118958: step 107070, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 08:58:31.172733: step 107080, loss = 1.07 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:58:38.280955: step 107090, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 08:58:45.366208: step 107100, loss = 1.08 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 08:58:55.605924: step 107110, loss = 1.15 (47.8 examples/sec; 0.670 sec/batch)
2018-10-17 08:59:02.616605: step 107120, loss = 1.04 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 08:59:09.770058: step 107130, loss = 1.02 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 08:59:16.885528: step 107140, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 08:59:23.973649: step 107150, loss = 1.05 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 08:59:31.087035: step 107160, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 08:59:38.156601: step 107170, loss = 1.02 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 08:59:45.204981: step 107180, loss = 1.04 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 08:59:52.285851: step 107190, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 08:59:59.298398: step 107200, loss = 1.10 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 09:00:09.110076: step 107210, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 09:00:16.163777: step 107220, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 09:00:23.234110: step 107230, loss = 1.07 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 09:00:30.334812: step 107240, loss = 1.15 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 09:00:37.457030: step 107250, loss = 1.06 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 09:00:44.512233: step 107260, loss = 1.19 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:00:51.658653: step 107270, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 09:00:58.711804: step 107280, loss = 1.13 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 09:01:05.827368: step 107290, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 09:01:12.878197: step 107300, loss = 1.14 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 09:01:22.480471: step 107310, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 09:01:29.568276: step 107320, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 09:01:36.646300: step 107330, loss = 1.08 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 09:01:43.716834: step 107340, loss = 1.13 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 09:01:50.771976: step 107350, loss = 1.01 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 09:01:57.879401: step 107360, loss = 1.08 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:02:04.951289: step 107370, loss = 1.07 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 09:02:12.033332: step 107380, loss = 1.05 (47.3 examples/sec; 0.676 sec/batch)
2018-10-17 09:02:19.068190: step 107390, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 09:02:26.138841: step 107400, loss = 1.07 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 09:02:36.131944: step 107410, loss = 1.15 (47.6 examples/sec; 0.673 sec/batch)
2018-10-17 09:02:43.202608: step 107420, loss = 1.00 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 09:02:50.264568: step 107430, loss = 1.27 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 09:02:57.346095: step 107440, loss = 1.14 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:03:04.442062: step 107450, loss = 0.99 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 09:03:11.519772: step 107460, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 09:03:18.611701: step 107470, loss = 1.06 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 09:03:25.724496: step 107480, loss = 0.99 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 09:03:32.797376: step 107490, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 09:03:39.799305: step 107500, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 09:03:49.400195: step 107510, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 09:03:56.436094: step 107520, loss = 1.01 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 09:04:03.515360: step 107530, loss = 1.06 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 09:04:10.563015: step 107540, loss = 1.03 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 09:04:17.629990: step 107550, loss = 1.18 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 09:04:24.660671: step 107560, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:04:31.708298: step 107570, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 09:04:38.777984: step 107580, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:04:45.906368: step 107590, loss = 1.15 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 09:04:53.002732: step 107600, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 09:05:03.113663: step 107610, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 09:05:10.142109: step 107620, loss = 1.04 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:05:17.226463: step 107630, loss = 1.14 (43.0 examples/sec; 0.743 sec/batch)
2018-10-17 09:05:24.266769: step 107640, loss = 1.11 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 09:05:31.397545: step 107650, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 09:05:38.453325: step 107660, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 09:05:45.617377: step 107670, loss = 1.12 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 09:05:52.651384: step 107680, loss = 1.09 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 09:05:59.750206: step 107690, loss = 1.05 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 09:06:06.785352: step 107700, loss = 0.99 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 09:06:16.481905: step 107710, loss = 1.03 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 09:06:23.582188: step 107720, loss = 1.03 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 09:06:30.647617: step 107730, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:06:37.706298: step 107740, loss = 1.00 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 09:06:44.722560: step 107750, loss = 1.00 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 09:06:51.912191: step 107760, loss = 0.99 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 09:06:58.961309: step 107770, loss = 1.02 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 09:07:06.039400: step 107780, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 09:07:13.098431: step 107790, loss = 1.01 (46.8 examples/sec; 0.683 sec/batch)
2018-10-17 09:07:20.250769: step 107800, loss = 0.99 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 09:07:29.918085: step 107810, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 09:07:37.007087: step 107820, loss = 1.12 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 09:07:44.046072: step 107830, loss = 1.32 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 09:07:51.112350: step 107840, loss = 1.00 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 09:07:58.198035: step 107850, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 09:08:05.341088: step 107860, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 09:08:12.429395: step 107870, loss = 0.99 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 09:08:19.552186: step 107880, loss = 1.16 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 09:08:26.683165: step 107890, loss = 1.04 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 09:08:33.801035: step 107900, loss = 1.04 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 09:08:43.405421: step 107910, loss = 0.99 (46.8 examples/sec; 0.684 sec/batch)
2018-10-17 09:08:50.427250: step 107920, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 09:08:57.528647: step 107930, loss = 1.06 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 09:09:04.601561: step 107940, loss = 1.01 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 09:09:11.587495: step 107950, loss = 1.17 (47.0 examples/sec; 0.680 sec/batch)
2018-10-17 09:09:18.703973: step 107960, loss = 1.11 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 09:09:25.786973: step 107970, loss = 1.09 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 09:09:32.915541: step 107980, loss = 1.08 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 09:09:40.012384: step 107990, loss = 1.00 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 09:09:47.080135: step 108000, loss = 1.00 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 09:09:56.816657: step 108010, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 09:10:03.868261: step 108020, loss = 1.02 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 09:10:10.967165: step 108030, loss = 1.13 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 09:10:18.104245: step 108040, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 09:10:25.176667: step 108050, loss = 1.05 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:10:32.217791: step 108060, loss = 1.13 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 09:10:39.391572: step 108070, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 09:10:46.510762: step 108080, loss = 1.01 (47.3 examples/sec; 0.676 sec/batch)
2018-10-17 09:10:53.603976: step 108090, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 09:11:00.697630: step 108100, loss = 1.06 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:11:10.294553: step 108110, loss = 1.05 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 09:11:17.297488: step 108120, loss = 1.30 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 09:11:24.408392: step 108130, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 09:11:31.507307: step 108140, loss = 1.06 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 09:11:38.580261: step 108150, loss = 1.03 (47.6 examples/sec; 0.672 sec/batch)
2018-10-17 09:11:45.776715: step 108160, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 09:11:52.920513: step 108170, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 09:12:00.023369: step 108180, loss = 1.00 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 09:12:07.027001: step 108190, loss = 1.06 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 09:12:14.143554: step 108200, loss = 1.14 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 09:12:23.845036: step 108210, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 09:12:30.915896: step 108220, loss = 1.05 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 09:12:38.017980: step 108230, loss = 1.06 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 09:12:45.047102: step 108240, loss = 1.18 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 09:12:52.067168: step 108250, loss = 1.05 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 09:12:59.117512: step 108260, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:13:06.235738: step 108270, loss = 1.06 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 09:13:13.376122: step 108280, loss = 1.02 (46.3 examples/sec; 0.690 sec/batch)
2018-10-17 09:13:20.356729: step 108290, loss = 1.01 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 09:13:27.377726: step 108300, loss = 1.13 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 09:13:37.198400: step 108310, loss = 1.11 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 09:13:44.184633: step 108320, loss = 1.02 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 09:13:51.331558: step 108330, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:13:58.405059: step 108340, loss = 1.03 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 09:14:05.531935: step 108350, loss = 1.06 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 09:14:12.532207: step 108360, loss = 1.20 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 09:14:19.612748: step 108370, loss = 1.00 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 09:14:26.641441: step 108380, loss = 1.07 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:14:33.748387: step 108390, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 09:14:40.837606: step 108400, loss = 1.02 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 09:14:50.447045: step 108410, loss = 1.02 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 09:14:57.504998: step 108420, loss = 1.00 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 09:15:04.607324: step 108430, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 09:15:11.622633: step 108440, loss = 1.03 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 09:15:18.747316: step 108450, loss = 0.99 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 09:15:25.839254: step 108460, loss = 1.17 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 09:15:32.928323: step 108470, loss = 1.02 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 09:15:40.031440: step 108480, loss = 1.01 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 09:15:47.100379: step 108490, loss = 1.03 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 09:15:54.203090: step 108500, loss = 1.02 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 09:16:03.878979: step 108510, loss = 1.21 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 09:16:10.878916: step 108520, loss = 0.99 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 09:16:18.032085: step 108530, loss = 1.00 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 09:16:25.043389: step 108540, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:16:32.197288: step 108550, loss = 1.13 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 09:16:39.347570: step 108560, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 09:16:46.425456: step 108570, loss = 1.06 (44.5 examples/sec; 0.718 sec/batch)
2018-10-17 09:16:53.519526: step 108580, loss = 1.04 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 09:17:00.527756: step 108590, loss = 1.02 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 09:17:07.576994: step 108600, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 09:17:17.482144: step 108610, loss = 1.00 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 09:17:24.551700: step 108620, loss = 0.99 (43.3 examples/sec; 0.740 sec/batch)
2018-10-17 09:17:31.688764: step 108630, loss = 1.11 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 09:17:38.767218: step 108640, loss = 1.08 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 09:17:45.929362: step 108650, loss = 1.01 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 09:17:52.906608: step 108660, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 09:18:00.049362: step 108670, loss = 1.05 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 09:18:07.110000: step 108680, loss = 1.13 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 09:18:14.190030: step 108690, loss = 1.12 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 09:18:21.341876: step 108700, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 09:18:30.925173: step 108710, loss = 0.99 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 09:18:38.043474: step 108720, loss = 1.03 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:18:45.171401: step 108730, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 09:18:52.290520: step 108740, loss = 0.99 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 09:18:59.364360: step 108750, loss = 1.11 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:19:06.470058: step 108760, loss = 1.27 (47.8 examples/sec; 0.670 sec/batch)
2018-10-17 09:19:13.540999: step 108770, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 09:19:20.648617: step 108780, loss = 1.04 (47.4 examples/sec; 0.674 sec/batch)
2018-10-17 09:19:27.758604: step 108790, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 09:19:34.905056: step 108800, loss = 1.08 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 09:19:44.636303: step 108810, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 09:19:51.694361: step 108820, loss = 1.01 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 09:19:58.754508: step 108830, loss = 1.12 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 09:20:05.933928: step 108840, loss = 1.01 (44.5 examples/sec; 0.718 sec/batch)
2018-10-17 09:20:12.976445: step 108850, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 09:20:20.023033: step 108860, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 09:20:27.094273: step 108870, loss = 1.02 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 09:20:34.251489: step 108880, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 09:20:41.322912: step 108890, loss = 1.14 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 09:20:48.319129: step 108900, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:20:57.948396: step 108910, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 09:21:04.979744: step 108920, loss = 1.04 (46.1 examples/sec; 0.693 sec/batch)
2018-10-17 09:21:12.005196: step 108930, loss = 1.09 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 09:21:19.092023: step 108940, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 09:21:26.206149: step 108950, loss = 1.10 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 09:21:33.305368: step 108960, loss = 0.99 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 09:21:40.419891: step 108970, loss = 1.04 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 09:21:47.479443: step 108980, loss = 1.09 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 09:21:54.615586: step 108990, loss = 1.04 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 09:22:01.737470: step 109000, loss = 1.13 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 09:22:11.717992: step 109010, loss = 1.06 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 09:22:18.776211: step 109020, loss = 1.20 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 09:22:25.793248: step 109030, loss = 1.08 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 09:22:32.955563: step 109040, loss = 1.10 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 09:22:39.965993: step 109050, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 09:22:47.110016: step 109060, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 09:22:54.181062: step 109070, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 09:23:01.287771: step 109080, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 09:23:08.291655: step 109090, loss = 1.01 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 09:23:15.341049: step 109100, loss = 1.05 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 09:23:25.096680: step 109110, loss = 1.04 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 09:23:32.154755: step 109120, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 09:23:39.310674: step 109130, loss = 1.26 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 09:23:46.416454: step 109140, loss = 1.04 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 09:23:53.567261: step 109150, loss = 1.01 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 09:24:00.628886: step 109160, loss = 1.19 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 09:24:07.718285: step 109170, loss = 1.08 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 09:24:14.873397: step 109180, loss = 1.00 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 09:24:21.903459: step 109190, loss = 1.02 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 09:24:28.914229: step 109200, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 09:24:38.843019: step 109210, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:24:45.969659: step 109220, loss = 1.16 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 09:24:52.993560: step 109230, loss = 1.08 (46.9 examples/sec; 0.682 sec/batch)
2018-10-17 09:25:00.048092: step 109240, loss = 1.23 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 09:25:07.217824: step 109250, loss = 1.23 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 09:25:14.325283: step 109260, loss = 1.00 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 09:25:21.464471: step 109270, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 09:25:28.584072: step 109280, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 09:25:35.696619: step 109290, loss = 1.06 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 09:25:42.784899: step 109300, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 09:25:52.459657: step 109310, loss = 1.00 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 09:25:59.484326: step 109320, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 09:26:06.554528: step 109330, loss = 1.07 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 09:26:13.680281: step 109340, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 09:26:20.755143: step 109350, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 09:26:27.798696: step 109360, loss = 1.02 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 09:26:34.926792: step 109370, loss = 0.99 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 09:26:42.025100: step 109380, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 09:26:49.111248: step 109390, loss = 1.21 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:26:56.154296: step 109400, loss = 1.05 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 09:27:05.950074: step 109410, loss = 1.03 (46.5 examples/sec; 0.689 sec/batch)
2018-10-17 09:27:12.983823: step 109420, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:27:20.055013: step 109430, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 09:27:27.190978: step 109440, loss = 1.30 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 09:27:34.317178: step 109450, loss = 1.08 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 09:27:41.375958: step 109460, loss = 1.16 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 09:27:48.519886: step 109470, loss = 1.09 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 09:27:55.636081: step 109480, loss = 1.12 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 09:28:02.782131: step 109490, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 09:28:09.873339: step 109500, loss = 1.18 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 09:28:19.511066: step 109510, loss = 1.07 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 09:28:26.588752: step 109520, loss = 1.17 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 09:28:33.721584: step 109530, loss = 1.01 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 09:28:40.879953: step 109540, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:28:47.973909: step 109550, loss = 1.09 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 09:28:55.088382: step 109560, loss = 1.01 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 09:29:02.156895: step 109570, loss = 1.15 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 09:29:09.254031: step 109580, loss = 1.01 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 09:29:16.444055: step 109590, loss = 1.05 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:29:23.542202: step 109600, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 09:29:33.233675: step 109610, loss = 1.02 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 09:29:40.306975: step 109620, loss = 1.15 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 09:29:47.471956: step 109630, loss = 1.03 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 09:29:54.597461: step 109640, loss = 1.19 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 09:30:01.730367: step 109650, loss = 1.00 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 09:30:08.790786: step 109660, loss = 0.99 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 09:30:15.898478: step 109670, loss = 0.99 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 09:30:22.961009: step 109680, loss = 1.07 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 09:30:30.076986: step 109690, loss = 1.02 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 09:30:37.247615: step 109700, loss = 1.26 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 09:30:47.458176: step 109710, loss = 1.07 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 09:30:54.586764: step 109720, loss = 1.17 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 09:31:01.734483: step 109730, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 09:31:08.820876: step 109740, loss = 1.07 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 09:31:15.971625: step 109750, loss = 1.02 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 09:31:23.083221: step 109760, loss = 1.00 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 09:31:30.253286: step 109770, loss = 1.04 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 09:31:37.340042: step 109780, loss = 1.05 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 09:31:44.458190: step 109790, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 09:31:51.514543: step 109800, loss = 1.06 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 09:32:01.230271: step 109810, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 09:32:08.313986: step 109820, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 09:32:15.451374: step 109830, loss = 1.11 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 09:32:22.539918: step 109840, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 09:32:29.680467: step 109850, loss = 1.14 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 09:32:36.865135: step 109860, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 09:32:44.048807: step 109870, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 09:32:51.092735: step 109880, loss = 1.09 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 09:32:58.187485: step 109890, loss = 1.17 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 09:33:05.375152: step 109900, loss = 1.03 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 09:33:15.057603: step 109910, loss = 1.06 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 09:33:22.112676: step 109920, loss = 1.08 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 09:33:29.270210: step 109930, loss = 0.99 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 09:33:36.344081: step 109940, loss = 0.99 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 09:33:43.466260: step 109950, loss = 1.05 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 09:33:50.630659: step 109960, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 09:33:57.807589: step 109970, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 09:34:04.908215: step 109980, loss = 1.02 (43.0 examples/sec; 0.744 sec/batch)
2018-10-17 09:34:12.036481: step 109990, loss = 1.19 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 09:34:19.144923: step 110000, loss = 1.15 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 09:34:32.302324: step 110010, loss = 0.99 (47.2 examples/sec; 0.678 sec/batch)
2018-10-17 09:34:39.131784: step 110020, loss = 1.03 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 09:34:46.284915: step 110030, loss = 1.01 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 09:34:53.397175: step 110040, loss = 1.11 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:35:00.544237: step 110050, loss = 1.02 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 09:35:07.673537: step 110060, loss = 1.18 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 09:35:14.818294: step 110070, loss = 1.00 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 09:35:22.007731: step 110080, loss = 1.01 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 09:35:29.157942: step 110090, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 09:35:36.296315: step 110100, loss = 1.03 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 09:35:46.525392: step 110110, loss = 0.99 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 09:35:53.655596: step 110120, loss = 1.11 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 09:36:00.768810: step 110130, loss = 1.11 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 09:36:07.850455: step 110140, loss = 1.15 (46.7 examples/sec; 0.685 sec/batch)
2018-10-17 09:36:15.054711: step 110150, loss = 1.01 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 09:36:22.214099: step 110160, loss = 1.06 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 09:36:29.333102: step 110170, loss = 1.03 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 09:36:36.488970: step 110180, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:36:43.612952: step 110190, loss = 1.00 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 09:36:50.688557: step 110200, loss = 1.10 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 09:37:00.345474: step 110210, loss = 1.06 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 09:37:07.591234: step 110220, loss = 1.08 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 09:37:14.673577: step 110230, loss = 1.10 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 09:37:21.855276: step 110240, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:37:28.942703: step 110250, loss = 1.03 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 09:37:36.071940: step 110260, loss = 1.00 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 09:37:43.175203: step 110270, loss = 1.03 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 09:37:50.305318: step 110280, loss = 1.02 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 09:37:57.384558: step 110290, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 09:38:04.552906: step 110300, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 09:38:14.255880: step 110310, loss = 1.28 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:38:21.320504: step 110320, loss = 1.20 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 09:38:28.538237: step 110330, loss = 1.22 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 09:38:35.640190: step 110340, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 09:38:42.775472: step 110350, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 09:38:49.935693: step 110360, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 09:38:57.120503: step 110370, loss = 1.24 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 09:39:04.260770: step 110380, loss = 1.02 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 09:39:11.322527: step 110390, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:39:18.438272: step 110400, loss = 1.12 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 09:39:28.555633: step 110410, loss = 0.99 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 09:39:35.685481: step 110420, loss = 1.11 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 09:39:42.775681: step 110430, loss = 1.03 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 09:39:49.887378: step 110440, loss = 1.00 (43.3 examples/sec; 0.740 sec/batch)
2018-10-17 09:39:56.899352: step 110450, loss = 1.12 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 09:40:04.094604: step 110460, loss = 1.02 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 09:40:11.159597: step 110470, loss = 1.00 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 09:40:18.320582: step 110480, loss = 1.14 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 09:40:25.462546: step 110490, loss = 1.10 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 09:40:32.590299: step 110500, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 09:40:42.142096: step 110510, loss = 1.01 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 09:40:49.151633: step 110520, loss = 1.13 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 09:40:56.216622: step 110530, loss = 1.08 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 09:41:03.386679: step 110540, loss = 1.16 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 09:41:10.518260: step 110550, loss = 1.09 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:41:17.645345: step 110560, loss = 1.03 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 09:41:24.771299: step 110570, loss = 1.00 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 09:41:31.910653: step 110580, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 09:41:38.980041: step 110590, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:41:46.084348: step 110600, loss = 1.00 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 09:41:55.793410: step 110610, loss = 0.99 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 09:42:02.871908: step 110620, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 09:42:09.977498: step 110630, loss = 1.09 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 09:42:17.126409: step 110640, loss = 1.10 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 09:42:24.264806: step 110650, loss = 1.07 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 09:42:31.370179: step 110660, loss = 1.18 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 09:42:38.479002: step 110670, loss = 1.15 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 09:42:45.559947: step 110680, loss = 1.02 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 09:42:52.648058: step 110690, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 09:42:59.792480: step 110700, loss = 1.00 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 09:43:09.329221: step 110710, loss = 1.03 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 09:43:16.490412: step 110720, loss = 1.11 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 09:43:23.548120: step 110730, loss = 1.08 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 09:43:30.648560: step 110740, loss = 1.04 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 09:43:37.746513: step 110750, loss = 1.07 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 09:43:44.888667: step 110760, loss = 1.01 (47.1 examples/sec; 0.679 sec/batch)
2018-10-17 09:43:52.016510: step 110770, loss = 1.03 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 09:43:59.160167: step 110780, loss = 1.15 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 09:44:06.196112: step 110790, loss = 1.05 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:44:13.296916: step 110800, loss = 1.01 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 09:44:23.016782: step 110810, loss = 1.08 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 09:44:30.102186: step 110820, loss = 1.02 (42.7 examples/sec; 0.750 sec/batch)
2018-10-17 09:44:37.238051: step 110830, loss = 1.03 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 09:44:44.302225: step 110840, loss = 1.01 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:44:51.437819: step 110850, loss = 0.99 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 09:44:58.498257: step 110860, loss = 1.00 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 09:45:05.629108: step 110870, loss = 1.00 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:45:12.774146: step 110880, loss = 1.00 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 09:45:19.879866: step 110890, loss = 1.04 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 09:45:27.044468: step 110900, loss = 1.04 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:45:36.643148: step 110910, loss = 1.06 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 09:45:43.657538: step 110920, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 09:45:50.747098: step 110930, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 09:45:57.870757: step 110940, loss = 1.11 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 09:46:04.909585: step 110950, loss = 1.03 (47.0 examples/sec; 0.681 sec/batch)
2018-10-17 09:46:11.976141: step 110960, loss = 1.07 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 09:46:19.169325: step 110970, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 09:46:26.212816: step 110980, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:46:33.329431: step 110990, loss = 1.10 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 09:46:40.425642: step 111000, loss = 1.02 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 09:46:50.771069: step 111010, loss = 1.12 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 09:46:57.887489: step 111020, loss = 1.06 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 09:47:04.977211: step 111030, loss = 0.99 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 09:47:12.102873: step 111040, loss = 1.09 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 09:47:19.227389: step 111050, loss = 1.08 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:47:26.359693: step 111060, loss = 1.01 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 09:47:33.484751: step 111070, loss = 1.01 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 09:47:40.607986: step 111080, loss = 1.22 (44.5 examples/sec; 0.718 sec/batch)
2018-10-17 09:47:47.727592: step 111090, loss = 1.04 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 09:47:54.899055: step 111100, loss = 1.01 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 09:48:04.935305: step 111110, loss = 1.01 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 09:48:12.152115: step 111120, loss = 1.00 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 09:48:19.245986: step 111130, loss = 1.08 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 09:48:26.336030: step 111140, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:48:33.515649: step 111150, loss = 1.10 (43.3 examples/sec; 0.738 sec/batch)
2018-10-17 09:48:40.595471: step 111160, loss = 1.29 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 09:48:47.731478: step 111170, loss = 1.04 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:48:54.837448: step 111180, loss = 1.08 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 09:49:01.973094: step 111190, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 09:49:09.073687: step 111200, loss = 1.05 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 09:49:18.696970: step 111210, loss = 1.05 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 09:49:25.806195: step 111220, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:49:32.928686: step 111230, loss = 1.01 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 09:49:40.053034: step 111240, loss = 1.16 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 09:49:47.143105: step 111250, loss = 0.99 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 09:49:54.183276: step 111260, loss = 0.99 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 09:50:01.278285: step 111270, loss = 1.03 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 09:50:08.423168: step 111280, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 09:50:15.579578: step 111290, loss = 1.05 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 09:50:22.755674: step 111300, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 09:50:32.484972: step 111310, loss = 1.04 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 09:50:39.589144: step 111320, loss = 1.01 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 09:50:46.716030: step 111330, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 09:50:53.823506: step 111340, loss = 1.11 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 09:51:01.004847: step 111350, loss = 1.20 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 09:51:08.141425: step 111360, loss = 0.99 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 09:51:15.281611: step 111370, loss = 1.11 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 09:51:22.377087: step 111380, loss = 1.03 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 09:51:29.427776: step 111390, loss = 1.07 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 09:51:36.652057: step 111400, loss = 1.04 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 09:51:46.452990: step 111410, loss = 1.01 (47.5 examples/sec; 0.673 sec/batch)
2018-10-17 09:51:53.661951: step 111420, loss = 1.07 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:52:00.752998: step 111430, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 09:52:07.902419: step 111440, loss = 1.11 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 09:52:14.985834: step 111450, loss = 1.00 (43.3 examples/sec; 0.740 sec/batch)
2018-10-17 09:52:22.096412: step 111460, loss = 1.11 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 09:52:29.239048: step 111470, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 09:52:36.359982: step 111480, loss = 1.00 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 09:52:43.487906: step 111490, loss = 1.12 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 09:52:50.587951: step 111500, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 09:53:00.283836: step 111510, loss = 1.09 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 09:53:07.363862: step 111520, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:53:14.481370: step 111530, loss = 1.31 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 09:53:21.533510: step 111540, loss = 1.02 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 09:53:28.653353: step 111550, loss = 1.07 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 09:53:35.747277: step 111560, loss = 1.00 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 09:53:42.892821: step 111570, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 09:53:50.019477: step 111580, loss = 1.05 (44.7 examples/sec; 0.717 sec/batch)
2018-10-17 09:53:57.241836: step 111590, loss = 1.01 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 09:54:04.326015: step 111600, loss = 1.18 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 09:54:14.055937: step 111610, loss = 1.06 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 09:54:21.175110: step 111620, loss = 1.09 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 09:54:28.278369: step 111630, loss = 1.01 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 09:54:35.354083: step 111640, loss = 1.11 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 09:54:42.414070: step 111650, loss = 1.05 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:54:49.582535: step 111660, loss = 1.17 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:54:56.708099: step 111670, loss = 0.99 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 09:55:03.816415: step 111680, loss = 0.99 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 09:55:10.919453: step 111690, loss = 1.04 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 09:55:17.949487: step 111700, loss = 1.07 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 09:55:27.623058: step 111710, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:55:34.736898: step 111720, loss = 0.99 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 09:55:41.840527: step 111730, loss = 1.00 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 09:55:48.949831: step 111740, loss = 1.00 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 09:55:56.061419: step 111750, loss = 1.05 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 09:56:03.262334: step 111760, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 09:56:10.379445: step 111770, loss = 1.08 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 09:56:17.526984: step 111780, loss = 1.01 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 09:56:24.613703: step 111790, loss = 1.13 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 09:56:31.701123: step 111800, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 09:56:41.436876: step 111810, loss = 1.07 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 09:56:48.503274: step 111820, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 09:56:55.629407: step 111830, loss = 1.06 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 09:57:02.797996: step 111840, loss = 1.07 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 09:57:09.901440: step 111850, loss = 1.03 (46.1 examples/sec; 0.693 sec/batch)
2018-10-17 09:57:17.039563: step 111860, loss = 1.04 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 09:57:24.261183: step 111870, loss = 1.06 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 09:57:31.396260: step 111880, loss = 1.14 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 09:57:38.541765: step 111890, loss = 1.09 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 09:57:45.676443: step 111900, loss = 1.11 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 09:57:55.294761: step 111910, loss = 1.11 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 09:58:02.445580: step 111920, loss = 1.08 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:58:09.472931: step 111930, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 09:58:16.563491: step 111940, loss = 1.01 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 09:58:23.766131: step 111950, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 09:58:30.876001: step 111960, loss = 1.00 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 09:58:38.054705: step 111970, loss = 1.06 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 09:58:45.182071: step 111980, loss = 1.09 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 09:58:52.343579: step 111990, loss = 1.20 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 09:58:59.423977: step 112000, loss = 1.05 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 09:59:09.124393: step 112010, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 09:59:16.182298: step 112020, loss = 1.12 (42.7 examples/sec; 0.749 sec/batch)
2018-10-17 09:59:23.241766: step 112030, loss = 1.02 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 09:59:30.360869: step 112040, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:59:37.442968: step 112050, loss = 1.06 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 09:59:44.572185: step 112060, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 09:59:51.706174: step 112070, loss = 1.14 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 09:59:58.858593: step 112080, loss = 1.06 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 10:00:06.030104: step 112090, loss = 1.02 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 10:00:13.189010: step 112100, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 10:00:22.849492: step 112110, loss = 1.01 (43.9 examples/sec; 0.730 sec/batch)
2018-10-17 10:00:29.980417: step 112120, loss = 1.02 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 10:00:37.118463: step 112130, loss = 1.00 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 10:00:44.223923: step 112140, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 10:00:51.259971: step 112150, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 10:00:58.386738: step 112160, loss = 1.01 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 10:01:05.526743: step 112170, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 10:01:12.665781: step 112180, loss = 1.02 (46.1 examples/sec; 0.693 sec/batch)
2018-10-17 10:01:19.792076: step 112190, loss = 1.04 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 10:01:26.906571: step 112200, loss = 1.03 (46.7 examples/sec; 0.686 sec/batch)
2018-10-17 10:01:36.669005: step 112210, loss = 1.07 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 10:01:43.690778: step 112220, loss = 1.15 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 10:01:50.729133: step 112230, loss = 1.01 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 10:01:57.850737: step 112240, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 10:02:04.911756: step 112250, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 10:02:12.026266: step 112260, loss = 1.07 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 10:02:19.135088: step 112270, loss = 1.15 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 10:02:26.284558: step 112280, loss = 1.10 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 10:02:33.372875: step 112290, loss = 1.15 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 10:02:40.560378: step 112300, loss = 1.05 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 10:02:50.341076: step 112310, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 10:02:57.392586: step 112320, loss = 1.01 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 10:03:04.480050: step 112330, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 10:03:11.676761: step 112340, loss = 1.15 (45.6 examples/sec; 0.701 sec/batch)
2018-10-17 10:03:18.793232: step 112350, loss = 1.20 (46.5 examples/sec; 0.688 sec/batch)
2018-10-17 10:03:25.946617: step 112360, loss = 1.12 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 10:03:33.052929: step 112370, loss = 1.06 (43.7 examples/sec; 0.731 sec/batch)
2018-10-17 10:03:40.216550: step 112380, loss = 1.20 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 10:03:47.342080: step 112390, loss = 1.01 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 10:03:54.382290: step 112400, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 10:04:04.032242: step 112410, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 10:04:11.052401: step 112420, loss = 1.12 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 10:04:18.098800: step 112430, loss = 1.17 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 10:04:25.186270: step 112440, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 10:04:32.310496: step 112450, loss = 1.12 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 10:04:39.381968: step 112460, loss = 1.04 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 10:04:46.520601: step 112470, loss = 1.03 (43.0 examples/sec; 0.744 sec/batch)
2018-10-17 10:04:53.622527: step 112480, loss = 1.04 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 10:05:00.731670: step 112490, loss = 1.11 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 10:05:07.914542: step 112500, loss = 1.02 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 10:05:17.630207: step 112510, loss = 1.08 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 10:05:24.752070: step 112520, loss = 1.07 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 10:05:31.891970: step 112530, loss = 1.00 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 10:05:39.087980: step 112540, loss = 1.02 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 10:05:46.288879: step 112550, loss = 1.10 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 10:05:53.374084: step 112560, loss = 0.99 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 10:06:00.569450: step 112570, loss = 0.99 (43.2 examples/sec; 0.741 sec/batch)
2018-10-17 10:06:07.690488: step 112580, loss = 1.08 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 10:06:14.770683: step 112590, loss = 1.01 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 10:06:21.911293: step 112600, loss = 0.99 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 10:06:31.705592: step 112610, loss = 1.05 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 10:06:38.840023: step 112620, loss = 1.15 (43.6 examples/sec; 0.735 sec/batch)
2018-10-17 10:06:45.980896: step 112630, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 10:06:53.140717: step 112640, loss = 0.99 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 10:07:00.354616: step 112650, loss = 1.00 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 10:07:07.439956: step 112660, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 10:07:14.576418: step 112670, loss = 1.08 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 10:07:21.695882: step 112680, loss = 1.22 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 10:07:28.848547: step 112690, loss = 1.12 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 10:07:36.084129: step 112700, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 10:07:46.202046: step 112710, loss = 1.01 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 10:07:53.373072: step 112720, loss = 1.08 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 10:08:00.549339: step 112730, loss = 1.06 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 10:08:07.774099: step 112740, loss = 1.22 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 10:08:14.973574: step 112750, loss = 1.04 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 10:08:22.093415: step 112760, loss = 1.01 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 10:08:29.214891: step 112770, loss = 1.05 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 10:08:36.424747: step 112780, loss = 0.99 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 10:08:43.575615: step 112790, loss = 1.13 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 10:08:50.775075: step 112800, loss = 1.08 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 10:09:00.533121: step 112810, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 10:09:07.604026: step 112820, loss = 1.13 (46.3 examples/sec; 0.692 sec/batch)
2018-10-17 10:09:14.732188: step 112830, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 10:09:21.914813: step 112840, loss = 0.99 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 10:09:29.061119: step 112850, loss = 1.00 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 10:09:36.320338: step 112860, loss = 1.00 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 10:09:43.488500: step 112870, loss = 1.05 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 10:09:50.650989: step 112880, loss = 1.08 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 10:09:57.903081: step 112890, loss = 1.01 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 10:10:05.154715: step 112900, loss = 1.13 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 10:10:15.868996: step 112910, loss = 1.01 (47.0 examples/sec; 0.680 sec/batch)
2018-10-17 10:10:23.051930: step 112920, loss = 1.00 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 10:10:30.305086: step 112930, loss = 0.99 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 10:10:37.509614: step 112940, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 10:10:44.835529: step 112950, loss = 1.10 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 10:10:52.090013: step 112960, loss = 1.17 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 10:10:59.358724: step 112970, loss = 1.06 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 10:11:06.510343: step 112980, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 10:11:13.731294: step 112990, loss = 1.00 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 10:11:20.954423: step 113000, loss = 1.04 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 10:11:31.658311: step 113010, loss = 1.01 (48.1 examples/sec; 0.665 sec/batch)
2018-10-17 10:11:38.694852: step 113020, loss = 1.12 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 10:11:45.821283: step 113030, loss = 1.02 (42.8 examples/sec; 0.748 sec/batch)
2018-10-17 10:11:52.914307: step 113040, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 10:12:00.043125: step 113050, loss = 1.14 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 10:12:07.183018: step 113060, loss = 0.99 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 10:12:14.325781: step 113070, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 10:12:21.555341: step 113080, loss = 1.00 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 10:12:28.773471: step 113090, loss = 1.15 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 10:12:36.013298: step 113100, loss = 1.18 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 10:12:46.070176: step 113110, loss = 1.03 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 10:12:53.333225: step 113120, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 10:13:00.584359: step 113130, loss = 1.00 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 10:13:07.861631: step 113140, loss = 1.02 (39.7 examples/sec; 0.806 sec/batch)
2018-10-17 10:13:15.342371: step 113150, loss = 1.01 (46.3 examples/sec; 0.691 sec/batch)
2018-10-17 10:13:22.585361: step 113160, loss = 1.01 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 10:13:29.656319: step 113170, loss = 1.05 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 10:13:37.158834: step 113180, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 10:13:44.575999: step 113190, loss = 1.11 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 10:13:51.789131: step 113200, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 10:14:01.510640: step 113210, loss = 0.99 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 10:14:08.637447: step 113220, loss = 1.09 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 10:14:15.796082: step 113230, loss = 1.04 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 10:14:22.957215: step 113240, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 10:14:30.145668: step 113250, loss = 1.00 (43.0 examples/sec; 0.744 sec/batch)
2018-10-17 10:14:37.311229: step 113260, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 10:14:44.483715: step 113270, loss = 1.06 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 10:14:51.606614: step 113280, loss = 0.99 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 10:14:58.778771: step 113290, loss = 1.00 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 10:15:05.958607: step 113300, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 10:15:15.636449: step 113310, loss = 1.09 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 10:15:22.786472: step 113320, loss = 1.02 (43.3 examples/sec; 0.740 sec/batch)
2018-10-17 10:15:30.023067: step 113330, loss = 1.09 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 10:15:37.130520: step 113340, loss = 1.05 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 10:15:44.280306: step 113350, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 10:15:51.406248: step 113360, loss = 1.10 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 10:15:58.566740: step 113370, loss = 1.04 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 10:16:05.705012: step 113380, loss = 1.06 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 10:16:12.848331: step 113390, loss = 1.00 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 10:16:20.057185: step 113400, loss = 1.13 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 10:16:29.713610: step 113410, loss = 0.99 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 10:16:36.835322: step 113420, loss = 1.01 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 10:16:44.045551: step 113430, loss = 1.00 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 10:16:51.235270: step 113440, loss = 1.13 (46.2 examples/sec; 0.692 sec/batch)
2018-10-17 10:16:58.482789: step 113450, loss = 1.05 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 10:17:05.607046: step 113460, loss = 1.02 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 10:17:12.723910: step 113470, loss = 1.11 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 10:17:19.891090: step 113480, loss = 1.04 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 10:17:27.090685: step 113490, loss = 0.99 (43.7 examples/sec; 0.733 sec/batch)
2018-10-17 10:17:34.275022: step 113500, loss = 1.06 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 10:17:44.533645: step 113510, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 10:17:51.652729: step 113520, loss = 1.09 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 10:17:58.737873: step 113530, loss = 1.09 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 10:18:05.872150: step 113540, loss = 1.04 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 10:18:13.042926: step 113550, loss = 1.02 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 10:18:20.184371: step 113560, loss = 0.99 (42.9 examples/sec; 0.746 sec/batch)
2018-10-17 10:18:27.344915: step 113570, loss = 1.01 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 10:18:34.586189: step 113580, loss = 1.09 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 10:18:41.752079: step 113590, loss = 1.17 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 10:18:48.921658: step 113600, loss = 1.03 (44.4 examples/sec; 0.720 sec/batch)
2018-10-17 10:18:58.661164: step 113610, loss = 1.07 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 10:19:05.800233: step 113620, loss = 1.08 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 10:19:13.024466: step 113630, loss = 1.01 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 10:19:20.190603: step 113640, loss = 1.03 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 10:19:27.349355: step 113650, loss = 1.16 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 10:19:34.619913: step 113660, loss = 1.03 (43.3 examples/sec; 0.740 sec/batch)
2018-10-17 10:19:41.756756: step 113670, loss = 1.11 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 10:19:48.875307: step 113680, loss = 1.00 (45.8 examples/sec; 0.698 sec/batch)
2018-10-17 10:19:56.028084: step 113690, loss = 1.34 (45.7 examples/sec; 0.701 sec/batch)
2018-10-17 10:20:03.157651: step 113700, loss = 0.99 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 10:20:12.914249: step 113710, loss = 1.00 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 10:20:19.965072: step 113720, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 10:20:27.072606: step 113730, loss = 1.04 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 10:20:34.150363: step 113740, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 10:20:41.301287: step 113750, loss = 1.04 (43.0 examples/sec; 0.744 sec/batch)
2018-10-17 10:20:48.456499: step 113760, loss = 1.15 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 10:20:55.577378: step 113770, loss = 1.00 (46.1 examples/sec; 0.694 sec/batch)
2018-10-17 10:21:02.680474: step 113780, loss = 1.01 (43.9 examples/sec; 0.729 sec/batch)
2018-10-17 10:21:09.797283: step 113790, loss = 1.01 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 10:21:16.897514: step 113800, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 10:21:26.579726: step 113810, loss = 0.99 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 10:21:33.681070: step 113820, loss = 1.07 (45.4 examples/sec; 0.706 sec/batch)
2018-10-17 10:21:40.765969: step 113830, loss = 1.03 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 10:21:47.841207: step 113840, loss = 1.07 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 10:21:54.981304: step 113850, loss = 0.99 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 10:22:02.216221: step 113860, loss = 1.02 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 10:22:09.332451: step 113870, loss = 1.07 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 10:22:16.358375: step 113880, loss = 1.03 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 10:22:23.433229: step 113890, loss = 1.10 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 10:22:30.582279: step 113900, loss = 1.06 (42.7 examples/sec; 0.750 sec/batch)
2018-10-17 10:22:40.308253: step 113910, loss = 1.21 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 10:22:47.470610: step 113920, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 10:22:54.572689: step 113930, loss = 1.03 (46.6 examples/sec; 0.687 sec/batch)
2018-10-17 10:23:01.729369: step 113940, loss = 1.01 (43.4 examples/sec; 0.737 sec/batch)
2018-10-17 10:23:08.894716: step 113950, loss = 1.02 (42.5 examples/sec; 0.752 sec/batch)
2018-10-17 10:23:16.082794: step 113960, loss = 1.22 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 10:23:23.248723: step 113970, loss = 1.01 (45.0 examples/sec; 0.712 sec/batch)
2018-10-17 10:23:30.450591: step 113980, loss = 1.00 (44.2 examples/sec; 0.725 sec/batch)
2018-10-17 10:23:37.590203: step 113990, loss = 1.05 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 10:23:44.787060: step 114000, loss = 1.22 (44.3 examples/sec; 0.722 sec/batch)
2018-10-17 10:23:54.468606: step 114010, loss = 1.15 (47.5 examples/sec; 0.674 sec/batch)
2018-10-17 10:24:01.628464: step 114020, loss = 1.02 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 10:24:08.755558: step 114030, loss = 1.12 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 10:24:16.004120: step 114040, loss = 1.09 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 10:24:23.123394: step 114050, loss = 1.11 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 10:24:30.299101: step 114060, loss = 1.10 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 10:24:37.500656: step 114070, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 10:24:44.680021: step 114080, loss = 1.02 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 10:24:51.783196: step 114090, loss = 1.03 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 10:24:59.018029: step 114100, loss = 1.07 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 10:25:08.760586: step 114110, loss = 1.00 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 10:25:15.941292: step 114120, loss = 0.99 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 10:25:23.085855: step 114130, loss = 1.04 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 10:25:30.344248: step 114140, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 10:25:37.463126: step 114150, loss = 1.05 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 10:25:44.532132: step 114160, loss = 1.11 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 10:25:51.780294: step 114170, loss = 1.01 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 10:25:58.996493: step 114180, loss = 1.01 (43.7 examples/sec; 0.731 sec/batch)
2018-10-17 10:26:06.228944: step 114190, loss = 1.07 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 10:26:13.454511: step 114200, loss = 1.08 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 10:26:23.135648: step 114210, loss = 1.03 (42.5 examples/sec; 0.753 sec/batch)
2018-10-17 10:26:30.246486: step 114220, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 10:26:37.342265: step 114230, loss = 1.06 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 10:26:44.543445: step 114240, loss = 1.03 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 10:26:51.655542: step 114250, loss = 1.02 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 10:26:58.816417: step 114260, loss = 1.18 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 10:27:06.002024: step 114270, loss = 1.06 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 10:27:13.134351: step 114280, loss = 1.01 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 10:27:20.341476: step 114290, loss = 1.01 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 10:27:27.520042: step 114300, loss = 0.99 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 10:27:37.435861: step 114310, loss = 1.03 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 10:27:44.496456: step 114320, loss = 1.07 (45.9 examples/sec; 0.698 sec/batch)
2018-10-17 10:27:51.593769: step 114330, loss = 1.12 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 10:27:58.773284: step 114340, loss = 1.06 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 10:28:05.945149: step 114350, loss = 1.03 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 10:28:13.196583: step 114360, loss = 1.03 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 10:28:20.306572: step 114370, loss = 1.20 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 10:28:27.386497: step 114380, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 10:28:34.561823: step 114390, loss = 1.09 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 10:28:41.612879: step 114400, loss = 0.99 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 10:28:51.403498: step 114410, loss = 0.99 (45.2 examples/sec; 0.707 sec/batch)
2018-10-17 10:28:58.500399: step 114420, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 10:29:05.615988: step 114430, loss = 1.00 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 10:29:12.755831: step 114440, loss = 1.08 (45.6 examples/sec; 0.702 sec/batch)
2018-10-17 10:29:19.842855: step 114450, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 10:29:26.993702: step 114460, loss = 1.05 (42.4 examples/sec; 0.754 sec/batch)
2018-10-17 10:29:34.163585: step 114470, loss = 1.05 (43.5 examples/sec; 0.735 sec/batch)
2018-10-17 10:29:41.256495: step 114480, loss = 1.01 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 10:29:48.455173: step 114490, loss = 1.00 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 10:29:55.609650: step 114500, loss = 0.99 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 10:30:05.252834: step 114510, loss = 1.02 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 10:30:12.419856: step 114520, loss = 1.05 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 10:30:19.591769: step 114530, loss = 1.09 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 10:30:26.669756: step 114540, loss = 1.04 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 10:30:33.751019: step 114550, loss = 1.03 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 10:30:41.006962: step 114560, loss = 1.10 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 10:30:48.175207: step 114570, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 10:30:55.356621: step 114580, loss = 1.03 (46.9 examples/sec; 0.683 sec/batch)
2018-10-17 10:31:02.513665: step 114590, loss = 1.03 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 10:31:09.628212: step 114600, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 10:31:19.590847: step 114610, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 10:31:26.720673: step 114620, loss = 1.07 (47.7 examples/sec; 0.671 sec/batch)
2018-10-17 10:31:33.870115: step 114630, loss = 1.14 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 10:31:40.972466: step 114640, loss = 1.09 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 10:31:48.142125: step 114650, loss = 1.01 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 10:31:55.279862: step 114660, loss = 1.00 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 10:32:02.444812: step 114670, loss = 1.20 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 10:32:09.639044: step 114680, loss = 1.01 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 10:32:16.894616: step 114690, loss = 1.11 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 10:32:24.094870: step 114700, loss = 1.00 (42.8 examples/sec; 0.748 sec/batch)
2018-10-17 10:32:34.145167: step 114710, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 10:32:41.363377: step 114720, loss = 1.04 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 10:32:48.471587: step 114730, loss = 1.02 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 10:32:55.731147: step 114740, loss = 1.05 (46.6 examples/sec; 0.686 sec/batch)
2018-10-17 10:33:02.834252: step 114750, loss = 0.99 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 10:33:10.032199: step 114760, loss = 1.06 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 10:33:17.254152: step 114770, loss = 1.07 (44.9 examples/sec; 0.712 sec/batch)
2018-10-17 10:33:24.368067: step 114780, loss = 1.12 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 10:33:31.531193: step 114790, loss = 1.02 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 10:33:38.764892: step 114800, loss = 1.11 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 10:33:48.538501: step 114810, loss = 1.00 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 10:33:55.628118: step 114820, loss = 1.00 (46.2 examples/sec; 0.693 sec/batch)
2018-10-17 10:34:02.797638: step 114830, loss = 1.03 (45.4 examples/sec; 0.705 sec/batch)
2018-10-17 10:34:09.956476: step 114840, loss = 1.03 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 10:34:17.176301: step 114850, loss = 1.05 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 10:34:24.280374: step 114860, loss = 1.14 (46.4 examples/sec; 0.690 sec/batch)
2018-10-17 10:34:31.440992: step 114870, loss = 1.02 (46.0 examples/sec; 0.696 sec/batch)
2018-10-17 10:34:38.521699: step 114880, loss = 1.19 (46.4 examples/sec; 0.689 sec/batch)
2018-10-17 10:34:45.704511: step 114890, loss = 0.99 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 10:34:52.879265: step 114900, loss = 1.08 (43.2 examples/sec; 0.740 sec/batch)
2018-10-17 10:35:02.876199: step 114910, loss = 1.04 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 10:35:10.711848: step 114920, loss = 1.04 (41.9 examples/sec; 0.763 sec/batch)
2018-10-17 10:35:17.843767: step 114930, loss = 1.02 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 10:35:25.119486: step 114940, loss = 0.99 (41.5 examples/sec; 0.771 sec/batch)
2018-10-17 10:35:33.034231: step 114950, loss = 1.07 (42.7 examples/sec; 0.749 sec/batch)
2018-10-17 10:35:40.275141: step 114960, loss = 1.20 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 10:35:47.403411: step 114970, loss = 1.11 (45.4 examples/sec; 0.704 sec/batch)
2018-10-17 10:35:54.607584: step 114980, loss = 1.02 (43.0 examples/sec; 0.745 sec/batch)
2018-10-17 10:36:01.736060: step 114990, loss = 1.03 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 10:36:08.895629: step 115000, loss = 0.99 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 10:36:21.859870: step 115010, loss = 1.05 (44.9 examples/sec; 0.713 sec/batch)
2018-10-17 10:36:28.814269: step 115020, loss = 1.18 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 10:36:36.009401: step 115030, loss = 1.02 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 10:36:43.411392: step 115040, loss = 1.12 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 10:36:50.836179: step 115050, loss = 1.04 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 10:36:58.242598: step 115060, loss = 1.01 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 10:37:05.563982: step 115070, loss = 1.02 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 10:37:12.776721: step 115080, loss = 1.08 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 10:37:19.978021: step 115090, loss = 1.02 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 10:37:27.237410: step 115100, loss = 1.10 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 10:37:37.149445: step 115110, loss = 1.01 (43.6 examples/sec; 0.734 sec/batch)
2018-10-17 10:37:44.500583: step 115120, loss = 1.08 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 10:37:51.713779: step 115130, loss = 1.01 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 10:37:58.950448: step 115140, loss = 1.00 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 10:38:06.124096: step 115150, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 10:38:13.362061: step 115160, loss = 1.00 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 10:38:20.506873: step 115170, loss = 1.04 (43.1 examples/sec; 0.743 sec/batch)
2018-10-17 10:38:27.779473: step 115180, loss = 1.12 (42.6 examples/sec; 0.751 sec/batch)
2018-10-17 10:38:35.063096: step 115190, loss = 1.14 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 10:38:42.352075: step 115200, loss = 1.01 (42.5 examples/sec; 0.753 sec/batch)
2018-10-17 10:38:52.287411: step 115210, loss = 1.07 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 10:38:59.484316: step 115220, loss = 1.07 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 10:39:06.718428: step 115230, loss = 1.09 (45.2 examples/sec; 0.709 sec/batch)
2018-10-17 10:39:13.977384: step 115240, loss = 1.09 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 10:39:21.182699: step 115250, loss = 1.01 (43.6 examples/sec; 0.733 sec/batch)
2018-10-17 10:39:28.394334: step 115260, loss = 1.02 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 10:39:35.741289: step 115270, loss = 1.24 (41.0 examples/sec; 0.780 sec/batch)
2018-10-17 10:39:42.978576: step 115280, loss = 1.00 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 10:39:50.203421: step 115290, loss = 1.12 (45.0 examples/sec; 0.711 sec/batch)
2018-10-17 10:39:57.479066: step 115300, loss = 1.01 (43.8 examples/sec; 0.730 sec/batch)
2018-10-17 10:40:07.300146: step 115310, loss = 1.10 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 10:40:14.527227: step 115320, loss = 1.13 (44.0 examples/sec; 0.727 sec/batch)
2018-10-17 10:40:21.679500: step 115330, loss = 1.09 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 10:40:28.889717: step 115340, loss = 1.00 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 10:40:36.118977: step 115350, loss = 1.12 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 10:40:43.285925: step 115360, loss = 1.13 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 10:40:50.655192: step 115370, loss = 1.05 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 10:40:57.872191: step 115380, loss = 1.00 (44.2 examples/sec; 0.724 sec/batch)
2018-10-17 10:41:05.143001: step 115390, loss = 1.16 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 10:41:12.439383: step 115400, loss = 1.02 (44.6 examples/sec; 0.718 sec/batch)
2018-10-17 10:41:22.561814: step 115410, loss = 1.00 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 10:41:29.773055: step 115420, loss = 1.01 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 10:41:37.082390: step 115430, loss = 1.01 (41.8 examples/sec; 0.765 sec/batch)
2018-10-17 10:41:44.264278: step 115440, loss = 0.99 (45.1 examples/sec; 0.710 sec/batch)
2018-10-17 10:41:51.529202: step 115450, loss = 1.00 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 10:41:58.811160: step 115460, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 10:42:06.030151: step 115470, loss = 1.13 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 10:42:13.383240: step 115480, loss = 1.12 (41.8 examples/sec; 0.766 sec/batch)
2018-10-17 10:42:20.679729: step 115490, loss = 1.01 (43.5 examples/sec; 0.736 sec/batch)
2018-10-17 10:42:28.047207: step 115500, loss = 0.99 (41.8 examples/sec; 0.765 sec/batch)
2018-10-17 10:42:38.299291: step 115510, loss = 1.16 (44.5 examples/sec; 0.719 sec/batch)
2018-10-17 10:42:45.427296: step 115520, loss = 1.13 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 10:42:52.636430: step 115530, loss = 1.14 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 10:42:59.832060: step 115540, loss = 1.13 (44.2 examples/sec; 0.723 sec/batch)
2018-10-17 10:43:07.046199: step 115550, loss = 1.09 (43.7 examples/sec; 0.732 sec/batch)
2018-10-17 10:43:14.238506: step 115560, loss = 1.17 (43.8 examples/sec; 0.731 sec/batch)
2018-10-17 10:43:21.440766: step 115570, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 10:43:28.654384: step 115580, loss = 1.00 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 10:43:35.903650: step 115590, loss = 0.99 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 10:43:43.161806: step 115600, loss = 1.03 (42.6 examples/sec; 0.752 sec/batch)
2018-10-17 10:43:53.726123: step 115610, loss = 1.01 (43.4 examples/sec; 0.738 sec/batch)
2018-10-17 10:44:00.971075: step 115620, loss = 1.00 (44.3 examples/sec; 0.723 sec/batch)
2018-10-17 10:44:08.209665: step 115630, loss = 1.05 (44.0 examples/sec; 0.728 sec/batch)
2018-10-17 10:44:15.429125: step 115640, loss = 1.13 (45.2 examples/sec; 0.708 sec/batch)
2018-10-17 10:44:22.580424: step 115650, loss = 1.04 (43.9 examples/sec; 0.728 sec/batch)
2018-10-17 10:44:29.734449: step 115660, loss = 1.07 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 10:44:36.899723: step 115670, loss = 1.01 (46.0 examples/sec; 0.695 sec/batch)
2018-10-17 10:44:44.184200: step 115680, loss = 1.11 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 10:44:51.376661: step 115690, loss = 1.06 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 10:44:58.534176: step 115700, loss = 1.03 (44.8 examples/sec; 0.714 sec/batch)
2018-10-17 10:45:08.213739: step 115710, loss = 1.01 (45.8 examples/sec; 0.699 sec/batch)
2018-10-17 10:45:15.327499: step 115720, loss = 1.06 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 10:45:22.594175: step 115730, loss = 1.06 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 10:45:29.764126: step 115740, loss = 1.00 (46.1 examples/sec; 0.695 sec/batch)
2018-10-17 10:45:36.935080: step 115750, loss = 1.02 (45.7 examples/sec; 0.700 sec/batch)
2018-10-17 10:45:44.171796: step 115760, loss = 1.04 (44.7 examples/sec; 0.716 sec/batch)
2018-10-17 10:45:51.480342: step 115770, loss = 1.22 (43.3 examples/sec; 0.739 sec/batch)
2018-10-17 10:45:58.781342: step 115780, loss = 1.09 (44.8 examples/sec; 0.715 sec/batch)
2018-10-17 10:46:06.035850: step 115790, loss = 0.99 (44.6 examples/sec; 0.717 sec/batch)
2018-10-17 10:46:13.289284: step 115800, loss = 1.01 (44.5 examples/sec; 0.720 sec/batch)
2018-10-17 10:46:23.140135: step 115810, loss = 1.03 (44.1 examples/sec; 0.726 sec/batch)
2018-10-17 10:46:30.274874: step 115820, loss = 1.00 (45.1 examples/sec; 0.709 sec/batch)
2018-10-17 10:46:37.554835: step 115830, loss = 1.00 (43.3 examples/sec; 0.740 sec/batch)
2018-10-17 10:46:44.833859: step 115840, loss = 1.02 (45.3 examples/sec; 0.706 sec/batch)
2018-10-17 10:46:52.041341: step 115850, loss = 1.08 (45.3 examples/sec; 0.707 sec/batch)
2018-10-17 10:46:59.273183: step 115860, loss = 1.02 (44.7 examples/sec; 0.715 sec/batch)
2018-10-17 10:47:06.471999: step 115870, loss = 1.02 (45.5 examples/sec; 0.703 sec/batch)
2018-10-17 10:47:13.809061: step 115880, loss = 1.02 (43.1 examples/sec; 0.742 sec/batch)
2018-10-17 10:47:21.023192: step 115890, loss = 1.06 (45.9 examples/sec; 0.697 sec/batch)
2018-10-17 10:47:28.261565: step 115900, loss = 1.01 (44.4 examples/sec; 0.721 sec/batch)
2018-10-17 10:47:38.056749: step 115910, loss = 1.14 (45.0 examples/sec; 0.710 sec/batch)
2018-10-17 10:47:45.436961: step 115920, loss = 0.99 (44.1 examples/sec; 0.725 sec/batch)
2018-10-17 10:47:52.555113: step 115930, loss = 1.02 (45.5 examples/sec; 0.704 sec/batch)
2018-10-17 10:47:59.881046: step 115940, loss = 1.11 (44.7 examples/sec; 0.717 sec/batch)
